<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=description content="The Computational Vision Lab combines computational theory with behavioral and brain image experiments to understand how we see the world around us."><meta name=author content="Daniel Kersten"><meta name=lang:clipboard.copy content="Copy to clipboard"><meta name=lang:clipboard.copied content="Copied to clipboard"><meta name=lang:search.language content=en><meta name=lang:search.pipeline.stopwords content=True><meta name=lang:search.pipeline.trimmer content=True><meta name=lang:search.result.none content="No matching documents"><meta name=lang:search.result.one content="1 matching document"><meta name=lang:search.result.other content="# matching documents"><meta name=lang:search.tokenizer content=[\s\-]+><link rel="shortcut icon" href=../../assets/images/favicon.ico><meta name=generator content="mkdocs-1.1, mkdocs-material-4.6.3"><title>PSY8036SP2019 - Computational Vision Lab</title><link rel=stylesheet href=../../assets/stylesheets/application.7f3669a3.css><script src=../../assets/javascripts/modernizr.eca31fed.js></script><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>body,input{font-family:"Open Sans","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style><link rel=stylesheet href=../../assets/fonts/material-icons.css></head> <body dir=ltr> <svg class=md-svg> <defs> </defs> </svg> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay data-md-component=overlay for=__drawer></label> <a href=#deep-networks-behavior-brain-and-theory tabindex=0 class=md-skip> Skip to content </a> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid"> <div class=md-flex> <div class="md-flex__cell md-flex__cell--shrink"> <a href=https://twin-cities.umn.edu title="University of Minnesota homepage" aria-label="University of Minnesota homepage" class="md-header-nav__button md-logo"> <img alt=logo src=/assets/images/D2D-gld-wht.svg width=288 height=38> </a> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--menu md-header-nav__button" for=__drawer></label> </div> <div class="md-flex__cell md-flex__cell--shrink"> <a href=https://twin-cities.umn.edu title="University of Minnesota homepage" aria-label="University of Minnesota homepage" class=block-m> <img alt=logo src=/assets/images/block-m-gold.svg height=24> </a> </div> <div class="md-flex__cell md-flex__cell--stretch"> <div class="md-flex__ellipsis md-header-nav__title" data-md-component=title> <span class=md-header-nav__topic> <a href=https://psych.umn.edu title="Department of Psychology" aria-label="Department of Psychology" class=md-header-nav__parentunit> <small>Department of Psychology</small> </a><br> <a href=../.. title="Computational Vision Lab" aria-label="Computational Vision Lab" class=md-header-nav__site-name> Computational Vision Lab </a> </span> <span class=md-header-nav__topic> <a href=../.. title="Computational Vision Lab" aria-label="Computational Vision Lab" class=md-header-nav__site-name-title> <small>Computational Vision Lab</small> </a> <br> <strong> PSY8036SP2019 </strong> </span> </div> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--search md-header-nav__button" for=__search></label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input aria-label=search name=query placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=query data-md-state=active> <label class="md-icon md-search__icon" for=__search></label> <button type=reset class="md-icon md-search__icon" data-md-component=reset tabindex=-1> &#xE5CD; </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=result> <div class=md-search-result__meta> Type to start searching </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container> <main class=md-main role=main> <div class="md-main__inner md-grid" data-md-component=container> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" data-md-level=0> <label class="md-nav__title md-nav__title--site" for=__drawer> <a href=../.. title="Computational Vision Lab" class="md-nav__button md-logo"> <img alt=logo src=../../assets/images/D2D-gld-wht.svg height=28> </a> Computational Vision Lab </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. title=Home class=md-nav__link> Home </a> </li> <li class=md-nav__item> <a href=../../publications/ title=Publications class=md-nav__link> Publications </a> </li> <li class=md-nav__item> <a href=../../research/ title=Research class=md-nav__link> Research </a> </li> <li class=md-nav__item> <a href=../../people/ title=People class=md-nav__link> People </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-5 type=checkbox id=nav-5 checked> <label class=md-nav__link for=nav-5> Courses </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-5> Courses </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../PSY5036F2019/ title=PSY5036F2019 class=md-nav__link> PSY5036F2019 </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-toggle md-nav__toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> PSY8036SP2019 </label> <a href=./ title=PSY8036SP2019 class="md-nav__link md-nav__link--active"> PSY8036SP2019 </a> <nav class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc>Table of contents</label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#abstract class=md-nav__link> Abstract </a> </li> <li class=md-nav__item> <a href=#background class=md-nav__link> Background </a> </li> <li class=md-nav__item> <a href=#introduction class=md-nav__link> Introduction </a> </li> <li class=md-nav__item> <a href=#week-1-deep-architectures class=md-nav__link> Week 1 Deep architectures </a> </li> <li class=md-nav__item> <a href=#week-2-class-cancelled-due-to-weather class=md-nav__link> Week 2 Class cancelled due to weather. </a> </li> <li class=md-nav__item> <a href=#week-3-deep-networks-and-the-brain-intro-to-julia-programming class=md-nav__link> Week 3 Deep networks and the brain &amp; Intro to Julia programming </a> </li> <li class=md-nav__item> <a href=#week-4-deep-networks-and-the-brain-reviews-simulations-with-the-julia-language class=md-nav__link> Week 4 Deep networks and the brain reviews &amp; Simulations with the Julia language </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#empirical-tests class=md-nav__link> Empirical tests </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-5-empirical-tests-of-dcnns-human-object-recognition class=md-nav__link> Week 5 Empirical tests of DCNNs: human object recognition </a> </li> <li class=md-nav__item> <a href=#week-6-empirical-tests-of-dcnns-and-methods-of-study class=md-nav__link> Week 6 Empirical tests of DCNNs and methods of study </a> </li> <li class=md-nav__item> <a href=#week-7-dcnns-and-the-brain class=md-nav__link> Week 7 DCNNs and the brain </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#understanding-deep-networks class=md-nav__link> Understanding deep networks </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-8-theory-shallow-vs-deep class=md-nav__link> Week 8 Theory: shallow vs. deep </a> </li> <li class=md-nav__item> <a href=#week-9-theory-dcnns-and-implicit-generative-models class=md-nav__link> Week 9 Theory: DCNNs and implicit generative models </a> </li> <li class=md-nav__item> <a href=#week-10-theory-invariance class=md-nav__link> Week 10 Theory: invariance </a> </li> <li class=md-nav__item> <a href=#week-11-theory-normalization class=md-nav__link> Week 11 Theory: normalization </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#whats-missing class=md-nav__link> What’s missing? </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-12-compositionality-and-semantic-accessibility class=md-nav__link> Week 12 Compositionality and semantic accessibility </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../PSY5038F2018/ title=PSY5038F2018 class=md-nav__link> PSY5038F2018 </a> </li> <li class=md-nav__item> <a href=../PSY8036SP2018/ title=PSY8036SP2018 class=md-nav__link> PSY8036SP2018 </a> </li> <li class=md-nav__item> <a href=../PSY5036F2017/ title=PSY5036F2017 class=md-nav__link> PSY5036F2017 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-6 type=checkbox id=nav-6> <label class=md-nav__link for=nav-6> Demos </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-6> Demos </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../demos/lightness/ title=Lightness class=md-nav__link> Lightness </a> </li> <li class=md-nav__item> <a href=../../demos/matte-shiny/ title="Shiny or Matte?" class=md-nav__link> Shiny or Matte? </a> </li> <li class=md-nav__item> <a href=../../demos/retinotopy/ title=Retinotopy class=md-nav__link> Retinotopy </a> </li> <li class=md-nav__item> <a href=../../demos/shadows/ title=Shadows class=md-nav__link> Shadows </a> </li> <li class=md-nav__item> <a href=../../demos/transparency/ title=Transparency class=md-nav__link> Transparency </a> </li> <li class=md-nav__item> <a href=../../images/BlojKerstenHurlbertDemo99.pdf title="Color and Mutual Illumination" class=md-nav__link> Color and Mutual Illumination </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-7 type=checkbox id=nav-7> <label class=md-nav__link for=nav-7> Data sets </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-7> Data sets </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../datasets/camouflage/camouflage/ title=Camouflage class=md-nav__link> Camouflage </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../contact/ title="Contact information" class=md-nav__link> Contact information </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc>Table of contents</label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#abstract class=md-nav__link> Abstract </a> </li> <li class=md-nav__item> <a href=#background class=md-nav__link> Background </a> </li> <li class=md-nav__item> <a href=#introduction class=md-nav__link> Introduction </a> </li> <li class=md-nav__item> <a href=#week-1-deep-architectures class=md-nav__link> Week 1 Deep architectures </a> </li> <li class=md-nav__item> <a href=#week-2-class-cancelled-due-to-weather class=md-nav__link> Week 2 Class cancelled due to weather. </a> </li> <li class=md-nav__item> <a href=#week-3-deep-networks-and-the-brain-intro-to-julia-programming class=md-nav__link> Week 3 Deep networks and the brain &amp; Intro to Julia programming </a> </li> <li class=md-nav__item> <a href=#week-4-deep-networks-and-the-brain-reviews-simulations-with-the-julia-language class=md-nav__link> Week 4 Deep networks and the brain reviews &amp; Simulations with the Julia language </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#empirical-tests class=md-nav__link> Empirical tests </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-5-empirical-tests-of-dcnns-human-object-recognition class=md-nav__link> Week 5 Empirical tests of DCNNs: human object recognition </a> </li> <li class=md-nav__item> <a href=#week-6-empirical-tests-of-dcnns-and-methods-of-study class=md-nav__link> Week 6 Empirical tests of DCNNs and methods of study </a> </li> <li class=md-nav__item> <a href=#week-7-dcnns-and-the-brain class=md-nav__link> Week 7 DCNNs and the brain </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#understanding-deep-networks class=md-nav__link> Understanding deep networks </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-8-theory-shallow-vs-deep class=md-nav__link> Week 8 Theory: shallow vs. deep </a> </li> <li class=md-nav__item> <a href=#week-9-theory-dcnns-and-implicit-generative-models class=md-nav__link> Week 9 Theory: DCNNs and implicit generative models </a> </li> <li class=md-nav__item> <a href=#week-10-theory-invariance class=md-nav__link> Week 10 Theory: invariance </a> </li> <li class=md-nav__item> <a href=#week-11-theory-normalization class=md-nav__link> Week 11 Theory: normalization </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#whats-missing class=md-nav__link> What’s missing? </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#week-12-compositionality-and-semantic-accessibility class=md-nav__link> Week 12 Compositionality and semantic accessibility </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <h1 id=deep-networks-behavior-brain-and-theory>Deep networks: Behavior, Brain and Theory<a class=headerlink href=#deep-networks-behavior-brain-and-theory title="Permanent link">&para;</a></h1> <p><strong>Dan Kersten &amp; Paul Schrater</strong> <em>February 5, 2019</em></p> <p>University of Minnesota, Spring Semester, 2019</p> <p>Topics in Computational Vision Psy 8036*<br> Psy 5993 Section 034*</p> <p>Instructors:</p> <p>Dan Kersten: <a href=mailto:kersten@umn.edu>kersten@umn.edu</a></p> <p>Paul Schrater: <a href=mailto:schrater@umn.edu>schrater@umn.edu</a></p> <p>Meeting time: First meeting Tuesday, Jan 22th, 3:00 pm.</p> <p>Place: Elliott N227</p> <p>*Students can sign up for either Topics in Computational Vision Psy 8036 (Kersten) or Psy 5993 Section 034 (Schrater).</p> <p>See <a href=https://canvas.umn.edu/courses/95602>Canvas page for updated weekly content.</a></p> <h2 id=abstract>Abstract<a class=headerlink href=#abstract title="Permanent link">&para;</a></h2> <p>Recent rapid advances in deep learning networks have provided the means to produce “image computable models” of human vision–models that take natural images as input and produce accurate predictions of perceptual decisions. However, the current and future value of deep network research for understanding the brain’s visual system faces both methodological and conceptual challenges. What are the best methods to compare deep networks to perceptual behavior and to the brain? And how can we achieve a conceptual understanding of the networks, to determine which elements are important and which are not? We will read and discuss empirical papers that compare network models of object recognition to behavior and the brain. The seminar will also review work that is helping to understand what functions networks can compute, and the limitations on learning to generalize. Finally, we will discuss advances that will be needed to understand the human ability to interpret virtually any image–an ability that spans a wide range of visual tasks. The class format will include short introductory lectures by the instructors to provide historical context and weekly student presentations of current literature. Students will have the opportunity to collaborate on final programming projects. The course will also introduce and use Julia, a rapidly developing language for scientific programming, which is fast, flexible, and relatively easy to learn and use.</p> <h2 id=background>Background<a class=headerlink href=#background title="Permanent link">&para;</a></h2> <p>There are good online resources for learning about artificial neural networks, and in particular deep convolutional neural networks. For video content, there is the <a href="https://www.youtube.com/playlist?list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9">Neural Networks for Machine Learning</a> from Geoff Hinton’s 2016 coursera lectures and Fei-Fei Li’s Stanford 231n course, <a href=http://cs231n.stanford.edu/ >Convolutional Neural Networks for Visual Recognition</a>. For books, see Ian Goodfellow’s <a href=https://www.deeplearningbook.org/ >Deep Learning</a> free online or <a href="https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=sr_1_1?ie=UTF8&qid=1472485235&sr=8-1&keywords=deep+learning+book">for purchase</a>, and <a href="https://www.amazon.com/Visual-Cortex-Deep-Networks-Representations/dp/0262034727/ref=sr_1_1?ie=UTF8&qid=1547404563&sr=8-1&keywords=tomaso+poggio">Visual Cortex and Deep Networks: Learning Invariant Representations</a> for purchase by Tomaso Poggio and Fabio Anselmi.</p> <p>For an excellent basic background and review, watch the first 9 lectures of the Stanford 231n course.</p> <div class="admonition danger"> <p class=admonition-title>Danger</p> <p>!!!DRAFT!!!</p> </div> <h2 id=introduction>Introduction<a class=headerlink href=#introduction title="Permanent link">&para;</a></h2> <h2 id=week-1-deep-architectures>Week 1 Deep architectures<a class=headerlink href=#week-1-deep-architectures title="Permanent link">&para;</a></h2> <p>The first class will cover background and overview of the problems of human image understanding and visual recognition and how these problems have been approached. We’ll go over the goals of the seminar in the context of three questions: 1) Is computational vision close to producing biologically consistent, predictive, models of human visual recognition performance? 2) assuming that candidate models exist; how well do we understand them for example, to decide when two networks are equivalent? 3) What is missing from current theories, in terms of conceptual understanding, behavioral functionality and their neural bases? To address the first question above, we’ll review empirical papers, from psychophysics and neuroscience aimed at understanding the basic-level or “core” function of rapid object identification with the goal of determining best ways of comparing network models to behavior and the brain. To address the second question, we will review theoretical work that seeks to understand what functions networks can compute, and how efficiently they can learn the parameters (e.g. “weights”) of those functions. Finally, we will assess where research may need to go to understand human ability to interpret virtually any image–a challenge that will require advances in dynamic neural architectures that allow task flexibility.</p> <p>Background:</p> <p>For a short video Introduction to deep networks, see <a href="https://www.youtube.com/watch?v=JN6H4rQvwgY&t=18s">Lecture 1</a> from the MIT short course (6.S191): Introduction to Deep Learning.</p> <p>To run Julia programs and Jupyter notebooks locally on your computer, first <a href=https://julialang.org/downloads/ >install Julia</a>, and then use the <a href=https://www.anaconda.com/download/ >anaconda distribution</a> to install Jupyter. For video instructions see: <a href="https://www.youtube.com/watch?v=oyx8M1yoboY">installing Julia and Jupyter</a>.</p> <p>But the quickest and easiest way to start learning and using the Julia language is to sign in to <a href=https://juliabox.com/ >JuliaBox</a> where you’ll immediately be able to create notebooks and access tutorials.</p> <h2 id=week-2-class-cancelled-due-to-weather>Week 2 Class cancelled due to weather.<a class=headerlink href=#week-2-class-cancelled-due-to-weather title="Permanent link">&para;</a></h2> <h2 id=week-3-deep-networks-and-the-brain-intro-to-julia-programming>Week 3 Deep networks and the brain &amp; Intro to Julia programming<a class=headerlink href=#week-3-deep-networks-and-the-brain-intro-to-julia-programming title="Permanent link">&para;</a></h2> <p>Background:</p> <p><a href="https://www.youtube.com/watch?v=8h8rQyEpiZA&amp;t">Video introduction</a> to Julia programming.</p> <p><a href=http://ucidatascienceinitiative.github.io/IntroToJulia/ >Introduction to Julia</a> for Data Science and Scientific Computing</p> <p>Readings:</p> <p>Kietzmann, T. C., McClure, P., &amp; Kriegeskorte, N. (2018, June 5). Deep Neural Networks In Computational Neuroscience. bioRxiv. doi:<a href=https://dx.doi.org/10.1101/133504>10.1101/133504</a></p> <h2 id=week-4-deep-networks-and-the-brain-reviews-simulations-with-the-julia-language>Week 4 Deep networks and the brain reviews &amp; Simulations with the Julia language<a class=headerlink href=#week-4-deep-networks-and-the-brain-reviews-simulations-with-the-julia-language title="Permanent link">&para;</a></h2> <p>Readings:</p> <p>Turner, M. H., Sanchez Giraldo, L. G., Schwartz, O., &amp; Rieke, F. (2019, January). Stimulus- and goal-oriented frameworks for understanding natural vision. Nature Neuroscience, 22(1), 15–24. doi:<a href=https://dx.doi.org/10.1038/s41593-018-0284-0>10.1038/s41593-018-0284-0</a></p> <p>Jacobs, R. A. &amp; Bates, C. J. (2018, November 27). Comparing the Visual Representations and Performance of Humans and Deep Neural Networks. Current Directions in Psychological Science, 0963721418801342. doi:<a href=https://dx.doi.org/10.1177/0963721418801342>10.1177/0963721418801342</a></p> <p>Kay, K. N. (2018, October 15). Principles for models of neural information processing. NeuroImage. New Advances in Encoding and Decoding of Brain Signals, 180, 101–109. doi:<a href=https://dx.doi.org/10.1016/j.neuroimage.2017.08.016>10.1016/j.neuroimage.2017.08.016</a></p> <h3 id=empirical-tests>Empirical tests<a class=headerlink href=#empirical-tests title="Permanent link">&para;</a></h3> <p>The most basic perceptual test is “look and see” which is what is often what is done by modelers. One can treat “adversarial examples” as behavioral tests to guide model development. However, one can be systematic and ask what parametric manipulations can be made of test images that we expect humans to generalize “for free”. How best to design “adversarial tasks" for a model network? For example, based on typical experience, we expect little cost to testing on certain families of novel image variations. These manipulations can be based on 3D variables (e.g. object transformations such as 3D rotation, cast shadows, occlusion), or image variables (e.g. lower contrast, blur, noise). Manipulations can be based on highly artificial variations that we already know humans have generalization abilities, if limited, such as reverse contrast, non-linear histograms, morphs, and atypical occlusions. Given these manipulations, what are good quantitative measures that can be applied to both human and model observers? When does more model training solve the problem, and can we understand when to rule out a class of network architectures?</p> <p>Most current perceptual tests have been with feedforward DCNNs, and that is where we start.</p> <h2 id=week-5-empirical-tests-of-dcnns-human-object-recognition>Week 5 Empirical tests of DCNNs: human object recognition<a class=headerlink href=#week-5-empirical-tests-of-dcnns-human-object-recognition title="Permanent link">&para;</a></h2> <p>Methodological challenges and the design of adversarial tasks.</p> <p>Readings:</p> <p>Ullman, S., Assif, L., Fetaya, E., &amp; Harari, D. (2016, March 8). Atoms of recognition in human and computer vision. Proceedings of the National Academy of Sciences, 113(10), 2744–2749. doi:<a href=https://dx.doi.org/10.1073/pnas.1513198113>10.1073/pnas.1513198113</a>. pmid: <a href=26884200>26884200</a></p> <p>Zhang, R., Isola, P., Efros, A. A., Shechtman, E., &amp; Wang, O. (2018). The Unreasonable Effectiveness of Deep Features as a Perceptual Metric, 10. Retrieved from <a href=http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0299.pdf>http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0299.pdf</a></p> <p>Elsayed, G. F., Shankar, S., Cheung, B., Papernot, N., Kurakin, A., Goodfellow, I., &amp; Sohl-Dickstein, J. (2018, February 22). Adversarial Examples that Fool both Computer Vision and Time-Limited Humans. arXiv: <a href=http://arxiv.org/abs/1802.08195>1802.08195 [cs, q-bio, stat]</a>. Retrieved November 13, 2018, from <a href=http://arxiv.org/abs/1802.08195>http://arxiv.org/abs/1802.08195</a></p> <p>Geirhos, R., Temme, C. R. M., Rauber, J., Sch�tt, H. H., Bethge, M., &amp; Wichmann, F. A. (2018). Generalisation in humans and deep neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, &amp; R. Garnett (Eds.), Advances in Neural Information Processing Systems 31 (pp. 7549–7561). Curran Associates, Inc. Retrieved January 28, 2019, from <a href=http://papers.nips.cc/paper/7982-generalisation-in-humans-and-deep-neural-networks.pdf>http://papers.nips.cc/paper/7982-generalisation-in-humans-and-deep-neural-networks.pdf</a></p> <h2 id=week-6-empirical-tests-of-dcnns-and-methods-of-study>Week 6 Empirical tests of DCNNs and methods of study<a class=headerlink href=#week-6-empirical-tests-of-dcnns-and-methods-of-study title="Permanent link">&para;</a></h2> <p>Including discussion of generative models, manifold discovery, posterior estimation – useful tools?</p> <p>Readings:</p> <p>Hill, M. Q., Parde, C. J., Castillo, C. D., Colon, Y. I., Ranjan, R., Chen, J.-C., …, &amp; O’Toole, A. J. (2018, December 28). Deep Convolutional Neural Networks in the Face of Caricature: Identity and Image Revealed. arXiv: <a href=http://arxiv.org/abs/1812.10902>1812.10902 [cs]</a>. Retrieved January 23, 2019, from <a href=http://arxiv.org/abs/1812.10902>http://arxiv.org/abs/1812.10902</a></p> <p>Zhang, M., Feng, J., Ma, K. T., Lim, J. H., Zhao, Q., &amp; Kreiman, G. (2018, December). Finding any Waldo with zero-shot invariant and efficient visual search. Nature Communications, 9(1). doi:<a href=https://dx.doi.org/10.1038/s41467-018-06217-x>10.1038/s41467-018-06217-x</a></p> <p>Ricci, M., Kim, J., &amp; Serre, T. (2018, February 9). Same-different problems strain convolutional neural networks. arXiv: <a href=http://arxiv.org/abs/1802.03390>1802.03390 [cs, q-bio]</a>. Retrieved December 11, 2018, from <a href=http://arxiv.org/abs/1802.03390>http://arxiv.org/abs/1802.03390</a></p> <p>Luo, W., Li, Y., Urtasun, R., &amp; Zemel, R. (n.d.). Understanding the Effective Receptive Field in Deep Convolutional Neural Networks, 9</p> <h2 id=week-7-dcnns-and-the-brain>Week 7 DCNNs and the brain<a class=headerlink href=#week-7-dcnns-and-the-brain title="Permanent link">&para;</a></h2> <p>Zhou, B., Bau, D., Oliva, A., &amp; Torralba, A. (2018). Interpreting Deep Visual Representations via Network Dissection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1–1. doi:<a href=https://dx.doi.org/10.1109/TPAMI.2018.2858759>10.1109/TPAMI.2018.2858759</a></p> <p>Readings:</p> <p>Rajalingham, R., Issa, E. B., Bashivan, P., Kar, K., Schmidt, K., &amp; DiCarlo, J. J. (2018, February 12). Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks. doi:<a href=https://dx.doi.org/10.1101/240614>10.1101/240614</a></p> <p>Schrimpf, M., Kubilius, J., Hong, H., Majaj, N. J., Rajalingham, R., Issa, E. B., …, &amp; DiCarlo, J. J. (2018, September 5). Brain-Score: Which Artificial Neural Network for Object Recognition is most Brain-Like? doi:<a href=https://dx.doi.org/10.1101/407007>10.1101/407007</a></p> <p>Breedlove, J. L., St-Yves, G., Olman, C. A., &amp; Naselaris, T. (2018, November 9). Human brain activity during mental imagery exhibits signatures of inference in a hierarchical generative model. doi:<a href=https://dx.doi.org/10.1101/462226>10.1101/462226</a></p> <h3 id=understanding-deep-networks>Understanding deep networks<a class=headerlink href=#understanding-deep-networks title="Permanent link">&para;</a></h3> <h2 id=week-8-theory-shallow-vs-deep>Week 8 Theory: shallow vs. deep<a class=headerlink href=#week-8-theory-shallow-vs-deep title="Permanent link">&para;</a></h2> <p>Universal approximators. Hierarchy as a solution to over-fitting.</p> <p>Poggio, T., Mhaskar, H., Rosasco, L., Miranda, B., &amp; Liao, Q. (2017, October 1). Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review. International Journal of Automation and Computing, 14(5), 503–519. doi:<a href=https://dx.doi.org/10.1007/s11633-017-1054-2>10.1007/s11633-017-1054-2</a></p> <p>Garriga-Alonso, A., Aitchison, L., &amp; Rasmussen, C. E. (2019). DEEP CONVOLUTIONAL NETWORKS AS SHALLOW GAUSSIAN PROCESSES, 16</p> <p>Lin, H. &amp; Jegelka, S. (2018, June 28). ResNet with one-neuron hidden layers is a Universal Approximator. arXiv: <a href=http://arxiv.org/abs/1806.10909>1806.10909 [cs, stat]</a>. Retrieved January 18, 2019, from <a href=http://arxiv.org/abs/1806.10909>http://arxiv.org/abs/1806.10909</a></p> <h2 id=week-9-theory-dcnns-and-implicit-generative-models>Week 9 Theory: DCNNs and implicit generative models<a class=headerlink href=#week-9-theory-dcnns-and-implicit-generative-models title="Permanent link">&para;</a></h2> <p>Feedforward DCNNs, implicit generative models, texture, and maximum entropy.</p> <p>Readings:</p> <p>Xie, J., Zhu, S.-C., &amp; Wu, Y. N. (2017). Synthesizing dynamic patterns by spatial-temporal generative convnet. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 7093–7101). Retrieved from <a href=http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Synthesizing_Dynamic_Patterns_CVPR_2017_paper.pdf>http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Synthesizing_Dynamic_Patterns_CVPR_2017_paper.pdf</a></p> <p>Zhang, Q., Wu, Y. N., &amp; Zhu, S.-C. (2017, October 2). Interpretable Convolutional Neural Networks. arXiv: <a href=http://arxiv.org/abs/1710.00935>1710.00935 [cs]</a>. Retrieved February 22, 2018, from <a href=http://arxiv.org/abs/1710.00935>http://arxiv.org/abs/1710.00935</a></p> <p>WU, Y. N., XIE, J., LU, Y., &amp; ZHU, S.-C. (2018). Sparse and Deep Generalizations of the FRAME Model. Retrieved from <a href=https://www.intlpress.com/site/pub/files/_fulltext/journals/amsa/2018/0003/0001/AMSA-2018-0003-0001-a007.pdf>https://www.intlpress.com/site/pub/files/_fulltext/journals/amsa/2018/0003/0001/AMSA-2018-0003-0001-a007.pdf</a></p> <h2 id=week-10-theory-invariance>Week 10 Theory: invariance<a class=headerlink href=#week-10-theory-invariance title="Permanent link">&para;</a></h2> <p>Readings:</p> <p>Tacchetti, A., Isik, L., &amp; Poggio, T. A. (2018, September 15). Invariant Recognition Shapes Neural Representations of Visual Input. Annual Review of Vision Science, 4(1), 403–422. doi:<a href=https://dx.doi.org/10.1146/annurev-vision-091517-034103>10.1146/annurev-vision-091517-034103</a></p> <p>Leibo, J. Z., Liao, Q., Anselmi, F., &amp; Poggio, T. (2015, October 23). The Invariance Hypothesis Implies Domain-Specific Regions in Visual Cortex. PLOS Computational Biology, 11(10), e1004390. doi:<a href=https://dx.doi.org/10.1371/journal.pcbi.1004390>10.1371/journal.pcbi.1004390</a></p> <p>Azulay, A. &amp; Weiss, Y. (2018, May 30). Why do deep convolutional networks generalize so poorly to small image transformations? arXiv: <a href=http://arxiv.org/abs/1805.12177>1805.12177 [cs]</a>. Retrieved October 4, 2018, from <a href=http://arxiv.org/abs/1805.12177>http://arxiv.org/abs/1805.12177</a></p> <h2 id=week-11-theory-normalization>Week 11 Theory: normalization<a class=headerlink href=#week-11-theory-normalization title="Permanent link">&para;</a></h2> <p>The value and types of normalization: batch, spatial, temporal, and channel normalization.</p> <p>Liao, Q., Kawaguchi, K., &amp; Poggio, T. (2016, October 19). Streaming Normalization: Towards Simpler and More Biologically-plausible Normalizations for Online and Recurrent Learning. arXiv: <a href=http://arxiv.org/abs/1610.06160>1610.06160 [cs]</a>. Retrieved December 7, 2018, from <a href=http://arxiv.org/abs/1610.06160>http://arxiv.org/abs/1610.06160</a></p> <h3 id=whats-missing>What’s missing?<a class=headerlink href=#whats-missing title="Permanent link">&para;</a></h3> <h2 id=week-12-compositionality-and-semantic-accessibility>Week 12 Compositionality and semantic accessibility<a class=headerlink href=#week-12-compositionality-and-semantic-accessibility title="Permanent link">&para;</a></h2> <p>The problems of occlusion and articulation (e.g. body pose). Computing spatial relationships,...</p> <p>Readings:</p> <p>Burgess, C. P., Matthey, L., Watters, N., Kabra, R., Higgins, I., Botvinick, M., &amp; Lerchner, A. (2019, January 22). MONet: Unsupervised Scene Decomposition and Representation. arXiv: <a href=http://arxiv.org/abs/1901.11390>1901.11390 [cs, stat]</a>. Retrieved February 5, 2019, from <a href=http://arxiv.org/abs/1901.11390>http://arxiv.org/abs/1901.11390</a></p> <p>Tang, H., Schrimpf, M., Lotter, W., Moerman, C., Paredes, A., Ortega Caro, J., …, &amp; Kreiman, G. (2018, August 28). Recurrent computations for visual pattern completion. Proceedings of the National Academy of Sciences, 115(35), 8835–8840. doi:<a href=https://dx.doi.org/10.1073/pnas.1719397115>10.1073/pnas.1719397115</a></p> <p>Zhang, Z., Xie, C., Wang, J.</p> </article> </div> </div> </main> <footer class=md-footer> <div class=md-footer-nav> <nav class="md-footer-nav__inner md-grid"> <a href=../PSY5036F2019/ title=PSY5036F2019 class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel=prev> <div class="md-flex__cell md-flex__cell--shrink"> <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i> </div> <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"> <span class=md-flex__ellipsis> <span class=md-footer-nav__direction> Previous </span> PSY5036F2019 </span> </div> </a> <a href=../PSY5038F2018/ title=PSY5038F2018 class="md-flex md-footer-nav__link md-footer-nav__link--next" rel=next> <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"> <span class=md-flex__ellipsis> <span class=md-footer-nav__direction> Next </span> PSY5038F2018 </span> </div> <div class="md-flex__cell md-flex__cell--shrink"> <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i> </div> </a> </nav> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> &copy; 2020 Regents of the University of Minnesota. All rights reserved. The University of Minnesota is an equal opportunity educator and employer. <br> <a href=http://privacy.umn.edu>Privacy Statement</a> </div> </div> <div class=md-footer-social> <link rel=stylesheet href=../../assets/fonts/font-awesome.css> <a href="https://scholar.google.com/citations?user=6j1ZjJsAAAAJ&hl=en&oi=ao" target=_blank rel=noopener title=google class="md-footer-social__link fa fa-google"></a> <a href=https://github.com/danielkersten/ target=_blank rel=noopener title=github class="md-footer-social__link fa fa-github"></a> </div> </div> </div> </footer> </div> <script src=../../assets/javascripts/application.9368ffdd.js></script> <script>app.initialize({version:"1.1",url:{base:"../.."}})</script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script> </body> </html>