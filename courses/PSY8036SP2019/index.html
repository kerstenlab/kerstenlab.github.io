
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="The Computational Vision Lab combines computational theory with behavioral and brain image experiments to understand how we see the world around us." name="description"/>
<meta content="Daniel Kersten" name="author"/>
<link href="https://kerstenlab.psych.umn.edu/courses/PSY8036SP2019/" rel="canonical"/>
<link href="../../imgs/blockm.ico" rel="icon"/>
<meta content="mkdocs-1.2.3, mkdocs-material-8.1.10" name="generator"/>
<title>PSY8036SP2019 - Computational Vision Lab</title>
<link href="../../assets/stylesheets/main.d6be258b.min.css" rel="stylesheet"/>
<link href="../../assets/stylesheets/palette.e6a45f82.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,400i,700%7C&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Open Sans";--md-code-font:""}</style>
<link href="../../stylesheets/color.css" rel="stylesheet"/>
<link href="../../stylesheets/admonition.css" rel="stylesheet"/>
<script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
</head>
<body data-md-color-accent="" data-md-color-primary="" data-md-color-scheme="umn" dir="ltr">
<script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#deep-networks-behavior-brain-and-theory-psy8036sp2019">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="Computational Vision Lab" class="md-header__button md-logo" data-md-component="logo" href="../.." title="Computational Vision Lab">
<img alt="logo" src="../../imgs/blockm.svg"/>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            Computational Vision Lab
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              PSY8036SP2019
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="" data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary="" data-md-color-scheme="umn" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_2" hidden="" title="Switch to dark mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"></path></svg>
</label>
<input aria-label="Switch to light mode" class="md-option" data-md-color-accent="" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary="" data-md-color-scheme="slate" id="__palette_2" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3z"></path></svg>
</label>
</form>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="Search" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"></path></svg>
</label>
<nav aria-label="Search" class="md-search__options">
<button aria-label="Clear" class="md-search__icon md-icon" tabindex="-1" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            Initializing search
          </div>
<ol class="md-search-result__list"></ol>
</div>
</div>
</div>
</div>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="Computational Vision Lab" class="md-nav__button md-logo" data-md-component="logo" href="../.." title="Computational Vision Lab">
<img alt="logo" src="../../imgs/blockm.svg"/>
</a>
    Computational Vision Lab
  </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../..">
        Home
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../publications/">
        Publications
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../people/">
        People
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../research/">
        Research
      </a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" id="__nav_5" type="checkbox"/>
<label class="md-nav__link" for="__nav_5">
          Courses
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-label="Courses" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_5">
<span class="md-nav__icon md-icon"></span>
          Courses
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../PSY5036F2019/">
        PSY5036F2019
      </a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" data-md-toggle="toc" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
          PSY8036SP2019
          <span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
        PSY8036SP2019
      </a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#background">
    Background
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#introduction">
    Introduction
  </a>
<nav aria-label="Introduction" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#week-1-deep-architectures">
    Week 1 Deep architectures
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#week-3-deep-networks-and-the-brain-intro-to-julia-programming">
    Week 3 Deep networks and the brain &amp; Intro to Julia programming
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#week-4-deep-networks-and-the-brain-reviews-simulations-with-the-julia-language">
    Week 4 Deep networks and the brain reviews &amp; Simulations with the Julia language
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#empirical-tests">
    Empirical tests
  </a>
<nav aria-label="Empirical tests" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#week-5-empirical-tests-of-dcnns-human-object-recognition">
    Week 5 Empirical tests of DCNNs: human object recognition
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#week-6-empirical-tests-of-dcnns-and-methods-of-study">
    Week 6 Empirical tests of DCNNs and methods of study
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#week-7-dcnns-and-the-brain">
    Week 7 DCNNs and the brain
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#understanding-deep-networks">
    Understanding deep networks
  </a>
<nav aria-label="Understanding deep networks" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#week-8-theory-shallow-vs-deep">
    Week 8 Theory: shallow vs. deep
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#week-9-theory-dcnns-and-implicit-generative-models">
    Week 9 Theory: DCNNs and implicit generative models
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#week-10-theory-invariance">
    Week 10 Theory: invariance
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#week-11-theory-normalization">
    Week 11 Theory: normalization
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#whats-missing">
    What’s missing?
  </a>
<nav aria-label="What’s missing?" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#week-12-compositionality-and-semantic-accessibility">
    Week 12 Compositionality and semantic accessibility
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../PSY5038F2018/">
        PSY5038F2018
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../PSY8036SP2018/">
        PSY8036SP2018
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../PSY8036SP2021.md">
        PSY8036SP2021
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" data-md-state="indeterminate" data-md-toggle="__nav_6" id="__nav_6" type="checkbox"/>
<label class="md-nav__link" for="__nav_6">
          Demos
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-label="Demos" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_6">
<span class="md-nav__icon md-icon"></span>
          Demos
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../demos/lightness-shape/">
        Lightness and Shape
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../demos/motion-surface/">
        Motion and Surface Material
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../demos/retinotopy/">
        Retinotopy
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../demos/shadows/">
        Shadows
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../demos/transparency/">
        Transparency
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../images/BlojKerstenHurlbertDemo99.pdf">
        Color and Mutual Illumination
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" data-md-state="indeterminate" data-md-toggle="__nav_7" id="__nav_7" type="checkbox"/>
<label class="md-nav__link" for="__nav_7">
          Data sets
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-label="Data sets" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_7">
<span class="md-nav__icon md-icon"></span>
          Data sets
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../datasets/camouflage/camouflage/">
        Camouflage
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../contact/">
        Contact
      </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#background">
    Background
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#introduction">
    Introduction
  </a>
<nav aria-label="Introduction" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#week-1-deep-architectures">
    Week 1 Deep architectures
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#week-3-deep-networks-and-the-brain-intro-to-julia-programming">
    Week 3 Deep networks and the brain &amp; Intro to Julia programming
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#week-4-deep-networks-and-the-brain-reviews-simulations-with-the-julia-language">
    Week 4 Deep networks and the brain reviews &amp; Simulations with the Julia language
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#empirical-tests">
    Empirical tests
  </a>
<nav aria-label="Empirical tests" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#week-5-empirical-tests-of-dcnns-human-object-recognition">
    Week 5 Empirical tests of DCNNs: human object recognition
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#week-6-empirical-tests-of-dcnns-and-methods-of-study">
    Week 6 Empirical tests of DCNNs and methods of study
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#week-7-dcnns-and-the-brain">
    Week 7 DCNNs and the brain
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#understanding-deep-networks">
    Understanding deep networks
  </a>
<nav aria-label="Understanding deep networks" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#week-8-theory-shallow-vs-deep">
    Week 8 Theory: shallow vs. deep
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#week-9-theory-dcnns-and-implicit-generative-models">
    Week 9 Theory: DCNNs and implicit generative models
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#week-10-theory-invariance">
    Week 10 Theory: invariance
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#week-11-theory-normalization">
    Week 11 Theory: normalization
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#whats-missing">
    What’s missing?
  </a>
<nav aria-label="What’s missing?" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#week-12-compositionality-and-semantic-accessibility">
    Week 12 Compositionality and semantic accessibility
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<h1 id="deep-networks-behavior-brain-and-theory-psy8036sp2019">Deep networks: Behavior, Brain and Theory <small>PSY8036SP2019</small><a class="headerlink" href="#deep-networks-behavior-brain-and-theory-psy8036sp2019" title="Permanent link">#</a></h1>
<p><em>University of Minnesota, Spring Semester, 2019</em></p>
<p><strong>Topics in Computational Vision</strong> </p>
<ul>
<li>PSY 8036</li>
<li>PSY 5993 Section 034</li>
</ul>
<p><strong>Instructors:</strong><br/>
Dan Kersten: <a href="mailto:kersten@umn.edu">kersten@umn.edu</a><br/>
Paul Schrater: <a href="mailto:schrater@umn.edu">schrater@umn.edu</a> </p>
<div class="admonition abstract">
<p class="admonition-title">Abstract</p>
<p>Recent rapid advances in deep learning networks have provided the means to produce “image computable models” of human vision–models that take natural images as input and produce accurate predictions of perceptual decisions. However, the current and future value of deep network research for understanding the brain’s visual system faces both methodological and conceptual challenges. What are the best methods to compare deep networks to perceptual behavior and to the brain? And how can we achieve a conceptual understanding of the networks, to determine which elements are important and which are not? We will read and discuss empirical papers that compare network models of object recognition to behavior and the brain. The seminar will also review work that is helping to understand what functions networks can compute, and the limitations on learning to generalize. Finally, we will discuss advances that will be needed to understand the human ability to interpret virtually any image–an ability that spans a wide range of visual tasks. The class format will include short introductory lectures by the instructors to provide historical context and weekly student presentations of current literature. Students will have the opportunity to collaborate on final programming projects. The course will also introduce and use Julia, a rapidly developing language for scientific programming, which is fast, flexible, and relatively easy to learn and use.</p>
</div>
<h2 id="background">Background<a class="headerlink" href="#background" title="Permanent link">#</a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>!!!DRAFT!!!</p>
</div>
<p>There are good online resources for learning about artificial neural networks, and in particular deep convolutional neural networks. For video content, there is the <a href="https://www.youtube.com/playlist?list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9">Neural Networks for Machine Learning</a> from Geoff Hinton’s 2016 coursera lectures and Fei-Fei Li’s Stanford 231n course, <a href="http://cs231n.stanford.edu/">Convolutional Neural Networks for Visual Recognition</a>. For books, see Ian Goodfellow’s <a href="https://www.deeplearningbook.org/">Deep Learning</a> free online or <a href="https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=sr_1_1?ie=UTF8&amp;qid=1472485235&amp;sr=8-1&amp;keywords=deep+learning+book">for purchase</a>, and <a href="https://www.amazon.com/Visual-Cortex-Deep-Networks-Representations/dp/0262034727/ref=sr_1_1?ie=UTF8&amp;qid=1547404563&amp;sr=8-1&amp;keywords=tomaso+poggio">Visual Cortex and Deep Networks: Learning Invariant Representations</a> for purchase by Tomaso Poggio and Fabio Anselmi.</p>
<p>For an excellent basic background and review, watch the first 9 lectures of the <a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv">Stanford 231n course</a>.</p>
<h2 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">#</a></h2>
<h3 id="week-1-deep-architectures">Week 1 <small>Deep architectures</small><a class="headerlink" href="#week-1-deep-architectures" title="Permanent link">#</a></h3>
<p>The first class will cover background and overview of the problems of human image understanding and visual recognition and how these problems have been approached. We’ll go over the goals of the seminar in the context of three questions: 1) Is computational vision close to producing biologically consistent, predictive, models of human visual recognition performance? 2) assuming that candidate models exist; how well do we understand them for example, to decide when two networks are equivalent? 3) What is missing from current theories, in terms of conceptual understanding, behavioral functionality and their neural bases? To address the first question above, we’ll review empirical papers, from psychophysics and neuroscience aimed at understanding the basic-level or “core” function of rapid object identification with the goal of determining best ways of comparing network models to behavior and the brain. To address the second question, we will review theoretical work that seeks to understand what functions networks can compute, and how efficiently they can learn the parameters (e.g. “weights”) of those functions. Finally, we will assess where research may need to go to understand human ability to interpret virtually any image–a challenge that will require advances in dynamic neural architectures that allow task flexibility.</p>
<p>Background:</p>
<p>For a short video Introduction to deep networks, see <a href="https://www.youtube.com/watch?v=JN6H4rQvwgY&amp;t=18s">Lecture 1</a> from the MIT short course (6.S191): Introduction to Deep Learning.</p>
<p>To run Julia programs and Jupyter notebooks locally on your computer, first <a href="https://julialang.org/downloads/">install Julia</a>, and then use the <a href="https://www.anaconda.com/download/">anaconda distribution</a> to install Jupyter. For video instructions see: <a href="https://www.youtube.com/watch?v=oyx8M1yoboY">installing Julia and Jupyter</a>.</p>
<p>But the quickest and easiest way to start learning and using the Julia language is to sign in to <a href="https://juliabox.com/">JuliaBox</a> where you’ll immediately be able to create notebooks and access tutorials.</p>
<h3 id="week-3-deep-networks-and-the-brain-intro-to-julia-programming">Week 3 <small>Deep networks and the brain &amp; Intro to Julia programming</small><a class="headerlink" href="#week-3-deep-networks-and-the-brain-intro-to-julia-programming" title="Permanent link">#</a></h3>
<p>Background:</p>
<p><a href="https://www.youtube.com/watch?v=8h8rQyEpiZA&amp;t">Video introduction</a> to Julia programming.</p>
<p><a href="http://ucidatascienceinitiative.github.io/IntroToJulia/">Introduction to Julia</a> for Data Science and Scientific Computing</p>
<p>Readings:</p>
<p>Kietzmann, T. C., McClure, P., &amp; Kriegeskorte, N. (2018, June 5). Deep Neural Networks In Computational Neuroscience. bioRxiv. doi:<a href="https://dx.doi.org/10.1101/133504">10.1101/133504</a></p>
<h3 id="week-4-deep-networks-and-the-brain-reviews-simulations-with-the-julia-language">Week 4 <small>Deep networks and the brain reviews &amp; Simulations with the Julia language</small><a class="headerlink" href="#week-4-deep-networks-and-the-brain-reviews-simulations-with-the-julia-language" title="Permanent link">#</a></h3>
<p>Readings:</p>
<p>Turner, M. H., Sanchez Giraldo, L. G., Schwartz, O., &amp; Rieke, F. (2019, January). Stimulus- and goal-oriented frameworks for understanding natural vision. Nature Neuroscience, 22(1), 15–24. doi:<a href="https://dx.doi.org/10.1038/s41593-018-0284-0">10.1038/s41593-018-0284-0</a></p>
<p>Jacobs, R. A. &amp; Bates, C. J. (2018, November 27). Comparing the Visual Representations and Performance of Humans and Deep Neural Networks. Current Directions in Psychological Science, 0963721418801342. doi:<a href="https://dx.doi.org/10.1177/0963721418801342">10.1177/0963721418801342</a></p>
<p>Kay, K. N. (2018, October 15). Principles for models of neural information processing. NeuroImage. New Advances in Encoding and Decoding of Brain Signals, 180, 101–109. doi:<a href="https://dx.doi.org/10.1016/j.neuroimage.2017.08.016">10.1016/j.neuroimage.2017.08.016</a></p>
<h2 id="empirical-tests">Empirical tests<a class="headerlink" href="#empirical-tests" title="Permanent link">#</a></h2>
<p>The most basic perceptual test is “look and see” which is what is often what is done by modelers. One can treat “adversarial examples” as behavioral tests to guide model development. However, one can be systematic and ask what parametric manipulations can be made of test images that we expect humans to generalize “for free”. How best to design “adversarial tasks" for a model network? For example, based on typical experience, we expect little cost to testing on certain families of novel image variations. These manipulations can be based on 3D variables (e.g. object transformations such as 3D rotation, cast shadows, occlusion), or image variables (e.g. lower contrast, blur, noise). Manipulations can be based on highly artificial variations that we already know humans have generalization abilities, if limited, such as reverse contrast, non-linear histograms, morphs, and atypical occlusions. Given these manipulations, what are good quantitative measures that can be applied to both human and model observers? When does more model training solve the problem, and can we understand when to rule out a class of network architectures?</p>
<p>Most current perceptual tests have been with feedforward DCNNs, and that is where we start.</p>
<h3 id="week-5-empirical-tests-of-dcnns-human-object-recognition">Week 5 <small>Empirical tests of DCNNs: human object recognition</small><a class="headerlink" href="#week-5-empirical-tests-of-dcnns-human-object-recognition" title="Permanent link">#</a></h3>
<p>Methodological challenges and the design of adversarial tasks.</p>
<p>Readings:</p>
<p>Ullman, S., Assif, L., Fetaya, E., &amp; Harari, D. (2016, March 8). Atoms of recognition in human and computer vision. Proceedings of the National Academy of Sciences, 113(10), 2744–2749. doi:<a href="https://dx.doi.org/10.1073/pnas.1513198113">10.1073/pnas.1513198113</a>. pmid: <a href="26884200">26884200</a></p>
<p>Zhang, R., Isola, P., Efros, A. A., Shechtman, E., &amp; Wang, O. (2018). The Unreasonable Effectiveness of Deep Features as a Perceptual Metric, 10. Retrieved from <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0299.pdf">http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0299.pdf</a></p>
<p>Elsayed, G. F., Shankar, S., Cheung, B., Papernot, N., Kurakin, A., Goodfellow, I., &amp; Sohl-Dickstein, J. (2018, February 22). Adversarial Examples that Fool both Computer Vision and Time-Limited Humans. arXiv: <a href="http://arxiv.org/abs/1802.08195">1802.08195 [cs, q-bio, stat]</a>. Retrieved November 13, 2018, from <a href="http://arxiv.org/abs/1802.08195">http://arxiv.org/abs/1802.08195</a></p>
<p>Geirhos, R., Temme, C. R. M., Rauber, J., Sch�tt, H. H., Bethge, M., &amp; Wichmann, F. A. (2018). Generalisation in humans and deep neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, &amp; R. Garnett (Eds.), Advances in Neural Information Processing Systems 31 (pp. 7549–7561). Curran Associates, Inc. Retrieved January 28, 2019, from <a href="http://papers.nips.cc/paper/7982-generalisation-in-humans-and-deep-neural-networks.pdf">http://papers.nips.cc/paper/7982-generalisation-in-humans-and-deep-neural-networks.pdf</a></p>
<h3 id="week-6-empirical-tests-of-dcnns-and-methods-of-study">Week 6 <small>Empirical tests of DCNNs and methods of study</small><a class="headerlink" href="#week-6-empirical-tests-of-dcnns-and-methods-of-study" title="Permanent link">#</a></h3>
<p>Including discussion of generative models, manifold discovery, posterior estimation – useful tools?</p>
<p>Readings:</p>
<p>Hill, M. Q., Parde, C. J., Castillo, C. D., Colon, Y. I., Ranjan, R., Chen, J.-C., …, &amp; O’Toole, A. J. (2018, December 28). Deep Convolutional Neural Networks in the Face of Caricature: Identity and Image Revealed. arXiv: <a href="http://arxiv.org/abs/1812.10902">1812.10902 [cs]</a>. Retrieved January 23, 2019, from <a href="http://arxiv.org/abs/1812.10902">http://arxiv.org/abs/1812.10902</a></p>
<p>Zhang, M., Feng, J., Ma, K. T., Lim, J. H., Zhao, Q., &amp; Kreiman, G. (2018, December). Finding any Waldo with zero-shot invariant and efficient visual search. Nature Communications, 9(1). doi:<a href="https://dx.doi.org/10.1038/s41467-018-06217-x">10.1038/s41467-018-06217-x</a></p>
<p>Ricci, M., Kim, J., &amp; Serre, T. (2018, February 9). Same-different problems strain convolutional neural networks. arXiv: <a href="http://arxiv.org/abs/1802.03390">1802.03390 [cs, q-bio]</a>. Retrieved December 11, 2018, from <a href="http://arxiv.org/abs/1802.03390">http://arxiv.org/abs/1802.03390</a></p>
<p>Luo, W., Li, Y., Urtasun, R., &amp; Zemel, R. (n.d.). Understanding the Effective Receptive Field in Deep Convolutional Neural Networks, 9</p>
<h3 id="week-7-dcnns-and-the-brain">Week 7 <small>DCNNs and the brain</small><a class="headerlink" href="#week-7-dcnns-and-the-brain" title="Permanent link">#</a></h3>
<p>Zhou, B., Bau, D., Oliva, A., &amp; Torralba, A. (2018). Interpreting Deep Visual Representations via Network Dissection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1–1. doi:<a href="https://dx.doi.org/10.1109/TPAMI.2018.2858759">10.1109/TPAMI.2018.2858759</a></p>
<p>Readings:</p>
<p>Rajalingham, R., Issa, E. B., Bashivan, P., Kar, K., Schmidt, K., &amp; DiCarlo, J. J. (2018, February 12). Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks. doi:<a href="https://dx.doi.org/10.1101/240614">10.1101/240614</a></p>
<p>Schrimpf, M., Kubilius, J., Hong, H., Majaj, N. J., Rajalingham, R., Issa, E. B., …, &amp; DiCarlo, J. J. (2018, September 5). Brain-Score: Which Artificial Neural Network for Object Recognition is most Brain-Like? doi:<a href="https://dx.doi.org/10.1101/407007">10.1101/407007</a></p>
<p>Breedlove, J. L., St-Yves, G., Olman, C. A., &amp; Naselaris, T. (2018, November 9). Human brain activity during mental imagery exhibits signatures of inference in a hierarchical generative model. doi:<a href="https://dx.doi.org/10.1101/462226">10.1101/462226</a></p>
<h2 id="understanding-deep-networks">Understanding deep networks<a class="headerlink" href="#understanding-deep-networks" title="Permanent link">#</a></h2>
<h3 id="week-8-theory-shallow-vs-deep">Week 8 <small>Theory: shallow vs. deep</small><a class="headerlink" href="#week-8-theory-shallow-vs-deep" title="Permanent link">#</a></h3>
<p>Universal approximators. Hierarchy as a solution to over-fitting.</p>
<p>Poggio, T., Mhaskar, H., Rosasco, L., Miranda, B., &amp; Liao, Q. (2017, October 1). Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review. International Journal of Automation and Computing, 14(5), 503–519. doi:<a href="https://dx.doi.org/10.1007/s11633-017-1054-2">10.1007/s11633-017-1054-2</a></p>
<p>Garriga-Alonso, A., Aitchison, L., &amp; Rasmussen, C. E. (2019). DEEP CONVOLUTIONAL NETWORKS AS SHALLOW GAUSSIAN PROCESSES, 16</p>
<p>Lin, H. &amp; Jegelka, S. (2018, June 28). ResNet with one-neuron hidden layers is a Universal Approximator. arXiv: <a href="http://arxiv.org/abs/1806.10909">1806.10909 [cs, stat]</a>. Retrieved January 18, 2019, from <a href="http://arxiv.org/abs/1806.10909">http://arxiv.org/abs/1806.10909</a></p>
<h3 id="week-9-theory-dcnns-and-implicit-generative-models">Week 9 <small>Theory: DCNNs and implicit generative models</small><a class="headerlink" href="#week-9-theory-dcnns-and-implicit-generative-models" title="Permanent link">#</a></h3>
<p>Feedforward DCNNs, implicit generative models, texture, and maximum entropy.</p>
<p>Readings:</p>
<p>Xie, J., Zhu, S.-C., &amp; Wu, Y. N. (2017). Synthesizing dynamic patterns by spatial-temporal generative convnet. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 7093–7101). Retrieved from <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Synthesizing_Dynamic_Patterns_CVPR_2017_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Synthesizing_Dynamic_Patterns_CVPR_2017_paper.pdf</a></p>
<p>Zhang, Q., Wu, Y. N., &amp; Zhu, S.-C. (2017, October 2). Interpretable Convolutional Neural Networks. arXiv: <a href="http://arxiv.org/abs/1710.00935">1710.00935 [cs]</a>. Retrieved February 22, 2018, from <a href="http://arxiv.org/abs/1710.00935">http://arxiv.org/abs/1710.00935</a></p>
<p>WU, Y. N., XIE, J., LU, Y., &amp; ZHU, S.-C. (2018). Sparse and Deep Generalizations of the FRAME Model. Retrieved from <a href="https://www.intlpress.com/site/pub/files/_fulltext/journals/amsa/2018/0003/0001/AMSA-2018-0003-0001-a007.pdf">https://www.intlpress.com/site/pub/files/_fulltext/journals/amsa/2018/0003/0001/AMSA-2018-0003-0001-a007.pdf</a></p>
<h3 id="week-10-theory-invariance">Week 10 <small>Theory: invariance</small><a class="headerlink" href="#week-10-theory-invariance" title="Permanent link">#</a></h3>
<p>Readings:</p>
<p>Tacchetti, A., Isik, L., &amp; Poggio, T. A. (2018, September 15). Invariant Recognition Shapes Neural Representations of Visual Input. Annual Review of Vision Science, 4(1), 403–422. doi:<a href="https://dx.doi.org/10.1146/annurev-vision-091517-034103">10.1146/annurev-vision-091517-034103</a></p>
<p>Leibo, J. Z., Liao, Q., Anselmi, F., &amp; Poggio, T. (2015, October 23). The Invariance Hypothesis Implies Domain-Specific Regions in Visual Cortex. PLOS Computational Biology, 11(10), e1004390. doi:<a href="https://dx.doi.org/10.1371/journal.pcbi.1004390">10.1371/journal.pcbi.1004390</a></p>
<p>Azulay, A. &amp; Weiss, Y. (2018, May 30). Why do deep convolutional networks generalize so poorly to small image transformations? arXiv: <a href="http://arxiv.org/abs/1805.12177">1805.12177 [cs]</a>. Retrieved October 4, 2018, from <a href="http://arxiv.org/abs/1805.12177">http://arxiv.org/abs/1805.12177</a></p>
<h3 id="week-11-theory-normalization">Week 11 <small>Theory: normalization</small><a class="headerlink" href="#week-11-theory-normalization" title="Permanent link">#</a></h3>
<p>The value and types of normalization: batch, spatial, temporal, and channel normalization.</p>
<p>Liao, Q., Kawaguchi, K., &amp; Poggio, T. (2016, October 19). Streaming Normalization: Towards Simpler and More Biologically-plausible Normalizations for Online and Recurrent Learning. arXiv: <a href="http://arxiv.org/abs/1610.06160">1610.06160 [cs]</a>. Retrieved December 7, 2018, from <a href="http://arxiv.org/abs/1610.06160">http://arxiv.org/abs/1610.06160</a></p>
<h2 id="whats-missing">What’s missing?<a class="headerlink" href="#whats-missing" title="Permanent link">#</a></h2>
<h3 id="week-12-compositionality-and-semantic-accessibility">Week 12 <small>Compositionality and semantic accessibility</small><a class="headerlink" href="#week-12-compositionality-and-semantic-accessibility" title="Permanent link">#</a></h3>
<p>The problems of occlusion and articulation (e.g. body pose). Computing spatial relationships,...</p>
<p>Readings:</p>
<p>Burgess, C. P., Matthey, L., Watters, N., Kabra, R., Higgins, I., Botvinick, M., &amp; Lerchner, A. (2019, January 22). MONet: Unsupervised Scene Decomposition and Representation. arXiv: <a href="http://arxiv.org/abs/1901.11390">1901.11390 [cs, stat]</a>. Retrieved February 5, 2019, from <a href="http://arxiv.org/abs/1901.11390">http://arxiv.org/abs/1901.11390</a></p>
<p>Tang, H., Schrimpf, M., Lotter, W., Moerman, C., Paredes, A., Ortega Caro, J., …, &amp; Kreiman, G. (2018, August 28). Recurrent computations for visual pattern completion. Proceedings of the National Academy of Sciences, 115(35), 8835–8840. doi:<a href="https://dx.doi.org/10.1073/pnas.1719397115">10.1073/pnas.1719397115</a></p>
<p>Zhang, Z., Xie, C., Wang, J.</p>
<hr/>
<div class="md-source-file">
<small>
    
      Last update:
      <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">March 18, 2020</span>
</small>
</div>
</article>
</div>
</div>
</main>
<footer class="md-footer">
<nav aria-label="Footer" class="md-footer__inner md-grid">
<a aria-label="Previous: PSY5036F2019" class="md-footer__link md-footer__link--prev" href="../PSY5036F2019/" rel="prev">
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"></path></svg>
</div>
<div class="md-footer__title">
<div class="md-ellipsis">
<span class="md-footer__direction">
                Previous
              </span>
              PSY5036F2019
            </div>
</div>
</a>
<a aria-label="Next: PSY5038F2018" class="md-footer__link md-footer__link--next" href="../PSY5038F2018/" rel="next">
<div class="md-footer__title">
<div class="md-ellipsis">
<span class="md-footer__direction">
                Next
              </span>
              PSY5038F2018
            </div>
</div>
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"></path></svg>
</div>
</a>
</nav>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
</div>
<div class="md-social">
<a class="md-social__link" href="https://github.com/kerstenlab" rel="noopener" target="_blank" title="github.com">
<svg viewbox="0 0 480 512" xmlns="http://www.w3.org/2000/svg"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"></path></svg>
</a>
<a class="md-social__link" href="https://scholar.google.com/citations?user=6j1ZjJsAAAAJ&amp;hl=en&amp;oi=ao" rel="noopener" target="_blank" title="scholar.google.com">
<svg viewbox="0 0 640 512" xmlns="http://www.w3.org/2000/svg"><path d="M622.34 153.2 343.4 67.5c-15.2-4.67-31.6-4.67-46.79 0L17.66 153.2c-23.54 7.23-23.54 38.36 0 45.59l48.63 14.94c-10.67 13.19-17.23 29.28-17.88 46.9C38.78 266.15 32 276.11 32 288c0 10.78 5.68 19.85 13.86 25.65L20.33 428.53C18.11 438.52 25.71 448 35.94 448h56.11c10.24 0 17.84-9.48 15.62-19.47L82.14 313.65C90.32 307.85 96 298.78 96 288c0-11.57-6.47-21.25-15.66-26.87.76-15.02 8.44-28.3 20.69-36.72L296.6 284.5c9.06 2.78 26.44 6.25 46.79 0l278.95-85.7c23.55-7.24 23.55-38.36 0-45.6zM352.79 315.09c-28.53 8.76-52.84 3.92-65.59 0l-145.02-44.55L128 384c0 35.35 85.96 64 192 64s192-28.65 192-64l-14.18-113.47-145.03 44.56z"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../..", "features": "navigation.indexes navigation.expand", "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../assets/javascripts/workers/search.092fa1f6.min.js"}</script>
<script src="../../assets/javascripts/bundle.e3b2bf44.min.js"></script>
</body>
</html>