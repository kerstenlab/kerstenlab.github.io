
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="The Computational Vision Lab combines computational theory with behavioral and brain image experiments to understand how we see the world around us.">
      
      
        <meta name="author" content="Daniel Kersten">
      
      
        <link rel="canonical" href="https://kerstenlab.psych.umn.edu/courses/PSY8036SP2019/">
      
      <link rel="icon" href="../../imgs/blockm.ico">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-8.2.1">
    
    
      
        <title>Deep networks: Behavior, Brain and Theory PSY8036SP2019 - Computational Vision Lab</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.e8d9bf0c.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.e6a45f82.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,400i,700%7C&display=fallback">
        <style>:root{--md-text-font:"Open Sans";--md-code-font:""}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/color.css">
    
      <link rel="stylesheet" href="../../stylesheets/admonition.css">
    
    <script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="umn" data-md-color-primary="" data-md-color-accent="">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#deep-networks-behavior-brain-and-theory-psy8036sp2019" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Computational Vision Lab" class="md-header__button md-logo" aria-label="Computational Vision Lab" data-md-component="logo">
      
  <img src="../../imgs/blockm.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Computational Vision Lab
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Deep networks: Behavior, Brain and Theory <small>PSY8036SP2019</small>
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="umn" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Computational Vision Lab" class="md-nav__button md-logo" aria-label="Computational Vision Lab" data-md-component="logo">
      
  <img src="../../imgs/blockm.svg" alt="logo">

    </a>
    Computational Vision Lab
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../publications/" class="md-nav__link">
        Publications
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../people/" class="md-nav__link">
        People
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../research/" class="md-nav__link">
        Research
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" data-md-state="indeterminate" type="checkbox" id="__nav_5" checked>
      
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5">
          Courses
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Courses" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Courses
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../PSY5038F2018/" class="md-nav__link">
        PSY5038F2018
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../PSY8036SP2018/" class="md-nav__link">
        PSY8036SP2018
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" data-md-state="indeterminate" type="checkbox" id="__nav_6" checked>
      
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_6">
          Demos
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Demos" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Demos
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../demos/lightness-shape/" class="md-nav__link">
        Lightness and Shape
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../demos/motion-surface/" class="md-nav__link">
        Motion and Surface Material
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../demos/retinotopy/" class="md-nav__link">
        Retinotopy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../demos/shadows/" class="md-nav__link">
        Shadows
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../demos/transparency/" class="md-nav__link">
        Transparency
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../images/BlojKerstenHurlbertDemo99.pdf" class="md-nav__link">
        Color and Mutual Illumination
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" data-md-state="indeterminate" type="checkbox" id="__nav_7" checked>
      
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_7">
          Data sets
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Data sets" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Data sets
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../datasets/camouflage/camouflage/" class="md-nav__link">
        Camouflage
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../contact/" class="md-nav__link">
        Contact
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#background" class="md-nav__link">
    Background
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction
  </a>
  
    <nav class="md-nav" aria-label="Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#week-1-deep-architectures" class="md-nav__link">
    Week 1 Deep architectures
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#week-3-deep-networks-and-the-brain-intro-to-julia-programming" class="md-nav__link">
    Week 3 Deep networks and the brain &amp; Intro to Julia programming
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#week-4-deep-networks-and-the-brain-reviews-simulations-with-the-julia-language" class="md-nav__link">
    Week 4 Deep networks and the brain reviews &amp; Simulations with the Julia language
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#empirical-tests" class="md-nav__link">
    Empirical tests
  </a>
  
    <nav class="md-nav" aria-label="Empirical tests">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#week-5-empirical-tests-of-dcnns-human-object-recognition" class="md-nav__link">
    Week 5 Empirical tests of DCNNs: human object recognition
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#week-6-empirical-tests-of-dcnns-and-methods-of-study" class="md-nav__link">
    Week 6 Empirical tests of DCNNs and methods of study
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#week-7-dcnns-and-the-brain" class="md-nav__link">
    Week 7 DCNNs and the brain
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#understanding-deep-networks" class="md-nav__link">
    Understanding deep networks
  </a>
  
    <nav class="md-nav" aria-label="Understanding deep networks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#week-8-theory-shallow-vs-deep" class="md-nav__link">
    Week 8 Theory: shallow vs. deep
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#week-9-theory-dcnns-and-implicit-generative-models" class="md-nav__link">
    Week 9 Theory: DCNNs and implicit generative models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#week-10-theory-invariance" class="md-nav__link">
    Week 10 Theory: invariance
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#week-11-theory-normalization" class="md-nav__link">
    Week 11 Theory: normalization
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#whats-missing" class="md-nav__link">
    What’s missing?
  </a>
  
    <nav class="md-nav" aria-label="What’s missing?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#week-12-compositionality-and-semantic-accessibility" class="md-nav__link">
    Week 12 Compositionality and semantic accessibility
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                


<h1 id="deep-networks-behavior-brain-and-theory-psy8036sp2019">Deep networks: Behavior, Brain and Theory <small>PSY8036SP2019</small><a class="headerlink" href="#deep-networks-behavior-brain-and-theory-psy8036sp2019" title="Permanent link">#</a></h1>
<p><em>University of Minnesota, Spring Semester, 2019</em></p>
<p><strong>Topics in Computational Vision</strong>  </p>
<ul>
<li>PSY 8036</li>
<li>PSY 5993 Section 034</li>
</ul>
<p><strong>Instructors:</strong><br />
Dan Kersten: <a href="mailto:kersten@umn.edu">kersten@umn.edu</a><br />
Paul Schrater: <a href="mailto:schrater@umn.edu">schrater@umn.edu</a>  </p>
<div class="admonition abstract">
<p class="admonition-title">Abstract</p>
<p>Recent rapid advances in deep learning networks have provided the means to produce “image computable models” of human vision–models that take natural images as input and produce accurate predictions of perceptual decisions. However, the current and future value of deep network research for understanding the brain’s visual system faces both methodological and conceptual challenges. What are the best methods to compare deep networks to perceptual behavior and to the brain? And how can we achieve a conceptual understanding of the networks, to determine which elements are important and which are not? We will read and discuss empirical papers that compare network models of object recognition to behavior and the brain. The seminar will also review work that is helping to understand what functions networks can compute, and the limitations on learning to generalize. Finally, we will discuss advances that will be needed to understand the human ability to interpret virtually any image–an ability that spans a wide range of visual tasks. The class format will include short introductory lectures by the instructors to provide historical context and weekly student presentations of current literature. Students will have the opportunity to collaborate on final programming projects. The course will also introduce and use Julia, a rapidly developing language for scientific programming, which is fast, flexible, and relatively easy to learn and use.</p>
</div>
<h2 id="background">Background<a class="headerlink" href="#background" title="Permanent link">#</a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>!!!DRAFT!!!</p>
</div>
<p>There are good online resources for learning about artificial neural networks, and in particular deep convolutional neural networks. For video content, there is the <a href="https://www.youtube.com/playlist?list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9">Neural Networks for Machine Learning</a> from Geoff Hinton’s 2016 coursera lectures and Fei-Fei Li’s Stanford 231n course, <a href="http://cs231n.stanford.edu/">Convolutional Neural Networks for Visual Recognition</a>. For books, see Ian Goodfellow’s <a href="https://www.deeplearningbook.org/">Deep Learning</a> free online or <a href="https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=sr_1_1?ie=UTF8&amp;qid=1472485235&amp;sr=8-1&amp;keywords=deep+learning+book">for purchase</a>, and <a href="https://www.amazon.com/Visual-Cortex-Deep-Networks-Representations/dp/0262034727/ref=sr_1_1?ie=UTF8&amp;qid=1547404563&amp;sr=8-1&amp;keywords=tomaso+poggio">Visual Cortex and Deep Networks: Learning Invariant Representations</a> for purchase by Tomaso Poggio and Fabio Anselmi.</p>
<p>For an excellent basic background and review, watch the first 9 lectures of the <a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv">Stanford 231n course</a>.</p>
<h2 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">#</a></h2>
<h3 id="week-1-deep-architectures">Week 1 <small>Deep architectures</small><a class="headerlink" href="#week-1-deep-architectures" title="Permanent link">#</a></h3>
<p>The first class will cover background and overview of the problems of human image understanding and visual recognition and how these problems have been approached. We’ll go over the goals of the seminar in the context of three questions: 1) Is computational vision close to producing biologically consistent, predictive, models of human visual recognition performance? 2) assuming that candidate models exist; how well do we understand them for example, to decide when two networks are equivalent? 3) What is missing from current theories, in terms of conceptual understanding, behavioral functionality and their neural bases? To address the first question above, we’ll review empirical papers, from psychophysics and neuroscience aimed at understanding the basic-level or “core” function of rapid object identification with the goal of determining best ways of comparing network models to behavior and the brain. To address the second question, we will review theoretical work that seeks to understand what functions networks can compute, and how efficiently they can learn the parameters (e.g. “weights”) of those functions. Finally, we will assess where research may need to go to understand human ability to interpret virtually any image–a challenge that will require advances in dynamic neural architectures that allow task flexibility.</p>
<p>Background:</p>
<p>For a short video Introduction to deep networks, see <a href="https://www.youtube.com/watch?v=JN6H4rQvwgY&amp;t=18s">Lecture 1</a> from the MIT short course (6.S191): Introduction to Deep Learning.</p>
<p>To run Julia programs and Jupyter notebooks locally on your computer, first <a href="https://julialang.org/downloads/">install Julia</a>, and then use the <a href="https://www.anaconda.com/download/">anaconda distribution</a> to install Jupyter. For video instructions see: <a href="https://www.youtube.com/watch?v=oyx8M1yoboY">installing Julia and Jupyter</a>.</p>
<p>But the quickest and easiest way to start learning and using the Julia language is to sign in to <a href="https://juliabox.com/">JuliaBox</a> where you’ll immediately be able to create notebooks and access tutorials.</p>
<h3 id="week-3-deep-networks-and-the-brain-intro-to-julia-programming">Week 3 <small>Deep networks and the brain &amp; Intro to Julia programming</small><a class="headerlink" href="#week-3-deep-networks-and-the-brain-intro-to-julia-programming" title="Permanent link">#</a></h3>
<p>Background:</p>
<p><a href="https://www.youtube.com/watch?v=8h8rQyEpiZA&amp;t">Video introduction</a> to Julia programming.</p>
<p><a href="http://ucidatascienceinitiative.github.io/IntroToJulia/">Introduction to Julia</a> for Data Science and Scientific Computing</p>
<p>Readings:</p>
<p>Kietzmann, T. C., McClure, P., &amp; Kriegeskorte, N. (2018, June 5). Deep Neural Networks In Computational Neuroscience. bioRxiv. doi:<a href="https://dx.doi.org/10.1101/133504">10.1101/133504</a></p>
<h3 id="week-4-deep-networks-and-the-brain-reviews-simulations-with-the-julia-language">Week 4 <small>Deep networks and the brain reviews &amp; Simulations with the Julia language</small><a class="headerlink" href="#week-4-deep-networks-and-the-brain-reviews-simulations-with-the-julia-language" title="Permanent link">#</a></h3>
<p>Readings:</p>
<p>Turner, M. H., Sanchez Giraldo, L. G., Schwartz, O., &amp; Rieke, F. (2019, January). Stimulus- and goal-oriented frameworks for understanding natural vision. Nature Neuroscience, 22(1), 15–24. doi:<a href="https://dx.doi.org/10.1038/s41593-018-0284-0">10.1038/s41593-018-0284-0</a></p>
<p>Jacobs, R. A. &amp; Bates, C. J. (2018, November 27). Comparing the Visual Representations and Performance of Humans and Deep Neural Networks. Current Directions in Psychological Science, 0963721418801342. doi:<a href="https://dx.doi.org/10.1177/0963721418801342">10.1177/0963721418801342</a></p>
<p>Kay, K. N. (2018, October 15). Principles for models of neural information processing. NeuroImage. New Advances in Encoding and Decoding of Brain Signals, 180, 101–109. doi:<a href="https://dx.doi.org/10.1016/j.neuroimage.2017.08.016">10.1016/j.neuroimage.2017.08.016</a></p>
<h2 id="empirical-tests">Empirical tests<a class="headerlink" href="#empirical-tests" title="Permanent link">#</a></h2>
<p>The most basic perceptual test is “look and see” which is what is often what is done by modelers. One can treat “adversarial examples” as behavioral tests to guide model development. However, one can be systematic and ask what parametric manipulations can be made of test images that we expect humans to generalize “for free”. How best to design “adversarial tasks" for a model network? For example, based on typical experience, we expect little cost to testing on certain families of novel image variations. These manipulations can be based on 3D variables (e.g. object transformations such as 3D rotation, cast shadows, occlusion), or image variables (e.g. lower contrast, blur, noise). Manipulations can be based on highly artificial variations that we already know humans have generalization abilities, if limited, such as reverse contrast, non-linear histograms, morphs, and atypical occlusions. Given these manipulations, what are good quantitative measures that can be applied to both human and model observers? When does more model training solve the problem, and can we understand when to rule out a class of network architectures?</p>
<p>Most current perceptual tests have been with feedforward DCNNs, and that is where we start.</p>
<h3 id="week-5-empirical-tests-of-dcnns-human-object-recognition">Week 5 <small>Empirical tests of DCNNs: human object recognition</small><a class="headerlink" href="#week-5-empirical-tests-of-dcnns-human-object-recognition" title="Permanent link">#</a></h3>
<p>Methodological challenges and the design of adversarial tasks.</p>
<p>Readings:</p>
<p>Ullman, S., Assif, L., Fetaya, E., &amp; Harari, D. (2016, March 8). Atoms of recognition in human and computer vision. Proceedings of the National Academy of Sciences, 113(10), 2744–2749. doi:<a href="https://dx.doi.org/10.1073/pnas.1513198113">10.1073/pnas.1513198113</a>. pmid: <a href="26884200">26884200</a></p>
<p>Zhang, R., Isola, P., Efros, A. A., Shechtman, E., &amp; Wang, O. (2018). The Unreasonable Effectiveness of Deep Features as a Perceptual Metric, 10. Retrieved from <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0299.pdf">http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0299.pdf</a></p>
<p>Elsayed, G. F., Shankar, S., Cheung, B., Papernot, N., Kurakin, A., Goodfellow, I., &amp; Sohl-Dickstein, J. (2018, February 22). Adversarial Examples that Fool both Computer Vision and Time-Limited Humans. arXiv: <a href="http://arxiv.org/abs/1802.08195">1802.08195 [cs, q-bio, stat]</a>. Retrieved November 13, 2018, from <a href="http://arxiv.org/abs/1802.08195">http://arxiv.org/abs/1802.08195</a></p>
<p>Geirhos, R., Temme, C. R. M., Rauber, J., Sch�tt, H. H., Bethge, M., &amp; Wichmann, F. A. (2018). Generalisation in humans and deep neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, &amp; R. Garnett (Eds.), Advances in Neural Information Processing Systems 31 (pp. 7549–7561). Curran Associates, Inc. Retrieved January 28, 2019, from <a href="http://papers.nips.cc/paper/7982-generalisation-in-humans-and-deep-neural-networks.pdf">http://papers.nips.cc/paper/7982-generalisation-in-humans-and-deep-neural-networks.pdf</a></p>
<h3 id="week-6-empirical-tests-of-dcnns-and-methods-of-study">Week 6 <small>Empirical tests of DCNNs and methods of study</small><a class="headerlink" href="#week-6-empirical-tests-of-dcnns-and-methods-of-study" title="Permanent link">#</a></h3>
<p>Including discussion of generative models, manifold discovery, posterior estimation – useful tools?</p>
<p>Readings:</p>
<p>Hill, M. Q., Parde, C. J., Castillo, C. D., Colon, Y. I., Ranjan, R., Chen, J.-C., …, &amp; O’Toole, A. J. (2018, December 28). Deep Convolutional Neural Networks in the Face of Caricature: Identity and Image Revealed. arXiv: <a href="http://arxiv.org/abs/1812.10902">1812.10902 [cs]</a>. Retrieved January 23, 2019, from <a href="http://arxiv.org/abs/1812.10902">http://arxiv.org/abs/1812.10902</a></p>
<p>Zhang, M., Feng, J., Ma, K. T., Lim, J. H., Zhao, Q., &amp; Kreiman, G. (2018, December). Finding any Waldo with zero-shot invariant and efficient visual search. Nature Communications, 9(1). doi:<a href="https://dx.doi.org/10.1038/s41467-018-06217-x">10.1038/s41467-018-06217-x</a></p>
<p>Ricci, M., Kim, J., &amp; Serre, T. (2018, February 9). Same-different problems strain convolutional neural networks. arXiv: <a href="http://arxiv.org/abs/1802.03390">1802.03390 [cs, q-bio]</a>. Retrieved December 11, 2018, from <a href="http://arxiv.org/abs/1802.03390">http://arxiv.org/abs/1802.03390</a></p>
<p>Luo, W., Li, Y., Urtasun, R., &amp; Zemel, R. (n.d.). Understanding the Effective Receptive Field in Deep Convolutional Neural Networks, 9</p>
<h3 id="week-7-dcnns-and-the-brain">Week 7 <small>DCNNs and the brain</small><a class="headerlink" href="#week-7-dcnns-and-the-brain" title="Permanent link">#</a></h3>
<p>Zhou, B., Bau, D., Oliva, A., &amp; Torralba, A. (2018). Interpreting Deep Visual Representations via Network Dissection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1–1. doi:<a href="https://dx.doi.org/10.1109/TPAMI.2018.2858759">10.1109/TPAMI.2018.2858759</a></p>
<p>Readings:</p>
<p>Rajalingham, R., Issa, E. B., Bashivan, P., Kar, K., Schmidt, K., &amp; DiCarlo, J. J. (2018, February 12). Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks. doi:<a href="https://dx.doi.org/10.1101/240614">10.1101/240614</a></p>
<p>Schrimpf, M., Kubilius, J., Hong, H., Majaj, N. J., Rajalingham, R., Issa, E. B., …, &amp; DiCarlo, J. J. (2018, September 5). Brain-Score: Which Artificial Neural Network for Object Recognition is most Brain-Like? doi:<a href="https://dx.doi.org/10.1101/407007">10.1101/407007</a></p>
<p>Breedlove, J. L., St-Yves, G., Olman, C. A., &amp; Naselaris, T. (2018, November 9). Human brain activity during mental imagery exhibits signatures of inference in a hierarchical generative model. doi:<a href="https://dx.doi.org/10.1101/462226">10.1101/462226</a></p>
<h2 id="understanding-deep-networks">Understanding deep networks<a class="headerlink" href="#understanding-deep-networks" title="Permanent link">#</a></h2>
<h3 id="week-8-theory-shallow-vs-deep">Week 8 <small>Theory: shallow vs. deep</small><a class="headerlink" href="#week-8-theory-shallow-vs-deep" title="Permanent link">#</a></h3>
<p>Universal approximators. Hierarchy as a solution to over-fitting.</p>
<p>Poggio, T., Mhaskar, H., Rosasco, L., Miranda, B., &amp; Liao, Q. (2017, October 1). Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review. International Journal of Automation and Computing, 14(5), 503–519. doi:<a href="https://dx.doi.org/10.1007/s11633-017-1054-2">10.1007/s11633-017-1054-2</a></p>
<p>Garriga-Alonso, A., Aitchison, L., &amp; Rasmussen, C. E. (2019). DEEP CONVOLUTIONAL NETWORKS AS SHALLOW GAUSSIAN PROCESSES, 16</p>
<p>Lin, H. &amp; Jegelka, S. (2018, June 28). ResNet with one-neuron hidden layers is a Universal Approximator. arXiv: <a href="http://arxiv.org/abs/1806.10909">1806.10909 [cs, stat]</a>. Retrieved January 18, 2019, from <a href="http://arxiv.org/abs/1806.10909">http://arxiv.org/abs/1806.10909</a></p>
<h3 id="week-9-theory-dcnns-and-implicit-generative-models">Week 9 <small>Theory: DCNNs and implicit generative models</small><a class="headerlink" href="#week-9-theory-dcnns-and-implicit-generative-models" title="Permanent link">#</a></h3>
<p>Feedforward DCNNs, implicit generative models, texture, and maximum entropy.</p>
<p>Readings:</p>
<p>Xie, J., Zhu, S.-C., &amp; Wu, Y. N. (2017). Synthesizing dynamic patterns by spatial-temporal generative convnet. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 7093–7101). Retrieved from <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Synthesizing_Dynamic_Patterns_CVPR_2017_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Synthesizing_Dynamic_Patterns_CVPR_2017_paper.pdf</a></p>
<p>Zhang, Q., Wu, Y. N., &amp; Zhu, S.-C. (2017, October 2). Interpretable Convolutional Neural Networks. arXiv: <a href="http://arxiv.org/abs/1710.00935">1710.00935 [cs]</a>. Retrieved February 22, 2018, from <a href="http://arxiv.org/abs/1710.00935">http://arxiv.org/abs/1710.00935</a></p>
<p>WU, Y. N., XIE, J., LU, Y., &amp; ZHU, S.-C. (2018). Sparse and Deep Generalizations of the FRAME Model. Retrieved from <a href="https://www.intlpress.com/site/pub/files/_fulltext/journals/amsa/2018/0003/0001/AMSA-2018-0003-0001-a007.pdf">https://www.intlpress.com/site/pub/files/_fulltext/journals/amsa/2018/0003/0001/AMSA-2018-0003-0001-a007.pdf</a></p>
<h3 id="week-10-theory-invariance">Week 10 <small>Theory: invariance</small><a class="headerlink" href="#week-10-theory-invariance" title="Permanent link">#</a></h3>
<p>Readings:</p>
<p>Tacchetti, A., Isik, L., &amp; Poggio, T. A. (2018, September 15). Invariant Recognition Shapes Neural Representations of Visual Input. Annual Review of Vision Science, 4(1), 403–422. doi:<a href="https://dx.doi.org/10.1146/annurev-vision-091517-034103">10.1146/annurev-vision-091517-034103</a></p>
<p>Leibo, J. Z., Liao, Q., Anselmi, F., &amp; Poggio, T. (2015, October 23). The Invariance Hypothesis Implies Domain-Specific Regions in Visual Cortex. PLOS Computational Biology, 11(10), e1004390. doi:<a href="https://dx.doi.org/10.1371/journal.pcbi.1004390">10.1371/journal.pcbi.1004390</a></p>
<p>Azulay, A. &amp; Weiss, Y. (2018, May 30). Why do deep convolutional networks generalize so poorly to small image transformations? arXiv: <a href="http://arxiv.org/abs/1805.12177">1805.12177 [cs]</a>. Retrieved October 4, 2018, from <a href="http://arxiv.org/abs/1805.12177">http://arxiv.org/abs/1805.12177</a></p>
<h3 id="week-11-theory-normalization">Week 11 <small>Theory: normalization</small><a class="headerlink" href="#week-11-theory-normalization" title="Permanent link">#</a></h3>
<p>The value and types of normalization: batch, spatial, temporal, and channel normalization.</p>
<p>Liao, Q., Kawaguchi, K., &amp; Poggio, T. (2016, October 19). Streaming Normalization: Towards Simpler and More Biologically-plausible Normalizations for Online and Recurrent Learning. arXiv: <a href="http://arxiv.org/abs/1610.06160">1610.06160 [cs]</a>. Retrieved December 7, 2018, from <a href="http://arxiv.org/abs/1610.06160">http://arxiv.org/abs/1610.06160</a></p>
<h2 id="whats-missing">What’s missing?<a class="headerlink" href="#whats-missing" title="Permanent link">#</a></h2>
<h3 id="week-12-compositionality-and-semantic-accessibility">Week 12 <small>Compositionality and semantic accessibility</small><a class="headerlink" href="#week-12-compositionality-and-semantic-accessibility" title="Permanent link">#</a></h3>
<p>The problems of occlusion and articulation (e.g. body pose). Computing spatial relationships,...</p>
<p>Readings:</p>
<p>Burgess, C. P., Matthey, L., Watters, N., Kabra, R., Higgins, I., Botvinick, M., &amp; Lerchner, A. (2019, January 22). MONet: Unsupervised Scene Decomposition and Representation. arXiv: <a href="http://arxiv.org/abs/1901.11390">1901.11390 [cs, stat]</a>. Retrieved February 5, 2019, from <a href="http://arxiv.org/abs/1901.11390">http://arxiv.org/abs/1901.11390</a></p>
<p>Tang, H., Schrimpf, M., Lotter, W., Moerman, C., Paredes, A., Ortega Caro, J., …, &amp; Kreiman, G. (2018, August 28). Recurrent computations for visual pattern completion. Proceedings of the National Academy of Sciences, 115(35), 8835–8840. doi:<a href="https://dx.doi.org/10.1073/pnas.1719397115">10.1073/pnas.1719397115</a></p>
<p>Zhang, Z., Xie, C., Wang, J.</p>

  <hr>
<div class="md-source-file">
  <small>
    
      Last update:
      <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">February 28, 2022</span>
      
    
  </small>
</div>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
        <div class="md-social">
  
    
    
      
      
    
    <a href="https://github.com/kerstenlab" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://scholar.google.com/citations?user=6j1ZjJsAAAAJ&hl=en&oi=ao" target="_blank" rel="noopener" title="scholar.google.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M623.1 136.9 340.4 35.7a62.951 62.951 0 0 0-42.43 0L16.05 136.9C6.438 140.4 0 149.6 0 160s6.438 19.65 16.05 23.09l60.02 21.51c-11.89 15.8-20.26 34.16-24.55 53.95C40.05 263.4 32 274.8 32 288c0 9.953 4.814 18.49 11.94 24.36l-24.83 149C17.48 471.1 25 480 34.89 480h58.22c9.887 0 17.41-8.879 15.78-18.63l-24.83-149C91.19 306.5 96 297.1 96 288c0-10.29-5.174-19.03-12.72-24.89a106.301 106.301 0 0 1 24.94-47.03l190.6 68.23a62.951 62.951 0 0 0 42.43 0l282.7-101.2C633.6 179.6 640 170.4 640 160s-6.4-19.6-16.9-23.1zm-272 177.5c-9.4 3.7-20.2 5.6-31.1 5.6a94.767 94.767 0 0 1-32-5.555L142.8 262.5 128 405.3c0 41.3 85.1 74.7 192 74.7 105.1 0 192-33.4 192-74.67l-14.78-142.9L351.1 314.4z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": "navigation.indexes navigation.expand", "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../assets/javascripts/workers/search.bd0b6b67.min.js"}</script>
    
    
      <script src="../../assets/javascripts/bundle.8aa65030.min.js"></script>
      
    
  </body>
</html>