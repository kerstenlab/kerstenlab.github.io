<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=description content="The Computational Vision Lab combines computational theory with behavioral and brain image experiments to understand how we see the world around us."><meta name=author content="Daniel Kersten"><meta name=lang:clipboard.copy content="Copy to clipboard"><meta name=lang:clipboard.copied content="Copied to clipboard"><meta name=lang:search.language content=en><meta name=lang:search.pipeline.stopwords content=True><meta name=lang:search.pipeline.trimmer content=True><meta name=lang:search.result.none content="No matching documents"><meta name=lang:search.result.one content="1 matching document"><meta name=lang:search.result.other content="# matching documents"><meta name=lang:search.tokenizer content=[\s\-]+><link rel="shortcut icon" href=../../assets/images/favicon.ico><meta name=generator content="mkdocs-1.1, mkdocs-material-4.6.3"><title>PSY5036F2019 - Computational Vision Lab</title><link rel=stylesheet href=../../assets/stylesheets/application.7f3669a3.css><script src=../../assets/javascripts/modernizr.eca31fed.js></script><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>body,input{font-family:"Open Sans","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style><link rel=stylesheet href=../../assets/fonts/material-icons.css></head> <body dir=ltr> <svg class=md-svg> <defs> </defs> </svg> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay data-md-component=overlay for=__drawer></label> <a href=#computational-vision tabindex=0 class=md-skip> Skip to content </a> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid"> <div class=md-flex> <div class="md-flex__cell md-flex__cell--shrink"> <a href=https://twin-cities.umn.edu title="University of Minnesota homepage" aria-label="University of Minnesota homepage" class="md-header-nav__button md-logo"> <img alt=logo src=/assets/images/D2D-gld-wht.svg width=288 height=38> </a> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--menu md-header-nav__button" for=__drawer></label> </div> <div class="md-flex__cell md-flex__cell--shrink"> <a href=https://twin-cities.umn.edu title="University of Minnesota homepage" aria-label="University of Minnesota homepage" class=block-m> <img alt=logo src=/assets/images/block-m-gold.svg height=24> </a> </div> <div class="md-flex__cell md-flex__cell--stretch"> <div class="md-flex__ellipsis md-header-nav__title" data-md-component=title> <span class=md-header-nav__topic> <a href=https://psych.umn.edu title="Department of Psychology" aria-label="Department of Psychology" class=md-header-nav__parentunit> <small>Department of Psychology</small> </a><br> <a href=../.. title="Computational Vision Lab" aria-label="Computational Vision Lab" class=md-header-nav__site-name> Computational Vision Lab </a> </span> <span class=md-header-nav__topic> <a href=../.. title="Computational Vision Lab" aria-label="Computational Vision Lab" class=md-header-nav__site-name-title> <small>Computational Vision Lab</small> </a> <br> <strong> PSY5036F2019 </strong> </span> </div> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--search md-header-nav__button" for=__search></label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input aria-label=search name=query placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=query data-md-state=active> <label class="md-icon md-search__icon" for=__search></label> <button type=reset class="md-icon md-search__icon" data-md-component=reset tabindex=-1> &#xE5CD; </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=result> <div class=md-search-result__meta> Type to start searching </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container> <main class=md-main role=main> <div class="md-main__inner md-grid" data-md-component=container> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" data-md-level=0> <label class="md-nav__title md-nav__title--site" for=__drawer> <a href=../.. title="Computational Vision Lab" class="md-nav__button md-logo"> <img alt=logo src=../../assets/images/D2D-gld-wht.svg height=28> </a> Computational Vision Lab </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. title=Home class=md-nav__link> Home </a> </li> <li class=md-nav__item> <a href=../../publications/ title=Publications class=md-nav__link> Publications </a> </li> <li class=md-nav__item> <a href=../ title=Courses class=md-nav__link> Courses </a> </li> <li class=md-nav__item> <a href=../../demos/ title=Demos class=md-nav__link> Demos </a> </li> <li class=md-nav__item> <a href=../../datasets/ title=Datasets class=md-nav__link> Datasets </a> </li> <li class=md-nav__item> <a href=../../people/ title=People class=md-nav__link> People </a> </li> <li class=md-nav__item> <a href=../../contact/ title="Contact information" class=md-nav__link> Contact information </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc>Table of contents</label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#readings class=md-nav__link> Readings </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#main class=md-nav__link> Main </a> </li> <li class=md-nav__item> <a href=#additional-readings class=md-nav__link> Additional readings </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#math-and-vision class=md-nav__link> Math and vision </a> </li> <li class=md-nav__item> <a href=#functional-human-vision class=md-nav__link> Functional human vision </a> </li> <li class=md-nav__item> <a href=#neurophysiology class=md-nav__link> Neurophysiology </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#software class=md-nav__link> Software </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#mathematica class=md-nav__link> Mathematica </a> </li> <li class=md-nav__item> <a href=#pythonipython class=md-nav__link> Python/IPython </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#writing class=md-nav__link> Writing </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <p>5036WSyllabusF2019</p> <h1 id=computational-vision><span style="font-family: Arial;">Computational Vision<a class=headerlink href=#computational-vision title="Permanent link">&para;</a></h1> <p></span><span style="font-family: Arial; font-size: large;"><em>courses.kersten.org</em></span><span style="font-family: Arial;"><br> </span><span style="font-family: Arial; font-size: large;"><strong>Psychology Department , University of Minnesota</strong></span><span style="font-family: Arial;"><br> </span><span style="font-family: Arial; font-size: large;"><strong>Psy 5036W, Fall 2019, 3 credits #34359</strong><br> </span><span style="font-size: large;">08:45 A.M. - 10:00 A.M. Mondays and Wednesdays<br> Elliott Hall N227</span></p> <p><em>Instructor</em>: Daniel Kersten. <em>Office</em>: S212 Elliott Hall. <em>Phone</em>: 612 625-2589 <em>email</em>: <a href=mailto:kersten@.umn.edu>kersten@umn.edu</a><br> <em>Office hours</em>: Mondays 10:00-11:00 am or by appointment.</p> <p>The visual perception of what is in the world is accomplished continually, instantaneously, and usually without conscious thought. The very effortlessness of perception disguises the underlying richness of the problem. We can gain insight into the processes and functions of human vision by studying the relationship between neural mechanisms and visual behavior through computer analysis and simulation. Students will learn about the anatomy and neurophysiology of vision and how they relate to the phenomona of perception. An underlying theme will be to treat vision as a process of statistical inference. There will be in-class programming exercises using the language Mathematica. No prior programming experience is required; however, some familiarity with probability, vector calculus and linear algebra is helpful.</p> <h3 id=readings>Readings<a class=headerlink href=#readings title="Permanent link">&para;</a></h3> <h4 id=main>Main<a class=headerlink href=#main title="Permanent link">&para;</a></h4> <ul> <li>Lecture notes, Main Readings &amp; Supplementary Material are all available online.</li> </ul> <h4 id=additional-readings>Additional readings<a class=headerlink href=#additional-readings title="Permanent link">&para;</a></h4> <h5 id=math-and-vision>Math and vision<a class=headerlink href=#math-and-vision title="Permanent link">&para;</a></h5> <ul> <li> <p>(<strong>EV</strong>) Early Vision. Yuille and Kersten. In <em>From Neuron to Cognition via Computational Neuroscience</em>, M.A. Arbib, James J. Bonaiuto Editors, Cambridge MA: The MIT Press, in 2016 (<a href=http://vision.psych.umn.edu/users/kersten//kersten-lab/papers/YuilleKerstenFinalChapter2016.pdf title=YuilleKerstenFinalChapter2016.pdf>preprint pdf</a>)</p> </li> <li> <p>Understanding Vision: Theory, Models, and Data. Li Zhaoping. 2014.(<a href="https://global.oup.com/academic/product/understanding-vision-9780199564668?q=Zhaoping%20Vision%20Book&lang=en&cc=gb">publisher page</a>) (<a href=http://www0.cs.ucl.ac.uk/staff/zhaoping.li/VisionBook.html>author's web outline</a>)</p> </li> </ul> <h5 id=functional-human-vision>Functional human vision<a class=headerlink href=#functional-human-vision title="Permanent link">&para;</a></h5> <ul> <li>(<strong>FV</strong>) <em>Foundations of Vision</em>. Wandell (<a href=https://foundationsofvision.stanford.edu>web</a>)</li> </ul> <h5 id=neurophysiology>Neurophysiology<a class=headerlink href=#neurophysiology title="Permanent link">&para;</a></h5> <ul> <li>(<strong>NVN</strong>) <a href=https://mitpress.mit.edu/books/new-visual-neurosciences><em>The New Visual Neurosciences</em></a>. John S. Werner and Leo M. Chalupa, edts. 2014. <a href=../../coursepapers/NewVisualNeurosciences9780262019163_toc_0001.pdf>(Table of Contents pdf)</a></li> </ul> <h3 id=software>Software<a class=headerlink href=#software title="Permanent link">&para;</a></h3> <h4 id=mathematica>Mathematica<a class=headerlink href=#mathematica title="Permanent link">&para;</a></h4> <p>Mathematica is the primary programming environment for this course. Students who have registered for the course will have access through the Psychology Department's site license.</p> <p><span class=p3>Alternatives: <span style="font-family: Arial;">Mathematica is available in several labs on campus, go to <a href=http://www.oit.umn.edu/computer-labs/software/index.htm>http://www.oit.umn.edu/computer-labs/software/index.htm<br> </a></span>You may wish to purchase <em><span style="font-size: 10.0pt; font-family: Arial;">Mathematica for Students</span></em> see <span style="font-family: Arial;"><a href=http://www.wolfram.com/products/student/mathforstudents/index.html><span style="font-size: 10.0pt;">http://www.wolfram.com/products/student/mathforstudents/index.html</span></a>.</span></span><span class=p3><br> </span></p> <p>For help using Mathematica, see: <a href=http://mathematica.stackexchange.com>http://mathematica.stackexchange.com</a></p> <h4 id=pythonipython>Python/IPython<a class=headerlink href=#pythonipython title="Permanent link">&para;</a></h4> <p><a href=http://ipython.org>http://ipython.org</a><a href=http://jupyter-notebook-beginner-guide.readthedocs.org/en/latest/index.html><br> http://jupyter-notebook-beginner-guide.readthedocs.org/en/latest/index.html</a><br> <a href=http://www.scipy.org>http://www.scipy.org</a></p> <p>For an online course in using Python and PsychoPy for research in human vision see:<a href=http://www.scipy.org><br> </a><a href=http://nbviewer.ipython.org/github/gestaltrevision/python_for_visres/blob/master/index.ipynb>http://nbviewer.ipython.org/github/gestaltrevision/python_for_visres/blob/master/index.ipynb</a></p> <h3 id=writing><strong><span style="font-family: Arial;">Writing</span></strong><a class=headerlink href=#writing title="Permanent link">&para;</a></h3> <ul> <li> <p>Gopen, G. D., &amp; Swan, J. A., 1990. The Science of Scientific Writing. <u>American Scientist</u>, <u>78</u>, 550-558. (<a href=../../coursepapers/GopenSwan1990.pdf>pdf</a>)</p> </li> <li> <p><span class=p3><span style="font-family: Arial;"><strong>Supplementary:</strong></span></span></p> <ul> <li><span class=p3>The Sense of Style: The Thinking Person's Guide to Writing in the 21<sup>st</sup> Century (2014), Pinker, Steven. (<a href=http://www.amazon.com/The-Sense-Style-Thinking-Persons/dp/0670025852>amazon link</a>)</span></li> <li><span class=p3><span style="font-family: Arial;">Penrose, A. M., &amp; Katz, S. B. (1998). <u>Writing in the Sciences: Exploring Conventions of Scientific Discourse</u>. New York: St. Martin's Press, Inc.</span></span></li> <li><span class=p3><span style="font-family: Arial;">American Psychological Association. (2009). <u>Publication manual of the American Psychological Association</u> (6<sup>th</sup> ed.). Washington, DC: American Psychological Association</span></span></li> </ul> </li> <li> <p><span class=p3><strong><span style="font-family: Arial;">Writing assistance.</span></strong> <span style="font-family: Arial;">THE CENTER FOR WRITING offers free one-to-one writing assistance to undergraduate and graduate students, with appointments up to 45 minutes. Nonnative speaker specialists are available. For more information, see <a href=http://writing.umn.edu>http://writing.umn.edu</a>.</span></span></p> </li> <li>Psychology department resources: <a href=http://writing.psych.umn.edu/student-resources>http://writing.psych.umn.edu/student-resources</a></li> </ul> <p>**Grade Requirements </p> <p>**<span style="font-family: Arial;">There will be programming assignments and a <a href=#FinalProject>final project</a>.</span></p> <p><span style="font-family: Arial;">The grade weights are:</span></p> <ul> <li> <p>Exercise/programming assignments: 55% </p> </li> <li> <p>Final project in-class presentations: 5 %</p> </li> <li> <p>Final project : 40% (five parts: 2% (title and outline) +5%(first draft) +5% (peer commentary) +8% (cover letter response) + 20% (final draft))</p> </li> </ul> <p><span style="font-family: Arial;">The programming assignments will use the <em>Mathematica</em> programming environment. No prior experience with <em>Mathematica</em> is necessary.</span></p> <dl> <dd> <span style="font-family: Arial;">Assignment due</span> <span style="color: #ff0000; font-family: Arial;">By the 6 am on</span> <span style="font-family: Arial;">the day after the nominal due date. _**<span style="color: #ff0000;">Late Policy</span>: Assignments turned in within 24 hours following the due date will have 15% deducted from the assignment score. Assignments turned in between 24 and 48 hours following the due date will have 30% deducted from the score. Assignments more than 48 hours late will receive a score of zero.**_</span> </dd> </dl> <hr> <h1 id=lectures><a name=LectureNotes></a>Lectures<a class=headerlink href=#lectures title="Permanent link">&para;</a></h1> <p><span style="font-family: Arial;"><strong><em>Check this section before each class for recent additions and revisions.</em></strong></span></p> <p><a href=http://vision.psych.umn.edu/users/kersten//kersten-lab/courses/Psy5036W2017/5036Syllabus.html>(5036W Course material from 2017)</a></p> <p><span style="font-family: Arial;"><strong><em><span style="text-align: center;"><span style="font-size: 9.0pt; font-family: Arial;">Lecture notes are in <span class=SpellE>Mathematica</span> Notebook and <span class=SpellE>pdf</span> format. You can download the <span class=SpellE>Mathematica</span> notebook files below to view with <span class=SpellE>Mathematica</span> or Wolfram CDF <a href=http://www.wolfram.com/products/player/ >Player</a> (which is free).</span></span><br> </em></strong></span></p> <table style="width: 79%;" border=1 width=79%> <tbody> <tr> <td style="width: 7.47331%;" align=center width=72><span class=p3>[University Calendar](http://onestop.umn.edu/onestop/calendar.html)</span></td> <td style="width: 3.13167%;" align=center width=24><span class=p3>**Date**</span></td> <td style="width: 8.75445%;" align=center valign=top width=159><span class=p3>**Lecture**</span></td> <td style="width: 25.2669%;" align=center valign=top width=159><span class=p3>_**Main Readings**_</span></td> <td style="width: 35.6584%;" align=center valign=top width=379><span class=p3>**Supplementary Material**</span></td> <td style="width: 19.6441%;" align=center valign=top width=163><span class=p3>**Assignments due**</span></td> </tr> <tr> <td style="width: 7.47331%;" rowspan=4 align=left valign=top width=72> <div><span class=p3>**I. Introduction**</span></div> </td> <td style="width: 3.13167%;" align=left valign=top> <div><span class=p3>Sep 4</span></div> </td> <td class=p3 style="width: 8.75445%;" align=left valign=top width=159><span class=p3>1\. Introduction to Computational Vision</span></td> <td style="width: 25.2669%;" align=left valign=top width=159> _1.IntroToComputationalVision.nb _ Olshausen, B. A. (2013). Perception as an Inference Problem. In M. Gazzaniga (Ed.), The New Cognitive Neurosciences, 5th Edition (pp. 1â€“22). MIT Press. (pp. 1â€“18). MIT Press.  </td> <td style="width: 35.6584%;" align=left valign=top width=379> Screencast: [http://www.wolfram.com/broadcast/screencasts/handsonstart/](http://www.wolfram.com/broadcast/screencasts/handsonstart/) (WITH AUDIO)[ ](Lectures/1_MultidisciplinaryStudy/FoxApertures2.mov)Check out demos under: **Life Sciences/Cognitive Science/Perception** and **Engineering & Technology/Image Processing** on the Mathematica Demonstrations site: [http://demonstrations.wolfram.com/](http://demonstrations.wolfram.com/) _Kersten, D., & Yuille, A. (2003). Bayesian models of object perception. Current Opinion in Neurobiology, 13(2), 1-9. _ EV: Section 1 </td> <td class=p3 style="width: 19.6441%;" align=left valign=top width=163> </td> </tr> <tr> <td style="width: 3.13167%;" align=left valign=top> <div><span class=p3>Sep 9</span></div> </td> <td style="width: 8.75445%;" align=left valign=top width=159><span class=p3>2.Limits to Vision</span></td> <td style="width: 25.2669%;" align=left valign=top width=159><span class=p3>_2.LimitsToVision.nb__ Hecht, S., Shlaer, S., & Pirenne, M. H. (1942). Energy, quanta, and vision. Journal of General Physiology, 25, 819-840. _</span></td> <td style="width: 35.6584%;" valign=top width=379> Barlow, H. B. (1981). Critical Limiting Factors in the Design of the Eye and Visual Cortex. Proc. Roy. Soc. Lond. B, 212, 1-34.  Baylor, D. A., Lamb, T. D., & Yau, K. W. (1979). Responses of retinal rods to single photons. Journal of Physiology, Lond., 288, 613-634.  Tinsley, J. N., Molodtsov, M. I., Prevedel, R., Wartmann, D., Pons, J. E. E., Lauwers, M., & Vaziri, A. (2016). Direct detection of a single photon by humans. Nature Communications, 7, 1–9. </td> <td style="width: 19.6441%;" valign=top width=163> </td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Sep 11</span></div> </td> <td style="width: 8.75445%;" valign=top width=159><span class=p3>3\. The Ideal Observer</span></td> <td style="width: 25.2669%;" valign=top width=159><span class=p3>_3.TheIdealObserver.nb _</span></td> <td style="width: 35.6584%;" valign=top width=379> ProbabilityOverview.nb Griffiths, T. L., & Yuille, A. (2008). A primer on probabilistic inference. In M. Oaksford and N. Chater (Eds.). The probabilistic mind: Prospects for rational models of cognition. Oxford: Oxford University Press [(pdf](../../coursepapers/GriffithsYuilleProbPrimerTICsmmc1.pdf)). Try your luck against an ideal discriminator of dot density YesNoDotDiscriminationDemo.nb </td> <td style="width: 19.6441%;" valign=top width=163> Upload Assignment #1 to [Canvas ](/courses/132509/assignments/701580) Assignment_1_Mathematica.nb </td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3><span class=p3>Sep 16</span></span></div> </td> <td style="width: 8.75445%;" valign=top width=159><span class=p3>4\. Ideal observer analysis: Humans vs. ideals. Neurons vs. ideals</span></td> <td style="width: 25.2669%;" valign=top width=159> _4.IdealObserverAnalysis.nb_ </td> <td style="width: 35.6584%;" valign=top width=379> Kersten and Mamassian (2008), Ideal observer theory. The New Encyclopedia of Neuroscience, Squire et al., editors (pdf). Geisler, W. S. (2011). Contributions of ideal observer theory to vision research. Vision Research, 51(7), 771–781. Burgess, A. E., Wagner, R. F., Jennings, R. J., & Barlow, H. B. (1981)_. Efficiency of human visual signal discrimination. Science, 214(4516), 93-94\._ Deneve, S., Latham, P. E., & Pouget, A. (1999). Reading population codes: a neural implementation of ideal observers. Nature Neuroscience, 2(8), 740–745. Measure your absolute efficiency to discriminate dot density using a 2AFC task 2AFCDotDiscriminationDemo.nb </td> <td style="width: 19.6441%;" valign=top width=163> </td> </tr> <tr> <td style="width: 7.47331%;" rowspan=3 width=72> <div><span class=p3>**II. Image formation, pattern synthesis**</span></div> </td> <td style="width: 3.13167%;" align=left> <div><span class=p3>Sep 18</span></div> </td> <td style="width: 8.75445%;" valign=top width=159><span class=p3>5.Psychophysics: tools & techniques</span></td> <td style="width: 25.2669%;" valign=top width=159> _5.Psychophysics.nb[](Lectures/5_PsychophysicsSKEobserver/5_Psychophysics.nb) _ </td> <td style="width: 35.6584%;" valign=top width=379> SKEDetection2AFCInLineDisplay.nb Farell, B. & Pelli, D. G. (1999) Psychophysical methods, or how to measure a threshold and why. In R. H. S. Carpenter & J. G. Robson (Eds.), Vision Research: A Practical Guide to Laboratory Methods, New York: Oxford University (Press.http://psych.nyu.edu/pelli/ Morgenstern, Y., & Elder, J. H. (2012). Local Visual Energy Mechanisms Revealed by Detection of Global Patterns. Journal of Neuroscience, 32(11), 3679–3696\. For a free Matlab psychophysics package, see: [http://psychotoolbox.org](http://psychtoolbox.org/) For a free Python psychophysics package, see: [http://www.psychopy.org](http://www.psychopy.org) </td> <td style="width: 19.6441%;" valign=top width=163> </td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Sep 23</span></div> </td> <td style="width: 8.75445%;" valign=top width=159><span class=p3>6\. Bayesian decision theory & perception</span></td> <td style="width: 25.2669%;" valign=top width=159> _6.BayesDecisionTheory.nb _ _Geisler, W. S., & Kersten, D. (2002). Illusions, perception and Bayes. Nat Neurosci, 5(6), 508-510\. ([pdf](http://gandalf.psych.umn.edu/%7Ekersten/kersten-lab/papers/GeislerKerstennn0602-508.pdf))_ </td> <td style="width: 35.6584%;" valign=top width=379> [EV Section 3](../../coursepapers/YuilleKerstenFinalChapter2016.pdf#3) </td> <td style="width: 19.6441%;" valign=top width=163><span class=p3> </span></td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Sep 25</span></div> </td> <td style="width: 8.75445%;" valign=top width=159><span class=p3>7\. Limits to spatial resolution, image modeling, introduction to linear systems</span></td> <td style="width: 25.2669%;" valign=top width=159> _7.ImageModelLinearSystems.nb _ _Campbell, F. W., & Green, D. (1965). Optical and retinal factors affecting visual resolution. Journal of Physiology (Lond.), 181, 576-593\. ([pdf](../../coursepapers/CampbellGreen_JP1965.pdf))_ </td> <td style="width: 35.6584%;" valign=top width=379> Williams, D. R. (1986). Seeing through the photoreceptor mosaic. 9(5), 193-197.  LinearAlgebraReview.nb Convolutions_Tutorial.nb IPython convolutions notebook </td> <td style="width: 19.6441%;" valign=top width=163>     Upload Assignment #2 to [Canvas](https://ay17.moodle.umn.edu/course/view.php?id=4100) Problem_Set_2.nb </td> </tr> <tr> <td style="width: 7.47331%;" rowspan=5 width=72> <div><span class=p3>**III. Early visual coding**</span></div> </td> <td style="width: 3.13167%;" align=left> <div><span class=p3>Sep 30</span></div> </td> <td style="width: 8.75445%;" valign=top width=159><span class=p3>8\. Linear systems analysis</span></td> <td style="width: 25.2669%;" valign=top width=159><span class=p3>_8.LinearSystemsOptics.nb _</span></td> <td style="width: 35.6584%;" valign=top width=379> EV: Section 2 _CSF.gif_ Tutorials: Fourier_neural_image.nb </td> <td style="width: 19.6441%;" valign=top width=163> </td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Oct 2 </span></div> </td> <td style="width: 8.75445%;" valign=top width=159><span class=p3>9\. Features and filters. Spatial filter models of early human vision</span></td> <td style="width: 25.2669%;" valign=top width=159> _9.NeuralSpatialFiltering.nb _ </td> <td style="width: 35.6584%;" valign=top width=379> Campbell, F. W., & Robson, J. R. (1968). Application of Fourier Analysis to the Visibility of Gratings. Journal of Physiology 197, 551-566.  De Valois, R. L., Albrecht, D. G., & Thorell, L. G. (1982). Spatial frequency selectivity of cells in macaque visual cortex. Vision Res, 22(5), 545-559.  Watson, A. B. (1987). Efficiency of a model human image code. J Opt Soc Am A, 4(12), 2401-2417.  IPython demo of gabor filtering Steerable pyramids: http://www.cns.nyu.edu/~eero/steerpyr/ </td> <td style="width: 19.6441%;" valign=top width=163> </td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Oct 7 </span></div> </td> <td style="width: 8.75445%;" valign=top width=159><span class=p3>10\. Features and filters. Local processing & image analysis</span></td> <td style="width: 25.2669%;" valign=top width=159><span class=p3>_10.ImageProcessing.nb _ Gollisch, T., & Meister, M. (2010). Eye Smarter than Scientists Believed: Neural Computations in Circuits of the Retina. Neuron, 65(2), 150–164. </span></td> <td style="width: 35.6584%;" valign=top width=379><span class=p3>Albrecht, D. G., De Valois, R. L., & Thorell, L. G. (1980). Visual cortical neurons: are bars or gratings the optimal stimuli? Science, 207(4426), 88-90. Adelson, E. H., & Bergen, J. R. (1991). The plenoptic function and the elements of early vision. In M. S. Landy & J. A. Movshon (Eds.), Computational Models of Visual Processing. Cambridge, MA: The MIT Press: A Bradford Book. ClassificationImage demo ([ReverseCorrelation.nb](Reverse%20correlation/ReverseCorrelation.nb)) Ahumada, A. J., Jr. (2002). Classification image weights and internal noise level estimation. J Vis, 2(1), 121-131\. ([pdf)](Reverse%20correlation/Ahumada-2002-jov-2-1-8.pdf)</span></td> <td style="width: 19.6441%;" valign=top width=163> Upload Assignment 3 to [Canvas](https://ay17.moodle.umn.edu/course/view.php?id=4100) Problem_Set_3.nb [](Grading,%20Exercises%20&%20Exams/Assignments_due/Assignmt_2_Convolve.nb) </td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Oct 9</span></div> </td> <td style="width: 8.75445%;" valign=top width=159><span class=p3>11\. Coding efficiency: Retina</span></td> <td style="width: 25.2669%;" valign=top width=159> _11.CodingEfficiency.nb _ Geisler, W. S. (2008). Visual perception and the statistical properties of natural scenes. Annu Rev Psychol, 59, 167-192\. </td> <td style="width: 35.6584%;" valign=top width=379> Laughlin, S. (1981). A simple coding procedure enhances a neuron's information capacity. Z Naturforsch [C], 36(9-10), 910-912. Atick, J. J., & Redlich, A. N. (1992). What does the retina know about natural scenes? Neural Computation, 4(2), 196–210\. Meister, M., & Berry, M. J., 2nd. (1999). The neural code of the retina. Neuron, 22(3), 435-450.[(pdf](../../coursepapers/meister+berry_99.pdf)) Srinivasan, M. V., Laughlin, S. B., & Dubs, A. (1982). Predictive coding: a fresh view of inhibition in the retina. Proc R Soc Lond B Biol Sci, 216(1205), 427-459. IPython demo of natural image statistics </td> <td style="width: 19.6441%;" valign=top width=163> </td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Oct 14</span></div> </td> <td style="width: 8.75445%;" valign=top width=159><span class=p3>12\. Coding efficiency: Cortex </span></td> <td style="width: 25.2669%;" valign=top width=159> _12.SpatialCodingEfficiency.nb _ _Simoncelli, E. P., & Olshausen, B. A. (2001). Natural image statistics and neural representation. Annu Rev Neurosci, 24, 1193-1216._ </td> <td style="width: 35.6584%;" valign=top width=379> ContrastNormalizationNotes.nb Laughlin, S. B., de Ruyter van Steveninck, R. R., & Anderson, J. C. (1998). The metabolic cost of neural information. Nat Neurosci, 1(1), 36-41. Lennie, P. (2003). The cost of cortical computation. Curr Biol, 13(6), 493-497.[](../../coursepapers/LennieCurBio2003.pdf) Multi-resolution, image pyramids, and efficient coding: JepsonFleet2005pyramids_notes.pdf AdelsonPyramidRCA84.pdf </td> <td style="width: 19.6441%;" valign=top width=163> </td> </tr> <tr> <td style="width: 7.47331%;" rowspan=11 width=72> <div><span class=p3>**IV. Intermediate-level vision, integration, grouping**</span></div> </td> <td style="width: 3.13167%;" align=left> <div><span class=p3>Oct 16</span></div> </td> <td style="width: 8.75445%;" valign=top width=159><span class=p3>13\. Edge detection</span></td> <td style="width: 25.2669%;" valign=top width=159><span class=p3>13.EdgeDetection.nb </span></td> <td style="width: 35.6584%;" valign=top width=379> Hubel, D. H., & Wiesel, T. N. (1977). Ferrier lecture. Functional architecture of macaque monkey visual cortex. Proc R Soc Lond B Biol Sci, 198(1130), 1-59. IPython demo of statistical edge detection </td> <td style="width: 19.6441%;" valign=top width=163> </td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Oct 21</span></div> </td> <td style="width: 8.75445%;" valign=top><span class=p3>14\. Objects and scenes from images. The visual cortical pathways and hierarchy.</span></td> <td style="width: 25.2669%;" valign=top> _14.ScenesfromImages.nb __ von der Heydt R (2003) Image parsing mechanisms of the visual cortex. In: The Visual Neurosciences (Werner JS, Chalupa LM, eds.), pp 1139-1150\. Cambridge, Mass.: MIT press._ Kersten, D. J., & Yuille, A. L. (2014). Inferential Models of the Visual Cortical Hierarchy. In M. S. Gazzaniga & G. R. Mangun (Eds.), The New Cognitive Neurosciences, 5th Edition (pp. 1â€“22). MIT Press.[](http://vision.psych.umn.edu/users/kersten//kersten-lab/coursepapers/KerstenYuilleNewCogNeuro2014.pdf) </td> <td style="width: 35.6584%;" valign=top><span class=p3>Zhou H, Friedman HS, von der Heydt R (2000) Coding of border ownership in monkey visual cortex. J Neuroscience 20: 6594-6611. </span></td> <td style="width: 19.6441%;" valign=top width=163> </td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Oct 23</span></div> </td> <td style="width: 8.75445%;" valign=top width=159><span class=p3>15\. Scene-based generative models</span></td> <td style="width: 25.2669%;" valign=top width=159> _15.SurfaceGeometryDepth.nb _ _Kersten, D., Mamassian, P., & Yuille, A. (2004). Object perception as Bayesian Inference. Annual Review of Psychology, 55, 271-304. _ </td> <td style="width: 35.6584%;" valign=top width=379></td> <td style="width: 19.6441%;" valign=top width=163></td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Oct 28</span></div> </td> <td class=p3 style="width: 8.75445%;" valign=top width=159><span class=p3>16\. Shape-from-X</span></td> <td style="width: 25.2669%;" valign=top width=159><span class=p3>_16.ShapeFromX.nb _</span></td> <td style="width: 35.6584%;" valign=top width=379> Reflectance map: Shape from shading: Horn BKP (1986) Robot Vision. Cambridge MA: MIT Press. Ch 11  Barron, J. T., & Malik, J. (2015). Shape, Illumination, and Reflectance from Shading. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(8), 1670â€“1687\. http://doi.org/10.1109/TPAMI.2014.2377712 Belhumeur, P. N., Kriegman, D. J., & Yuille, A. (1997). The Bas-Relief Ambiguity. ([pdf](https://pdfs.semanticscholar.org/71ac/0dc7634e4a56fd41dfac270ec5883fcbb44f.pdf)) Johnson, M. K., & Adelson, E. H. (2011). Shape Estimation in Natural Illumination. Computer Vision and Pattern Recognition (CVPR), 2553–2560. Muryy, A. A., Welchman, A. E., Blake, A., & Fleming, R. W. (2013). Specular reflections and the estimation of shape from binocular disparity. Proceedings of the National Academy of Sciences of the United States of America, 110(6), 2413–2418. cube.mov random.mov </td> <td style="width: 19.6441%;" valign=top width=163></td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3> Oct 30</span></div> </td> <td class=p3 style="width: 8.75445%;" valign=top width=159> 17\. Shape from shading Overview of python/ipython for computational vision[ ](https://wakari.io/sharing/bundle/kersten/Lect_19Intro_Python) </td> <td style="width: 25.2669%;" valign=top width=159> _17\. Shape from shading.nb_ Lect_17Intro_Python.ipynb (source) <span class=p3>_17\. IPython notebook_</span> Demos <span class=p3>by Weichao Qiu and Dan Kersten, supplement to _**Early Vision**._ Yuille and Kersten. A chapter in _From Neuron to Cognition via Computational Neuroscience_, M.A. Arbib, James J. Bonaiuto Editors, Cambridge MA: The MIT Press, in 2016</span> </td> <td style="width: 35.6584%;" valign=top width=379> [Anaconda](https://store.continuum.io/cshop/anaconda/) python installation recommended. We will use [Juypter/IPython](http://ipython.org), a browser-based notebook interface for python. See [here](http://nbviewer.ipython.org/github/ipython/ipython/blob/1.x/examples/notebooks/Part%205%20-%20Rich%20Display%20System.ipynb) for illustrations of IPython cell types, and [here](https://github.com/ipython/ipython/wiki/A-gallery-of-interesting-IPython-Notebooks) for a collection of sample notebooks. Look [here](http://iacs-courses.seas.harvard.edu/courses/am207/blog/installing-python.html) for some good tips on installation, as well as the parent directory for excellent ipython-based course material on scientific computing using Monte Carlo methods. For a quick start to scientific programming, see: [http://nbviewer.ipython.org/gist/rpmuller/5920182](http://nbviewer.ipython.org/gist/rpmuller/5920182) For a comphrensive coverage of scientific python see[:https://scipy-lectures.github.io](https://scipy-lectures.github.io) And for a ground-up set of tutorials on python see: [http://learnpythonthehardway.org/book/](http://learnpythonthehardway.org/book/) Switching from matlab to python? [http://wiki.scipy.org/NumPy_for_Matlab_User](http://mathesaurus.sourceforge.net/matlab-numpy.html) ProjectIdeasF2015.nb </td> <td style="width: 19.6441%;" valign=top width=163></td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Nov 4</span></div> </td> <td class=p3 style="width: 8.75445%;" valign=top width=159><span class=p3>18\. Motion: optic flow</span></td> <td style="width: 25.2669%;" valign=top width=159> _18.MotionOpticFlow.nb _ _OpenCV python demo:_ [OpticFlowSparse.ipynb](http://nbviewer.ipython.org/url/gandalf.psych.umn.edu/users/kersten/kersten-lab/courses/Psy5036W2015/Lectures/18.MotionOpticFlow/OpticFlowSparse.ipynb) needs: [648aa10.avi](Lectures/17_PythonForVision/648aa10.avi) </td> <td style="width: 35.6584%;" valign=top width=379> Horn, B. K. P., & Schunck, B. G. (1981). Determining Optical Flow. Artificial Intelligence, 17, 185-203.  Optic Flow (2013) Florian Raudies, Scholarpedia, 8(7):30724\. doi:10.4249/scholarpedia.30724  (with available matlab code) Optic flow matlab code from Michael Black's lab.  Borst, A. (2007). Correlation versus gradient type motion detectors: the pros and cons. Philos Trans R Soc Lond B Biol Sci, 362(1479), 369-374. [http://web.mit.edu/persci/people/adelson/illusions_demos.html](http://web.mit.edu/persci/people/adelson/illusions_demos.html) [IPython aperture demo](http://nbviewer.ipython.org/url/gandalf.psych.umn.edu/users/kersten/kersten-lab/courses/Psy5036W2015/Lectures/17_PythonForVision/Aperture%20demo.ipynb) EV: Section 2.4 FV: Chapter 10 </td> <td style="width: 19.6441%;" valign=top width=163></td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Nov 6</span></div> </td> <td class=p3 style="width: 8.75445%;" valign=top width=159><span class=p3>19\. Motion: biological, human perception</span></td> <td style="width: 25.2669%;" valign=top width=159> _19.MotionHumanPerception.nb _ _Weiss, Y., Simoncelli, E. P., & Adelson, E. H. (2002). Motion illusions as optimal percepts. Nat Neurosci, 5(6), 598-604. ([pdf](../../coursepapers/WeissSimonAdelNatNeu2002.pdf))_ </td> <td style="width: 35.6584%;" valign=top width=379> Heeger, D. J., Simoncelli, E. P., & Movshon, J. A. (1996). Computational models of cortical visual processing. Proc Natl Acad Sci U S A, 93(2), 623-627\. ([pdf](../../coursepapers/Heeger96-reprint%20PNAS.pdf)) [http://demonstrations.wolfram.com/DisappearingDotIllusion/](http://demonstrations.wolfram.com/DisappearingDotIllusion/) [http://www.biomotionlab.ca/Demos/BMLwalker.html](http://www.biomotionlab.ca/Demos/BMLwalker.html) EV: Section 4.4 FV: Chapter 10 </td> <td style="width: 19.6441%;" valign=top width=163> </td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Nov 11</span></div> </td> <td class=p3 style="width: 8.75445%;" valign=top width=159><span class=p3>20\. Material perception</span></td> <td style="width: 25.2669%;" valign=top width=159> _20.SurfaceMaterial.nb _ V1 and lightness Doerschner, K., Fleming, R. W., Yilmaz, O., Schrater, P. R., Hartung, B., & Kersten, D. (2011). Visual motion and the perception of surface material. Current Biology, 21(23), 2010–2016. </td> <td style="width: 35.6584%;" valign=top width=379><span class=p3>Fleming, R. W., Dror, R. O., & Adelson, E. H. (2003). Real-world illumination and the perception of surface reflectance properties. J Vis, 3(5), 347-368\. _Adelson, E. H. (1993). Perceptual organization and the judgment of brightness. Science, 262, 2042-2044 _ Boyaci, H., Fang, F., Murray, S. O., & Kersten, D. (2007). Responses to lightness variations in early human visual cortex. Curr Biol, 17(11), 989-993 ([pdf](../../coursepapers/BoyaciCurrBiol2007.pdf))_[http://www.bilkent.edu.tr/~hboyaci/Vision/](http://www.bilkent.edu.tr/%7Ehboyaci/Vision/)_ [http://web.mit.edu/persci/people/adelson/checkershadow_illusion.html](http://web.mit.edu/persci/people/adelson/checkershadow_illusion.html) [http://gandalf.psych.umn.edu/users/kersten/kersten-lab/demos/transparency.html](http://gandalf.psych.umn.edu/users/kersten/kersten-lab/demos/transparency.html) [http://gandalf.psych.umn.edu/~kersten/kersten-lab/demos/MatteOrShiny.html](http://gandalf.psych.umn.edu/%7Ekersten/kersten-lab/demos/MatteOrShiny.html)</span></td> <td style="width: 19.6441%;" valign=top width=163> Upload Assignment 5 to [Canvas](https://ay17.moodle.umn.edu/course/view.php?id=4100) texture_classification_plot_gabor.ipynb Upload Final project title & paragraph outline to [C](https://ay17.moodle.umn.edu/course/view.php?id=4100)anvas </td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Nov 13</span></div> </td> <td class=p3 style="width: 8.75445%;" valign=top width=159><span class=p3>21\. Texture.</span></td> <td style="width: 25.2669%;" valign=top width=159> _21.Texture.nb _ Freeman, J., & Simoncelli, E. P. (2011). Metamers of the ventral stream. Nature Publishing Group, 14(9), 1195-1201\. http://doi.org/10.1038/nn.2889  </td> <td style="width: 35.6584%;" valign=top width=379> Heeger DJ and Bergen JR, Pyramid Based Texture Analysis/Synthesis, Computer Graphics Proceedings, p. 229-238, 1995\. ([pdf)](../../coursepapers/heeger-siggraph95.pdf). EfrosTextureSynthesis.ipynb From: [https://github.com/rbaravalle/efros](https://github.com/rbaravalle/efros) [img2.png](Lectures/21.%20Texture/img2.png) A sample: [out2.png](Lectures/21.%20Texture/out2.png) </td> <td style="width: 19.6441%;" valign=top> </td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Nov 18</span></div> </td> <td class=p3 style="width: 8.75445%;" align=center valign=top><span class=p3>22.Science writing (Thanksgiving week)</span></td> <td style="width: 25.2669%;" valign=top><span class=p3>_22.ScienceWriting.nb_</span></td> <td style="width: 35.6584%;" valign=top> Gopen & Swan, 1990 ([pdf](../../coursepapers/GopenSwan1990.pdf)) [UM Psychology](http://writing.psych.umn.edu/student-resources) </td> <td style="width: 19.6441%;" valign=top width=163> </td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Nov 20  </span></div> </td> <td class=p3 style="width: 8.75445%;" valign=top><span class=p3>23.Perceptual integration</span></td> <td style="width: 25.2669%;" valign=top><span class=p3>_23.PerceptualIntegration.nb _</span></td> <td style="width: 35.6584%;" valign=top> McDermott, J., Weiss, Y., & Adelson, E. H. (2001). Beyond junctions: nonlocal form constraints on motion interpretation. Perception, 30(8), 905-923\. ([pdf](../../coursepapers/McDermottbeyond_junctions.pdf)) [http://www.perceptionweb.com/perception/perc0801/square.html](http://www.perceptionweb.com/perception/perc0801/square.html) Hillis, J. M., Ernst, M. O., Banks, M. S., & Landy, M. S. (2002). Combining sensory information: mandatory fusion within, but not between, senses. Science, 298(5598), 1627-1630.([pdf](../../coursepapers/HillisErnstBanksLandyscience02.pdf)) Ernst, M. O., & Banks, M. S. (2002). Humans integrate visual and haptic information in a statistically optimal fashion. Nature, 415(6870), 429-433.  Stocker, A. A., & Simoncelli, E. (2008). A Bayesian model of conditioned perception. Advances in Neural Information Processing Systems, 20, 1409-1416.  [IPython demo of ideal integration](http://nbviewer.ipython.org/url/gandalf.psych.umn.edu/users/kersten/kersten-lab/courses/Psy5036W2015/ArbibFinalChapterJun17_2015/Demos/ipynb/5.Cue%20Combination.ipynb) EV: Section 5 </td> <td style="width: 19.6441%;" valign=top> </td> </tr> <tr> <td style="width: 7.47331%;" rowspan=4 width=72> <div><span class=p3>**V. High-level vision**</span></div> </td> <td style="width: 3.13167%;" align=left> <div><span class=p3>Nov 25</span></div> </td> <td class=p3 style="width: 8.75445%;" valign=top><span class=p3>24\. Object recognition I</span></td> <td style="width: 25.2669%;" valign=top> _24.ObjectRecognition.nb _ _DiCarlo, J. J., Zoccolan, D., & Rust, N. C. (2012). How does the brain solve visual object recognition? Neuron, 73(3), 415–434. _ </td> <td style="width: 35.6584%;" valign=top width=379> Liu, Z., Knill, D. C., & Kersten, D. (1995). Object Classification for Human and Ideal Observers. Vision Research, 35(4), 549-568\. ([pdf)](../../papers/LiuKnillKersten95.pdf) Tjan, B., Braje, W., Legge, G. E., & Kersten, D. (1995). Human efficiency for recognizing 3-D objects in luminance noise. Vision Research, 35(21), 3053-3069.  Tanaka K (2003) Columns for complex visual object features in the inferotemporal cortex: clustering of cells with similar but slightly different stimulus selectivities. Cerebral cortex 13:90-99. Serre, T., Oliva, A., & Poggio, T. (2007). A feedforward architecture accounts for rapid categorization. Proc Natl Acad Sci U S A, 104(15), 6424-6429\. Yamins, D. L. K., Hong, H., Cadieu, C. F., Solomon, E. A., Seibert, D., & DiCarlo, J. J. (2014). Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proceedings of the National Academy of Sciences of the United States of America, 111(23), 8619-8624.  </td> <td style="width: 19.6441%;" valign=top width=163></td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Nov 27</span></div> </td> <td class=p3 style="width: 8.75445%;" valign=top width=159><span class=p3>25\. Object recognition II feeforward architectures</span></td> <td style="width: 25.2669%;" valign=top width=159><span class=p3>_25_Bidirectional_I.key.pdf  __ Ullman, S., Vidal-Naquet, M., & Sali, E. (2002). Visual features of intermediate complexity and their use in classification. Nat Neurosci, 5(7), 682-687.  _</span></td> <td style="width: 35.6584%;" valign=top width=379> Grill-Spector, K. (2003). The neural basis of object perception. Curr Opin Neurobiol, 13(2), 159-166. Rao, R. P., & Ballard, D. H. (1999). Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nat Neurosci, 2(1), 79-87.  Bullier, J. (2001). Integrated model of visual processing. Brain Res Brain Res Rev, 36(2-3), 96-107.  Tenenbaum JB: Bayesian modeling of human concept learning. In Advances in Neural Information Processing Systems. Edited by Kearns MSS, Solla A, Cohn DA: Cambridge, MA: MIT Press: 1999. </td> <td style="width: 19.6441%;" valign=top width=163></td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Dec 2 </span></div> </td> <td class=p3 style="width: 8.75445%;" valign=top width=159>26\. Object recognition III feedback architectures</td> <td style="width: 25.2669%;" valign=top width=159> _26_BidirectionalFeedback.key.pdf_ [(pdf](Lectures/26.%20ObjectRecognition_III/26_BidirectionalFeedback.pdf)) _ <span style="font-family: Arial;"> </span>_ </td> <td style="width: 35.6584%;" valign=top width=379> Torralba, A., Oliva, A., Castelhano, M. S., & Henderson, J. M. (2006). Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search. Psychol Rev, 113(4), 766-786. Chikkerur, S., Serre, T., Tan, C., & Poggio, T. (2010). What and where: A Bayesian inference theory of attention. Vision Research, 50(22), 2233–2247. </td> <td style="width: 19.6441%;" valign=top width=163> Upload Assignment 6 to [Canvas](https://ay17.moodle.umn.edu/course/view.php?id=4100) Problem_Set_6 .nb </td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Dec 4</span></div> </td> <td class=p3 style="width: 8.75445%;" valign=top> 27\. Empirical evidence for bidirectional computations </td> <td style="width: 25.2669%;" valign=top> 27.EmpiricalEvidenceBidirectionalProcessing </td> <td style="width: 35.6584%;" valign=top> Longuet-Higgins, H. C., & Prazdny, K. (1980). The Interpretation of a Moving Retinal Image. Proceedings of the Royal Society of London B, 208, 385-397.  Horn BKP (1986) Robot Vision. Cambridge MA: MIT Press., chapter 17  Schrater PR, Kersten D (2000) How optimal depth cue integration depends on the task. International Journal of Computer Vision 40:73-91.  </td> <td style="width: 19.6441%;" valign=top><span class=p3>Upload a complete DRAFT of FINAL PROJECT to [Canvas](https://ay17.moodle.umn.edu/course/view.php?id=4100) by _** December 4**__**th.**_</span></td> </tr> <tr> <td style="width: 7.47331%;" rowspan=3 width=72> </td> <td style="width: 3.13167%;" align=left> <div><span class=p3>Dec 9</span></div> </td> <td class=p3 style="width: 8.75445%;" valign=top>28\. Vision for action, spatial layout, heading. Homegeneous coordinates.</td> <td style="width: 25.2669%;" valign=top> _28.SpatialLayoutScenes.nb _ _Kalman filter notes (pdf)_ </td> <td style="width: 35.6584%;" valign=top> </td> <td style="width: 19.6441%;" valign=top><span class=p3>Upload your peer comments to Canvas by Monday Dec 11th</span></td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Dec 11</span></div> <span class=p3>(Last day of class) </span></td> <td class=p3 style="width: 8.75445%;" valign=top width=159> </td> <td style="width: 25.2669%;" valign=top width=159><span class=p3>_In Class Project Presentations_</span></td> <td style="width: 35.6584%;" valign=top width=379> </td> <td style="width: 19.6441%;" valign=top width=163> Drafts returned to you with Instructor comments </td> </tr> <tr> <td style="width: 3.13167%;" align=left> <div><span class=p3>Dec 21</span></div> </td> <td class=p3 style="width: 8.75445%;" valign=top> </td> <td style="width: 25.2669%;" valign=top> </td> <td style="width: 35.6584%;" valign=top> </td> <td style="width: 19.6441%;" valign=top><span id=R class=p3>Upload Final Revised Draft of Project to Canvas.</span></td> </tr> </tbody> </table> <hr> <p><span style="font-family: Arial;"><a name=FinalProject></a><strong>Final Project Assignment</strong>.</span></p> <p><span style="font-family: Arial;">Goal: This course integrates the behavioral, neural and computational principles of perception. Students often find the interdisciplinary integration to be the most challenging aspect of the course. Through writing, you will learn to synthesize results from diverse and typically isolated disciplines. By writing about your project work, you will learn to think through the broader implications of your project, and to effectively communicate the rationale and results of your contribution in words. You will do a final page research report in which you will describe, in the form of a scientific paper, the results of an original computer program on a topic in computational vision.</span></p> <p><span style="font-family: Arial;">Your final project will involve: 1) a computer program and; 2) a 2000-3000 word final paper describing your project. For your computer project, you will do one of the following: 1) Write a program to simulate a model from the computer vision literature ; 2) Design and program a method for solving some problem in perception. 3) Design and program a psychophysical experiment to study an aspect of human visual perception. The results of your final project should be written up in the form of a short scientific paper or Mathematica Notebook, describing the motivation, methods, results, and interpretation.</span></p> <p>If you choose to write your program in Mathematica, your paper and program can be combined can be formated as a Mathematica notebook. See: <a href=http://www.wolfram.com/solutions/publishing/tutorials.html>Books and Tutorials on Notebooks.</a> If you do your final project using Python, you can turn your paper in as a <a href=https://jupyter.org>Jupyter</a> notebook.</p> <p><span style="font-family: Arial;">Your paper will be critiqued and returned for you to revise and resubmit in final form. You should write for an audience consisting of your class peers.</span></p> <p>Completing the final paper involves 4 steps. Each step requires that you email a document to the teaching assistant.</p> <ol> <li> <p><strong>Outline</strong> <strong>(2% of grade)**</strong>.** You will submit a working title and paragraph outline by the deadline noted in the syllabus. These outlines will be critiqued in order to help you find an appropriate focus for your papers. (Consult with the instructor or TA for ideas well ahead of time).</p> </li> <li> <p><strong>Complete draft (5% of grade).</strong> A double-spaced, complete draft of the paper must be turned in by the deadline noted in the syllabus. Papers should be between <strong>2000</strong> and <strong>3000</strong> words. In addition to the title, author and date lines, papers must include the following sections: Abstract, Introduction, Methods, Results, Discussion, and Bibliography. Use citations to motivate your problem and to justify your claims. Cite authors by name and date, e.g. (Marr &amp; Poggio, 1979). <em>Citations should be original sources, not wikipedia.</em> Use a standard citation format, such as APA. (The UM library has information on <a href="http://www.lib.umn.edu/libdata/page_print.phtml?page_id=791">style guides</a>, and in particular <a href=http://tutorial.lib.umn.edu/infomachine4051.html>APA style</a>.) Papers must be typed, with a page number on each page. Figures should be numbered and have figure captions. This draft will be reviewed by your instructor and one of you class peers. <strong>The point break down for the total 5% is: 2 pts for completing Introduction, 2 pts for completing Methods, 1 pt for completing Discussion)</strong></p> </li> <li> <p><strong>Peer commentary (5% of grade)</strong>. You will submit a written commentary <strong>(200</strong> to <strong>500</strong> words) on a complete draft of one of your class peers. The project drafts and commentaries will be anonymous. The commentary should provide feedback to improve the quality and clarity of the writing.</p> </li> <li> <p><strong>Final draft (20% of grade) and "Cover letter" (8% of grade).</strong> The final draft must be turned in by the date noted on the syllabus. The "Cover letter" should describe how your revision addressed comments from your peer evaluator and from your instructor. It should itemize key criticisms together with a brief description of the changes you made to your draft manuscript.</p> </li> <li> <p><strong>Some Resources:</strong></p> </li> <li> <p>Student Writing Support: Center for Writing, 306b Lind Hall and satellite locations (612.625.1893) <a href=http://writing.umn.edu>http://writing.umn.edu</a>.</p> <p><em>NOTE: Plagiarism, a form of scholastic dishonesty and a disciplinary offense, is described by the Regents as follows: Scholastic dishonesty means plagiarizing; cheating on assignments or examinations; engaging in unauthorized collaboration on academic work; taking, acquiring, or using test materials without faculty permission; submitting false or incomplete records of academic achievement; acting alone or in cooperation with another to falsify records or to obtain dishonestly grades, honors, awards, or professional endorsement; altering, forging, or misusing a University academic record; or fabricating or falsifying data, research procedures, or data analysis. <a href=http://www1.umn.edu/regents/policies/academic/Code_of_Conduct.html>http://www1.umn.edu/regents/policies/academic/Code_of_Conduct.html.</a> See too: <a href=http://writing.umn.edu/tww/plagiarism/ >http://writing.umn.edu/tww/plagiarism/</a> and</em><a href=http://writing.umn.edu/tww/plagiarism/definitions.html>http://writing.umn.edu/tww/plagiarism/definitions.html</a></p> </li> </ol> <p><span style="font-family: Arial;"><span style="font-family: Arial;"><span style="font-family: Arial;"><span style="font-family: Arial;"><span style="font-family: Arial;"><a href=http://privacy.umn.edu/ >Privacy Statement</a></span></span></span></span></span></p> </article> </div> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> &copy; 2020 Regents of the University of Minnesota. All rights reserved. The University of Minnesota is an equal opportunity educator and employer. <br> <a href=http://privacy.umn.edu>Privacy Statement</a> </div> </div> <div class=md-footer-social> <link rel=stylesheet href=../../assets/fonts/font-awesome.css> <a href="https://scholar.google.com/citations?user=6j1ZjJsAAAAJ&hl=en&oi=ao" target=_blank rel=noopener title=google class="md-footer-social__link fa fa-google"></a> <a href=https://github.com/danielkersten/ target=_blank rel=noopener title=github class="md-footer-social__link fa fa-github"></a> </div> </div> </div> </footer> </div> <script src=../../assets/javascripts/application.9368ffdd.js></script> <script>app.initialize({version:"1.1",url:{base:"../.."}})</script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script> </body> </html>