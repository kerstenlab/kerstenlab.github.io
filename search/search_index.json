{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kersten Computational Vision Lab \u00b6 The Computational Vision Lab combines computational theory with behavioral and brain image experiments to understand how we see the world around us. Read more about our research and publications . Recent Publications \u00b6 Vizioli, L., De Martino, F., Petro, L. S., Kersten, D., Ugurbil, K., Yacoub, E., & Muckli, L. (2019). Multivoxel pattern of blood oxygen level dependent activity can be sensitive to stimulus specific fine scale responses. bioRxiv . https://doi.org/10.1101/798306 Peterson, L. M., Kersten, D. J., & Mannion, D. J. (2018). Surface curvature from kinetic depth can affect lightness. Journal of Experimental Psychology: Human Perception and Performance , 44 (12), 1856. https://doi.org/10.1037/xhp0000575 Morgenstern, Y., & Kersten, D. J. (2017). The perceptual dimensions of natural dynamic flow. Journal of Vision , 17 (12), 7\u20137. https://doi.org/10.1167/17.12.7 High-impact publications \u00b6 Kersten, D., Mamassian, P., & Yuille, A. (2004). Object perception as bayesian inference. Annu. Rev. Psychol. , 55 , 271\u2013304. https://doi.org/10.1146/annurev.psych.55.090902.142005 Yuille, A., & Kersten, D. (2006). Vision as bayesian inference: Analysis by synthesis? Trends in Cognitive Sciences , 10 (7), 301\u2013308. https://doi.org/10.1016/j.tics.2006.05.002 Murray, S. O., Kersten, D., Olshausen, B. A., Schrater, P., & Woods, D. L. (2002). Shape perception reduces activity in human primary visual cortex. Proceedings of the National Academy of Sciences , 99 (23), 15164\u201315169. https://doi.org/10.1073/pnas.192579399 Murray, S. O., Boyaci, H., & Kersten, D. (2006a). The representation of perceived angular size in human primary visual cortex. Nature Neuroscience , 9 (3), 429\u2013434. https://doi.org/10.1038/nn1641","title":"Home"},{"location":"#kersten-computational-vision-lab","text":"The Computational Vision Lab combines computational theory with behavioral and brain image experiments to understand how we see the world around us. Read more about our research and publications .","title":"Kersten Computational Vision Lab"},{"location":"#recent-publications","text":"Vizioli, L., De Martino, F., Petro, L. S., Kersten, D., Ugurbil, K., Yacoub, E., & Muckli, L. (2019). Multivoxel pattern of blood oxygen level dependent activity can be sensitive to stimulus specific fine scale responses. bioRxiv . https://doi.org/10.1101/798306 Peterson, L. M., Kersten, D. J., & Mannion, D. J. (2018). Surface curvature from kinetic depth can affect lightness. Journal of Experimental Psychology: Human Perception and Performance , 44 (12), 1856. https://doi.org/10.1037/xhp0000575 Morgenstern, Y., & Kersten, D. J. (2017). The perceptual dimensions of natural dynamic flow. Journal of Vision , 17 (12), 7\u20137. https://doi.org/10.1167/17.12.7","title":"Recent Publications"},{"location":"#high-impact-publications","text":"Kersten, D., Mamassian, P., & Yuille, A. (2004). Object perception as bayesian inference. Annu. Rev. Psychol. , 55 , 271\u2013304. https://doi.org/10.1146/annurev.psych.55.090902.142005 Yuille, A., & Kersten, D. (2006). Vision as bayesian inference: Analysis by synthesis? Trends in Cognitive Sciences , 10 (7), 301\u2013308. https://doi.org/10.1016/j.tics.2006.05.002 Murray, S. O., Kersten, D., Olshausen, B. A., Schrater, P., & Woods, D. L. (2002). Shape perception reduces activity in human primary visual cortex. Proceedings of the National Academy of Sciences , 99 (23), 15164\u201315169. https://doi.org/10.1073/pnas.192579399 Murray, S. O., Boyaci, H., & Kersten, D. (2006a). The representation of perceived angular size in human primary visual cortex. Nature Neuroscience , 9 (3), 429\u2013434. https://doi.org/10.1038/nn1641","title":"High-impact publications"},{"location":"contact/","text":"kersten@umn.edu Lab Location N13 Elliott Hall 75 East River Road Minneapolis, MN 55455 Call Lab Phone: 612-625-1337 for access. Once in Elliott Hall, take the elevator on the North end of the building to the basement. The lab is in room N13.","title":"Contact"},{"location":"people/","text":"People \u00b6 Director \u00b6 Daniel J Kersten, PhD Department of Psychology N218 Elliott Hall kersten@umn.edu scholar | homepage Graduate Students \u00b6 Doug Addleman Department of Psychology N218 Elliott Hall addle005@umn.edu scholar | homepage Alexander Bratch Department of Psychology N218 Elliott Hall bratc006@umn.edu scholar Yijun Ge Department of Psychology N218 Elliott Hall gexxx119@umn.edu Subhankar Ghosh Computer Science & Engineering ghosh117@umn.edu Jiaqi Liu liu00687@umn.edu Siyun Liu Department of Psychology N218 Elliott Hall liux4433@umn.edu Ziwei Liu Department of Psychology N218 Elliott Hall liu00964@umn.edu Link Swanson Cognitive Science S309 Elliott Hall link@umn.edu scholar | homepage Undergraduates \u00b6 Yixiong Chen chen5256@umn.edu Anusha Duggirala duggi008@umn.edu Lab Alumni \u00b6 Academic genealogy of past and current lab members at Neurotree .","title":"People"},{"location":"people/#people","text":"","title":"People"},{"location":"people/#director","text":"Daniel J Kersten, PhD Department of Psychology N218 Elliott Hall kersten@umn.edu scholar | homepage","title":"Director"},{"location":"people/#graduate-students","text":"Doug Addleman Department of Psychology N218 Elliott Hall addle005@umn.edu scholar | homepage Alexander Bratch Department of Psychology N218 Elliott Hall bratc006@umn.edu scholar Yijun Ge Department of Psychology N218 Elliott Hall gexxx119@umn.edu Subhankar Ghosh Computer Science & Engineering ghosh117@umn.edu Jiaqi Liu liu00687@umn.edu Siyun Liu Department of Psychology N218 Elliott Hall liux4433@umn.edu Ziwei Liu Department of Psychology N218 Elliott Hall liu00964@umn.edu Link Swanson Cognitive Science S309 Elliott Hall link@umn.edu scholar | homepage","title":"Graduate Students"},{"location":"people/#undergraduates","text":"Yixiong Chen chen5256@umn.edu Anusha Duggirala duggi008@umn.edu","title":"Undergraduates"},{"location":"people/#lab-alumni","text":"Academic genealogy of past and current lab members at Neurotree .","title":"Lab Alumni"},{"location":"publications/","text":"Publications \u00b6 Vizioli, L., De Martino, F., Petro, L. S., Kersten, D., Ugurbil, K., Yacoub, E., & Muckli, L. (2019). Multivoxel pattern of blood oxygen level dependent activity can be sensitive to stimulus specific fine scale responses. bioRxiv . https://doi.org/10.1101/798306 Peterson, L. M., Kersten, D. J., & Mannion, D. J. (2018). Surface curvature from kinetic depth can affect lightness. Journal of Experimental Psychology: Human Perception and Performance , 44 (12), 1856. https://doi.org/10.1037/xhp0000575 Morgenstern, Y., & Kersten, D. J. (2017). The perceptual dimensions of natural dynamic flow. Journal of Vision , 17 (12), 7\u20137. https://doi.org/10.1167/17.12.7 Thompson, W. B., Legge, G. E., Kersten, D. J., Shakespeare, R. A., & Lei, Q. (2017). Simulating visibility under reduced acuity and contrast sensitivity. JOSA A , 34 (4), 583\u2013593. https://doi.org/10.1364/josaa.34.000583 Fan, X., Wang, L., Shao, H., Kersten, D., & He, S. (2016). Temporally flexible feedback signal to foveal cortex for peripheral object recognition. Proceedings of the National Academy of Sciences , 113 (41), 11627\u201311632. https://doi.org/10.1073/pnas.1606137113 Kersten, D., & Mamassian, P. (2016). Cast shadow illusion. In A. G. Shapiro & D. Todorovic (Eds.), The oxford compendium of visual illusions (pp. 214\u2013220). https://doi.org/10.1093/acprof:oso/9780199794607.003.0020 Qiu, C., Burton, P. C., Kersten, D., & Olman, C. A. (2016). Responses in early visual areas to contour integration are context dependent. Journal of Vision , 16 (8), 19\u201319. https://doi.org/10.1167/16.8.19 Yuille, A., & Kersten, D. (2016). Early vision. In M. A. Arbib & J. J. Bonaiuto (Eds.), From neuron to cognition via computational neuroscience (pp. 345\u2013408). MIT Press. Green, C. S., Kattner, F., Siegel, M. H., Kersten, D., & Schrater, P. R. (2015). Differences in perceptual learning transfer as a function of training task. Journal of Vision , 15 (10), 5\u20135. https://doi.org/10.1167/15.10.5 Kam, T.-E., Mannion, D. J., Lee, S.-W., Doerschner, K., & Kersten, D. J. (2015). Human visual cortical responses to specular and matte motion flows. Frontiers in Human Neuroscience , 9 , 579. https://doi.org/10.3389/fnhum.2015.00579 Mannion, D. J., Kersten, D. J., & Olman, C. A. (2015). Scene coherence can affect the local response to natural images in human v1. European Journal of Neuroscience , 42 (11), 2895\u20132903. https://doi.org/10.1111/ejn.13082 Akin, B., Ozdem, C., Eroglu, S., Keskin, D. T., Fang, F., Doerschner, K., \u2026 Boyaci, H. (2014). Attention modulates neuronal correlates of interhemispheric integration and global motion perception. Journal of Vision , 14 (12), 30\u201330. https://doi.org/10.1167/14.12.30 Kersten, D., & Yuille, A. (2014a). Inferential models of the visual cortical hierarchy. In M. S. Gazzaniga & G. R. Mangun (Eds.), The cognitive neurosciences (Fifth Edition, pp. 398\u2013404). The MIT Press. Kersten, D., & Yuille, A. (2014b). Vision: Bayesian inference and beyond. In J. S. Werner & L. M. Chalupa (Eds.), The new visual neurosciences (pp. 1263\u20131278). The MIT Press. Mannion, D. J., Kersten, D. J., & Olman, C. A. (2014). Regions of mid-level human visual cortex sensitive to the global coherence of local image patches. Journal of Cognitive Neuroscience , 26 (8), 1764\u20131774. https://doi.org/10.1162/jocn_a_00588 Kersten, D., Shakespeare, R., & Thompson, W. (2013). Predicting visibility in designs of public spaces. University of Utah Technical Reports , UUCS 13-001 . Mannion, D. J., Kersten, D., & Olman, C. A. (2013). Consequences of polar form coherence for fMRI responses in human visual cortex. NeuroImage , 78 , 152\u2013158. https://doi.org/10.1016/j.neuroimage.2013.04.036 McMenamin, B. W., Radue, J., Trask, J., Huskamp, K., Kersten, D., & Marsolek, C. J. (2013). The diagnosticity of color for emotional objects. Motivation and Emotion , 37 (3), 609\u2013622. https://doi.org/10.1007/s11031-012-9319-0 Qiu, C., Kersten, D., & Olman, C. A. (2013). Segmentation decreases the magnitude of the tilt illusion. Journal of Vision , 13 (13), 19\u201319. https://doi.org/10.1167/13.13.19 Hauffen, K., Bart, E., Brady, M., Kersten, D., & Hegd\u00e9, J. (2012). Creating objects and object categories for studying perception and perceptual learning. JoVE (Journal of Visualized Experiments) , (69), e3358. https://doi.org/10.3791/3358 He, D., Kersten, D., & Fang, F. (2012). Opposite modulation of high-and low-level visual aftereffects by perceptual grouping. Current Biology , 22 (11), 1040\u20131045. https://doi.org/10.1016/j.cub.2012.04.026 Hegd\u00e9, J., Thompson, S. K., Brady, M., & Kersten, D. (2012). Object recognition in clutter: Cortical responses depend on the type of learning. Frontiers in Human Neuroscience , 6 , 170. https://doi.org/10.3389/fnhum.2012.00170 Battaglia, P. W., Kersten, D., & Schrater, P. R. (2011a). How haptic size sensations improve distance perception. PLoS Computational Biology , 7 (6). https://doi.org/10.1371/journal.pcbi.1002080 Battaglia, P. W., Kersten, D., & Schrater, P. R. (2011b). The role of generative knowledge in object perception. In J. Trommershauser, K. Kording, & M. S. Landy (Eds.), Sensory cue integration (pp. 46\u201362). https://doi.org/10.1093/acprof:oso/9780195387247.003.0003 Doerschner, K., Fleming, R. W., Yilmaz, O., Schrater, P. R., Hartung, B., & Kersten, D. (2011). Visual motion and the perception of surface material. Current Biology , 21 (23), 2010\u20132016. https://doi.org/10.1016/j.cub.2011.10.036 Doerschner, K., Kersten, D., & Schrater, P. R. (2011). Rapid classification of specular and diffuse reflection from image velocities. Pattern Recognition , 44 (9), 1874\u20131884. https://doi.org/10.1016/j.patcog.2010.09.007 Battaglia, P. W., Di Luca, M., Ernst, M. O., Schrater, P. R., Machulla, T., & Kersten, D. (2010). Within-and cross-modal distance information disambiguate visual size-change perception. PLoS Computational Biology , 6 (3). https://doi.org/10.1371/journal.pcbi.1000697 Boyaci, H., Fang, F., Murray, S. O., & Kersten, D. (2010). Perceptual grouping-dependent lightness processing in human early visual cortex. Journal of Vision , 10 (9), 4\u20134. https://doi.org/10.1167/10.9.4 Green, C., Benson, C., Kersten, D., & Schrater, P. (2010). Alterations in choice behavior by manipulations of world model. Proceedings of the National Academy of Sciences , 107 (37), 16401\u201316406. https://doi.org/10.1073/pnas.1001709107 Hegd\u00e9, J., & Kersten, D. (2010). A link between visual disambiguation and visual memory. Journal of Neuroscience , 30 (45), 15124\u201315133. https://doi.org/10.1523/jneurosci.4415-09.2010 Kersten, D., & Murray, S. O. (2010). Vision: When does looking bigger mean seeing better? Current Biology , 20 (9), R398\u2013R399. https://doi.org/10.1016/j.cub.2010.03.021 Doerschner, K., Kersten, D., & Schrater, P. (2009). Rapid classification of surface reflectance from image velocities. International conference on computer analysis of images and patterns , 856\u2013864. https://doi.org/10.1007/978-3-642-03767-2_104 Fang, F., Boyaci, H., & Kersten, D. (2009). Border ownership selectivity in human early visual cortex and its modulation by attention. Journal of Neuroscience , 29 (2), 460\u2013465. https://doi.org/10.1523/jneurosci.4628-08.2009 Gold, J. M., Abbey, C., Tjan, B. S., & Kersten, D. (2009). Ideal observers and efficiency: Commemorating 50 years of tanner and birdsall: Introduction. JOSA A , 26 (11), IO1\u2013IO2. https://doi.org/10.1364/josaa.26.000io1 Kersten, D., & Mamassian, P. (2009). Ideal observer theory. In L. R. Squire, N. Dronkers, & J. Baldo (Eds.), Encyclopedia of neuroscience (pp. 89\u201395). https://doi.org/10.1016/b978-008045046-9.01435-2 Fang, F., Boyaci, H., Kersten, D., & Murray, S. O. (2008a). Attention-dependent representation of a size illusion in human v1. Current Biology , 18 (21), 1707\u20131712. https://doi.org/10.1016/j.cub.2008.09.025 Fang, F., Kersten, D., & Murray, S. O. (2008b). Perceptual grouping and inverse fMRI activity patterns in human visual cortex. Journal of Vision , 8 (7), 2\u20132. https://doi.org/10.1167/8.7.2 Hegd\u00e9, J., Bart, E., & Kersten, D. (2008a). Fragment-based learning of visual object categories. Current Biology , 18 (8), 597\u2013601. https://doi.org/10.1016/j.cub.2008.03.058 Hegd\u00e9, J., Fang, F., Murray, S. O., & Kersten, D. (2008b). Preferential responses to occluded objects in the human visual cortex. Journal of Vision , 8 (4), 16\u201316. https://doi.org/10.1167/8.4.16 Boyaci, H., Fang, F., Murray, S. O., & Kersten, D. (2007). Responses to lightness variations in early human visual cortex. Current Biology , 17 (11), 989\u2013993. https://doi.org/10.1016/j.cub.2007.05.005 Murray, S. O., Boyaci, H., & Kersten, D. (2006a). The representation of perceived angular size in human primary visual cortex. Nature Neuroscience , 9 (3), 429\u2013434. https://doi.org/10.1038/nn1641 Murray, S. O., Olman, C. A., & Kersten, D. (2006b). Spatially specific fMRI repetition effects in human visual cortex. Journal of Neurophysiology , 95 (4), 2439\u20132445. https://doi.org/10.1152/jn.01236.2005 Yuille, A., & Kersten, D. (2006). Vision as bayesian inference: Analysis by synthesis? Trends in Cognitive Sciences , 10 (7), 301\u2013308. https://doi.org/10.1016/j.tics.2006.05.002 Battaglia, P. W., Schrater, P. R., & Kersten, D. (2005). Auxiliary object knowledge influences visually-guided interception behavior. Proceedings of the 2 nd symposium on applied perception in graphics and visualization , 145\u2013152. https://doi.org/10.1145/1080402.1080430 Fang, F., Murray, S. O., Kersten, D., & He, S. (2005). Orientation-tuned fMRI adaptation in human visual cortex. Journal of Neurophysiology , 94 (6), 4188\u20134195. https://doi.org/10.1152/jn.00378.2005 Hartung, B., Schrater, P. R., B\u00fclthoff, H. H., Kersten, D., & Franz, V. H. (2005). Is prior knowledge of object geometry used in visually guided reaching? Journal of Vision , 5 (6), 2\u20132. https://doi.org/10.1167/5.6.2 Fang, F., Kersten, D., Schrater, P. R., & Yuille, A. L. (2004). Human and ideal observers for detecting image curves. Advances in neural information processing systems , 1459\u20131466. Kersten, D., Mamassian, P., & Yuille, A. (2004). Object perception as bayesian inference. Annu. Rev. Psychol. , 55 , 271\u2013304. https://doi.org/10.1146/annurev.psych.55.090902.142005 Murray, S. O., Schrater, P., & Kersten, D. (2004). Perceptual grouping and the interactions between visual cortical areas. Neural Networks , 17 (5-6), 695\u2013705. https://doi.org/10.1016/j.neunet.2004.03.010 Olman, C. A., Ugurbil, K., Schrater, P., & Kersten, D. (2004). BOLD fMRI and psychophysical measurements of contrast response to broadband images. Vision Research , 44 (7), 669\u2013683. https://doi.org/10.1016/j.visres.2003.10.022 Olman, C., & Kersten, D. (2004). Classification objects, ideal observers & generative models. Cognitive Science , 28 (2), 227\u2013239. https://doi.org/10.1207/s15516709cog2802_5 Brady, M. J., & Kersten, D. (2003). Bootstrapped learning of novel objects. Journal of Vision , 3 (6), 2\u20132. https://doi.org/10.1167/3.6.2 Kersten, D., & Yuille, A. (2003). Bayesian models of object perception. Current Opinion in Neurobiology , 13 (2), 150\u2013158. https://doi.org/10.1016/s0959-4388(03)00042-4 Liu, Z., & Kersten, D. (2003). Three-dimensional symmetric shapes are discriminated more efficiently than asymmetric ones. JOSA A , 20 (7), 1331\u20131340. https://doi.org/10.1364/josaa.20.001331 Naor-Raz, G., Tarr, M. J., & Kersten, D. (2003). Is color an intrinsic property of object representation? Perception , 32 (6), 667\u2013680. https://doi.org/10.1068/p5050 Geisler, W. S., & Kersten, D. (2002). Illusions, perception and bayes. Nature Neuroscience , 5 (6), 508. https://doi.org/10.1038/nn0602-508 Kersten, D. (2002). Object perception: Generative image models and bayesian inference. International workshop on biologically motivated computer vision , 207\u2013218. https://doi.org/10.1007/3-540-36181-2_21 Kersten, D., & Schrater, P. (2002). Pattern inference theory: A probabilistic approach to vision. In D. Heyer & R. Mausfeld (Eds.), Perception and the physical world: Psychological and philosophical issues in perception . https://doi.org/10.1002/0470013427.ch7 Murray, S. O., Kersten, D., Olshausen, B. A., Schrater, P., & Woods, D. L. (2002). Shape perception reduces activity in human primary visual cortex. Proceedings of the National Academy of Sciences , 99 (23), 15164\u201315169. https://doi.org/10.1073/pnas.192579399 Schrater, P., & Kersten, D. (2002). Vision, psychophysics and bayes. In R. P. Rao, B. A. Olshausen, & M. S. Lewicki (Eds.), Probabilistic models of the brain: Perception and neural function (pp. 37\u201360). The MIT press. Madison, C., Thompson, W., Kersten, D., Shirley, P., & Smits, B. (2001). Use of interreflection and shadow for surface contact. Perception & Psychophysics , 63 (2), 187\u2013194. https://doi.org/10.3758/bf03194461 Braje, W. L., Legge, G. E., & Kersten, D. (2000). Invariant recognition of natural objects in the presence of shadows. Perception , 29 (4), 383\u2013398. https://doi.org/10.1068/p3051 Kersten, D. (2000). High-level vision as statistical inference. In M. Gazzaniga (Ed.), The new cognitive neurosciences (Second Edition, pp. 353\u2013364). The MIT Press. Schrater, P. R., & Kersten, D. (2000). How optimal depth cue integration depends on the task. International Journal of Computer Vision , 40 (1), 71\u201389. https://doi.org/10.1023/A:1026557704054 Bloj, M., Kersten, D., & Hurlbert, A. (1999). 3D shape perception influences colour perception via mutual illumination. Nature , 402 , 877\u2013879. Liu, Z., Kersten, D., & Knill, D. C. (1999). Dissociating stimulus information from internal representation\u2014a case study in object recognition. Vision Research , 39 (3), 603\u2013612. https://doi.org/10.1016/s0042-6989(98)00167-9 Schrater, P. R., & Kersten, D. (1999). Statistical structure and task dependence in visual cue integration. Workshop on statistical and computational theories of vision\u2013modeling, learning, computing, and sampling, fort collins, colorado . Troje, N. F., & Kersten, D. (1999). Viewpoint-dependent recognition of familiar faces. Perception , 28 (4), 483\u2013487. https://doi.org/10.1068/p2901 Braje, W. L., Kersten, D., Tarr, M. J., & Troje, N. F. (1998). Illumination effects in face recognition. Psychobiology , 26 (4), 371\u2013380. https://doi.org/10.3758/BF03330623 Liu, Z., & Kersten, D. (1998). 2D affine transformations cannot account for human 3D object recognition. Sixth international conference on computer vision (ieee cat. No. 98CH36271) , 549\u2013554. IEEE. Mamassian, P., Knill, D. C., & Kersten, D. (1998). The perception of cast shadows. Trends in Cognitive Sciences , 2 (8), 288\u2013295. https://doi.org/10.1016/s1364-6613(98)01204-2 Tarr, M. J., Kersten, D., & B\u00fclthoff, H. H. (1998). Why the visual recognition system might encode the effects of illumination. Vision Research , 38 (15-16), 2259\u20132275. https://doi.org/10.1016/s0042-6989(98)00041-8 Thompson, W. B., Shirley, P., Smits, B., Kersten, D. J., & Madison, C. (1998). Visual glue. University of Utah Technical Reports , UUCS-98-007 . Troje, N. F., & Kersten, D. (1998). Viewer-centered recognition of familiar faces . Kersten, D. (1997a). Inverse 3-d graphics: A metaphor for visual perception. Behavior Research Methods, Instruments, & Computers , 29 (1), 37\u201346. https://doi.org/10.3758/bf03200564 Kersten, D. (1997b). Perceptual categories for spatial layout. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences , 352 (1358), 1155\u20131163. https://doi.org/10.1098/rstb.1997.0099 Kersten, D., Mamassian, P., & Knill, D. C. (1997). Moving cast shadows induce apparent motion in depth. Perception , 26 (2), 171\u2013192. https://doi.org/10.1068/p260171 Knill, D. C., Mamassian, P., & Kersten, D. (1997). Geometry of shadows. JOSA A , 14 (12), 3216\u20133232. https://doi.org/10.1364/josaa.14.003216 D\u2019AVOSSA, G., & Kersten, D. (1996). Evidence in human subjects for independent coding of azimuth and elevation for direction of heading from optic flow. Vision Research , 36 (18), 2915\u20132924. https://doi.org/10.1016/0042-6989(96)00010-7 Kersten, D. (1996). Commentary on: Pattern theory: A unifying perspective. In D. C. Knill & W. Richards (Eds.), Perception as bayesian inference (pp. 213\u2013236). Cambridge University Press. Kersten, D., Knill, D. C., Mamassian, P., & B\u00fclthoff, I. (1996a). Illusory motion from shadows. Nature , 379 (6560), 31. https://doi.org/10.1038/379031a0 Kersten, D., Troje, N. F., & B\u00fclthoff, H. H. (1996b). Phenomenal competition for poses of the human head. Perception , 25 (3), 367\u2013368. https://doi.org/10.1068/p250367 Knill, D. C., Kersten, D., & Yuille, A. (1996). Introduction: A bayesian formulation of visual perception. In D. C. Knill & W. Richards (Eds.), Perception as bayesian inference (pp. 1\u201321). https://doi.org/10.1017/CBO9780511984037.002 Knill, D., Kersten, D., & Mamassian, P. (1996). The bayesian framework for visual information processing: Implications for psychophysics. In D. C. Knill & W. Richards (Eds.), Perception as bayesian inference (pp. 239\u2013286). https://doi.org/10.1017/CBO9780511984037.009 Mamassian, P., & Kersten, D. (1996). Illumination, shading and the perception of local orientation. Vision Research , 36 (15), 2351\u20132367. https://doi.org/10.1016/0042-6989(95)00286-3 Mamassian, P., Kersten, D., & Knill, D. C. (1996). Categorical local-shape perception. Perception , 25 (1), 95\u2013107. https://doi.org/10.1068/p250095 Liu, Z., Knill, D. C., & Kersten, D. (1995). Object classification for human and ideal observers. Vision Research , 35 (4), 549\u2013568. https://doi.org/10.1016/0042-6989(94)00150-k Mamassian, P., B\u00fclthoff, H., & Kersten, D. (1995). Eye-hand coordination for 3-d oriented objects . Tjan, B. S., Braje, W. L., Legge, G. E., & Kersten, D. (1995). Human efficiency for recognizing 3-d objects in luminance noise. Vision Research , 35 (21), 3053\u20133069. https://doi.org/10.1016/0042-6989(95)00070-g Kersten, D., Mamassian, P., & Knill, D. C. (1994). Moving cast shadows and the perception of relative depth . Kersten, D. J., & Madarasmi, S. (1993). The visual perception of surfaces, their properties and relationships. Partitioning data sets , 373\u2013390. Madarasmi, S., Kersten, D., & Pong, T.-C. (1993a). Multi-layer surface segmentation using energy minimization. Proceedings of ieee conference on computer vision and pattern recognition , 774\u2013775. IEEE. Madarasmi, S., Kersten, D., & Pong, T.-C. (1993b). The computation of stereo disparity for transparent and for opaque surfaces. Advances in neural information processing systems , 385\u2013392. Kersten, D., B\u00fclthoff, H. H., Schwartz, B. L., & Kurtz, K. J. (1992). Interaction between transparency and structure from motion. Neural Computation , 4 (4), 573\u2013589. https://doi.org/10.1162/neco.1992.4.4.573 O\u2019toole, A. J., & Kersten, D. J. (1992). Learning to see random-dot stereograms. Perception , 21 (2), 227\u2013243. https://doi.org/10.1068/p210227 Thompson, W. B., Kersten, D., & Knecht, W. R. (1992). Structure-from-motion based on information at surface boundaries. Biological Cybernetics , 66 (4), 327\u2013333. https://doi.org/10.1007/bf00203669 Kersten, D. (1991). Transparency and the cooperative computation of scene attributes. In M. S. Landy, J. A. Movshon, & others (Eds.), Computational models of visual processing . The MIT Press. Knill, D. C., & Kersten, D. (1991). Apparent surface curvature affects lightness perception. Nature , 351 (6323), 228\u2013230. https://doi.org/10.1038/351228a0 Knill, D., & Kersten, D. (1991). Ideal perceptual observers for computation, psychophysics and neural networks. In R. J. Watt (Ed.), Pattern recognition by man and machine (vision and visual dysfunction) . McMillan. Kersten, D. (1990). Statistical limits to image understanding. In C. Blakemore, K. Adler, & M. Pointon (Eds.), Vision: Coding and efficiency (pp. 32\u201344). https://doi.org/10.1017/cbo9780511626197.005 Knill, D. C., Field, D., & Kerstent, D. (1990). Human discrimination of fractal images. JOSA A , 7 (6), 1113\u20131123. https://doi.org/10.1364/josaa.7.001113 Knill, D. C., & Kersten, D. (1990). Learning a near-optimal estimator for surface shape from shading. Computer Vision, Graphics, and Image Processing , 50 (1), 75\u2013100. https://doi.org/10.1016/0734-189x(90)90068-7 Kersten, D., Hess, R., & Plant, G. (1988). Assessing contrast sensitivity behind cloudy media. Clinical Vision Sciences , 2 (3), 143\u2013158. Sereno, M., Kersten, D., & Anderson, J. A. (1988). A neural network model of an aspect of motion perception. Science at the John von Neumann National Supercomputer Center , 173\u2013178. Kersten, D. (1987a). Predictability and redundancy of natural images. JOSA A , 4 (12), 2395\u20132400. https://doi.org/10.1364/josaa.4.002395 Kersten, D. (1987b). Statistical efficiency for the detection of visual noise. Vision Research , 27 (6), 1029\u20131040. https://doi.org/10.1016/0042-6989(87)90016-2 Kersten, D., O\u2019toole, A. J., Sereno, M. E., Knill, D. C., & Anderson, J. A. (1987). Associative learning of scene parameters from images. Applied Optics , 26 (23), 4999\u20135006. https://doi.org/10.1364/ao.26.004999 Legge, G. E., Kersten, D., & Burgess, A. E. (1987). Contrast discrimination in noise. JOSA A , 4 (2), 391\u2013404. https://doi.org/10.1364/josaa.4.000391 Burkhardt, D. A., Gottesman, J., Kersten, D., & Legge, G. E. (1984). Symmetry and constancy in the perception of negative and positive luminance contrast. JOSA A , 1 (3), 309\u2013316. https://doi.org/10.1364/josaa.1.000309 Kersten, D. (1984). Spatial summation in visual noise. Vision Research , 24 (12), 1977\u20131990. https://doi.org/10.1016/0042-6989(84)90033-6 Kersten, D. (1983). A comparison of human and ideal performance for the detection of visual pattern (PhD thesis). University of Minnesota. Kersten, D., & Legge, G. E. (1983). Convergence accommodation. JOSA , 73 (3), 332\u2013338. https://doi.org/10.1364/josa.73.000332 Legge, G. E., & Kersten, D. (1983). Light and dark bars; contrast discrimination. Vision Research , 23 (5), 473\u2013483. https://doi.org/10.1016/0042-6989(83)90122-0","title":"Publications"},{"location":"publications/#publications","text":"Vizioli, L., De Martino, F., Petro, L. S., Kersten, D., Ugurbil, K., Yacoub, E., & Muckli, L. (2019). Multivoxel pattern of blood oxygen level dependent activity can be sensitive to stimulus specific fine scale responses. bioRxiv . https://doi.org/10.1101/798306 Peterson, L. M., Kersten, D. J., & Mannion, D. J. (2018). Surface curvature from kinetic depth can affect lightness. Journal of Experimental Psychology: Human Perception and Performance , 44 (12), 1856. https://doi.org/10.1037/xhp0000575 Morgenstern, Y., & Kersten, D. J. (2017). The perceptual dimensions of natural dynamic flow. Journal of Vision , 17 (12), 7\u20137. https://doi.org/10.1167/17.12.7 Thompson, W. B., Legge, G. E., Kersten, D. J., Shakespeare, R. A., & Lei, Q. (2017). Simulating visibility under reduced acuity and contrast sensitivity. JOSA A , 34 (4), 583\u2013593. https://doi.org/10.1364/josaa.34.000583 Fan, X., Wang, L., Shao, H., Kersten, D., & He, S. (2016). Temporally flexible feedback signal to foveal cortex for peripheral object recognition. Proceedings of the National Academy of Sciences , 113 (41), 11627\u201311632. https://doi.org/10.1073/pnas.1606137113 Kersten, D., & Mamassian, P. (2016). Cast shadow illusion. In A. G. Shapiro & D. Todorovic (Eds.), The oxford compendium of visual illusions (pp. 214\u2013220). https://doi.org/10.1093/acprof:oso/9780199794607.003.0020 Qiu, C., Burton, P. C., Kersten, D., & Olman, C. A. (2016). Responses in early visual areas to contour integration are context dependent. Journal of Vision , 16 (8), 19\u201319. https://doi.org/10.1167/16.8.19 Yuille, A., & Kersten, D. (2016). Early vision. In M. A. Arbib & J. J. Bonaiuto (Eds.), From neuron to cognition via computational neuroscience (pp. 345\u2013408). MIT Press. Green, C. S., Kattner, F., Siegel, M. H., Kersten, D., & Schrater, P. R. (2015). Differences in perceptual learning transfer as a function of training task. Journal of Vision , 15 (10), 5\u20135. https://doi.org/10.1167/15.10.5 Kam, T.-E., Mannion, D. J., Lee, S.-W., Doerschner, K., & Kersten, D. J. (2015). Human visual cortical responses to specular and matte motion flows. Frontiers in Human Neuroscience , 9 , 579. https://doi.org/10.3389/fnhum.2015.00579 Mannion, D. J., Kersten, D. J., & Olman, C. A. (2015). Scene coherence can affect the local response to natural images in human v1. European Journal of Neuroscience , 42 (11), 2895\u20132903. https://doi.org/10.1111/ejn.13082 Akin, B., Ozdem, C., Eroglu, S., Keskin, D. T., Fang, F., Doerschner, K., \u2026 Boyaci, H. (2014). Attention modulates neuronal correlates of interhemispheric integration and global motion perception. Journal of Vision , 14 (12), 30\u201330. https://doi.org/10.1167/14.12.30 Kersten, D., & Yuille, A. (2014a). Inferential models of the visual cortical hierarchy. In M. S. Gazzaniga & G. R. Mangun (Eds.), The cognitive neurosciences (Fifth Edition, pp. 398\u2013404). The MIT Press. Kersten, D., & Yuille, A. (2014b). Vision: Bayesian inference and beyond. In J. S. Werner & L. M. Chalupa (Eds.), The new visual neurosciences (pp. 1263\u20131278). The MIT Press. Mannion, D. J., Kersten, D. J., & Olman, C. A. (2014). Regions of mid-level human visual cortex sensitive to the global coherence of local image patches. Journal of Cognitive Neuroscience , 26 (8), 1764\u20131774. https://doi.org/10.1162/jocn_a_00588 Kersten, D., Shakespeare, R., & Thompson, W. (2013). Predicting visibility in designs of public spaces. University of Utah Technical Reports , UUCS 13-001 . Mannion, D. J., Kersten, D., & Olman, C. A. (2013). Consequences of polar form coherence for fMRI responses in human visual cortex. NeuroImage , 78 , 152\u2013158. https://doi.org/10.1016/j.neuroimage.2013.04.036 McMenamin, B. W., Radue, J., Trask, J., Huskamp, K., Kersten, D., & Marsolek, C. J. (2013). The diagnosticity of color for emotional objects. Motivation and Emotion , 37 (3), 609\u2013622. https://doi.org/10.1007/s11031-012-9319-0 Qiu, C., Kersten, D., & Olman, C. A. (2013). Segmentation decreases the magnitude of the tilt illusion. Journal of Vision , 13 (13), 19\u201319. https://doi.org/10.1167/13.13.19 Hauffen, K., Bart, E., Brady, M., Kersten, D., & Hegd\u00e9, J. (2012). Creating objects and object categories for studying perception and perceptual learning. JoVE (Journal of Visualized Experiments) , (69), e3358. https://doi.org/10.3791/3358 He, D., Kersten, D., & Fang, F. (2012). Opposite modulation of high-and low-level visual aftereffects by perceptual grouping. Current Biology , 22 (11), 1040\u20131045. https://doi.org/10.1016/j.cub.2012.04.026 Hegd\u00e9, J., Thompson, S. K., Brady, M., & Kersten, D. (2012). Object recognition in clutter: Cortical responses depend on the type of learning. Frontiers in Human Neuroscience , 6 , 170. https://doi.org/10.3389/fnhum.2012.00170 Battaglia, P. W., Kersten, D., & Schrater, P. R. (2011a). How haptic size sensations improve distance perception. PLoS Computational Biology , 7 (6). https://doi.org/10.1371/journal.pcbi.1002080 Battaglia, P. W., Kersten, D., & Schrater, P. R. (2011b). The role of generative knowledge in object perception. In J. Trommershauser, K. Kording, & M. S. Landy (Eds.), Sensory cue integration (pp. 46\u201362). https://doi.org/10.1093/acprof:oso/9780195387247.003.0003 Doerschner, K., Fleming, R. W., Yilmaz, O., Schrater, P. R., Hartung, B., & Kersten, D. (2011). Visual motion and the perception of surface material. Current Biology , 21 (23), 2010\u20132016. https://doi.org/10.1016/j.cub.2011.10.036 Doerschner, K., Kersten, D., & Schrater, P. R. (2011). Rapid classification of specular and diffuse reflection from image velocities. Pattern Recognition , 44 (9), 1874\u20131884. https://doi.org/10.1016/j.patcog.2010.09.007 Battaglia, P. W., Di Luca, M., Ernst, M. O., Schrater, P. R., Machulla, T., & Kersten, D. (2010). Within-and cross-modal distance information disambiguate visual size-change perception. PLoS Computational Biology , 6 (3). https://doi.org/10.1371/journal.pcbi.1000697 Boyaci, H., Fang, F., Murray, S. O., & Kersten, D. (2010). Perceptual grouping-dependent lightness processing in human early visual cortex. Journal of Vision , 10 (9), 4\u20134. https://doi.org/10.1167/10.9.4 Green, C., Benson, C., Kersten, D., & Schrater, P. (2010). Alterations in choice behavior by manipulations of world model. Proceedings of the National Academy of Sciences , 107 (37), 16401\u201316406. https://doi.org/10.1073/pnas.1001709107 Hegd\u00e9, J., & Kersten, D. (2010). A link between visual disambiguation and visual memory. Journal of Neuroscience , 30 (45), 15124\u201315133. https://doi.org/10.1523/jneurosci.4415-09.2010 Kersten, D., & Murray, S. O. (2010). Vision: When does looking bigger mean seeing better? Current Biology , 20 (9), R398\u2013R399. https://doi.org/10.1016/j.cub.2010.03.021 Doerschner, K., Kersten, D., & Schrater, P. (2009). Rapid classification of surface reflectance from image velocities. International conference on computer analysis of images and patterns , 856\u2013864. https://doi.org/10.1007/978-3-642-03767-2_104 Fang, F., Boyaci, H., & Kersten, D. (2009). Border ownership selectivity in human early visual cortex and its modulation by attention. Journal of Neuroscience , 29 (2), 460\u2013465. https://doi.org/10.1523/jneurosci.4628-08.2009 Gold, J. M., Abbey, C., Tjan, B. S., & Kersten, D. (2009). Ideal observers and efficiency: Commemorating 50 years of tanner and birdsall: Introduction. JOSA A , 26 (11), IO1\u2013IO2. https://doi.org/10.1364/josaa.26.000io1 Kersten, D., & Mamassian, P. (2009). Ideal observer theory. In L. R. Squire, N. Dronkers, & J. Baldo (Eds.), Encyclopedia of neuroscience (pp. 89\u201395). https://doi.org/10.1016/b978-008045046-9.01435-2 Fang, F., Boyaci, H., Kersten, D., & Murray, S. O. (2008a). Attention-dependent representation of a size illusion in human v1. Current Biology , 18 (21), 1707\u20131712. https://doi.org/10.1016/j.cub.2008.09.025 Fang, F., Kersten, D., & Murray, S. O. (2008b). Perceptual grouping and inverse fMRI activity patterns in human visual cortex. Journal of Vision , 8 (7), 2\u20132. https://doi.org/10.1167/8.7.2 Hegd\u00e9, J., Bart, E., & Kersten, D. (2008a). Fragment-based learning of visual object categories. Current Biology , 18 (8), 597\u2013601. https://doi.org/10.1016/j.cub.2008.03.058 Hegd\u00e9, J., Fang, F., Murray, S. O., & Kersten, D. (2008b). Preferential responses to occluded objects in the human visual cortex. Journal of Vision , 8 (4), 16\u201316. https://doi.org/10.1167/8.4.16 Boyaci, H., Fang, F., Murray, S. O., & Kersten, D. (2007). Responses to lightness variations in early human visual cortex. Current Biology , 17 (11), 989\u2013993. https://doi.org/10.1016/j.cub.2007.05.005 Murray, S. O., Boyaci, H., & Kersten, D. (2006a). The representation of perceived angular size in human primary visual cortex. Nature Neuroscience , 9 (3), 429\u2013434. https://doi.org/10.1038/nn1641 Murray, S. O., Olman, C. A., & Kersten, D. (2006b). Spatially specific fMRI repetition effects in human visual cortex. Journal of Neurophysiology , 95 (4), 2439\u20132445. https://doi.org/10.1152/jn.01236.2005 Yuille, A., & Kersten, D. (2006). Vision as bayesian inference: Analysis by synthesis? Trends in Cognitive Sciences , 10 (7), 301\u2013308. https://doi.org/10.1016/j.tics.2006.05.002 Battaglia, P. W., Schrater, P. R., & Kersten, D. (2005). Auxiliary object knowledge influences visually-guided interception behavior. Proceedings of the 2 nd symposium on applied perception in graphics and visualization , 145\u2013152. https://doi.org/10.1145/1080402.1080430 Fang, F., Murray, S. O., Kersten, D., & He, S. (2005). Orientation-tuned fMRI adaptation in human visual cortex. Journal of Neurophysiology , 94 (6), 4188\u20134195. https://doi.org/10.1152/jn.00378.2005 Hartung, B., Schrater, P. R., B\u00fclthoff, H. H., Kersten, D., & Franz, V. H. (2005). Is prior knowledge of object geometry used in visually guided reaching? Journal of Vision , 5 (6), 2\u20132. https://doi.org/10.1167/5.6.2 Fang, F., Kersten, D., Schrater, P. R., & Yuille, A. L. (2004). Human and ideal observers for detecting image curves. Advances in neural information processing systems , 1459\u20131466. Kersten, D., Mamassian, P., & Yuille, A. (2004). Object perception as bayesian inference. Annu. Rev. Psychol. , 55 , 271\u2013304. https://doi.org/10.1146/annurev.psych.55.090902.142005 Murray, S. O., Schrater, P., & Kersten, D. (2004). Perceptual grouping and the interactions between visual cortical areas. Neural Networks , 17 (5-6), 695\u2013705. https://doi.org/10.1016/j.neunet.2004.03.010 Olman, C. A., Ugurbil, K., Schrater, P., & Kersten, D. (2004). BOLD fMRI and psychophysical measurements of contrast response to broadband images. Vision Research , 44 (7), 669\u2013683. https://doi.org/10.1016/j.visres.2003.10.022 Olman, C., & Kersten, D. (2004). Classification objects, ideal observers & generative models. Cognitive Science , 28 (2), 227\u2013239. https://doi.org/10.1207/s15516709cog2802_5 Brady, M. J., & Kersten, D. (2003). Bootstrapped learning of novel objects. Journal of Vision , 3 (6), 2\u20132. https://doi.org/10.1167/3.6.2 Kersten, D., & Yuille, A. (2003). Bayesian models of object perception. Current Opinion in Neurobiology , 13 (2), 150\u2013158. https://doi.org/10.1016/s0959-4388(03)00042-4 Liu, Z., & Kersten, D. (2003). Three-dimensional symmetric shapes are discriminated more efficiently than asymmetric ones. JOSA A , 20 (7), 1331\u20131340. https://doi.org/10.1364/josaa.20.001331 Naor-Raz, G., Tarr, M. J., & Kersten, D. (2003). Is color an intrinsic property of object representation? Perception , 32 (6), 667\u2013680. https://doi.org/10.1068/p5050 Geisler, W. S., & Kersten, D. (2002). Illusions, perception and bayes. Nature Neuroscience , 5 (6), 508. https://doi.org/10.1038/nn0602-508 Kersten, D. (2002). Object perception: Generative image models and bayesian inference. International workshop on biologically motivated computer vision , 207\u2013218. https://doi.org/10.1007/3-540-36181-2_21 Kersten, D., & Schrater, P. (2002). Pattern inference theory: A probabilistic approach to vision. In D. Heyer & R. Mausfeld (Eds.), Perception and the physical world: Psychological and philosophical issues in perception . https://doi.org/10.1002/0470013427.ch7 Murray, S. O., Kersten, D., Olshausen, B. A., Schrater, P., & Woods, D. L. (2002). Shape perception reduces activity in human primary visual cortex. Proceedings of the National Academy of Sciences , 99 (23), 15164\u201315169. https://doi.org/10.1073/pnas.192579399 Schrater, P., & Kersten, D. (2002). Vision, psychophysics and bayes. In R. P. Rao, B. A. Olshausen, & M. S. Lewicki (Eds.), Probabilistic models of the brain: Perception and neural function (pp. 37\u201360). The MIT press. Madison, C., Thompson, W., Kersten, D., Shirley, P., & Smits, B. (2001). Use of interreflection and shadow for surface contact. Perception & Psychophysics , 63 (2), 187\u2013194. https://doi.org/10.3758/bf03194461 Braje, W. L., Legge, G. E., & Kersten, D. (2000). Invariant recognition of natural objects in the presence of shadows. Perception , 29 (4), 383\u2013398. https://doi.org/10.1068/p3051 Kersten, D. (2000). High-level vision as statistical inference. In M. Gazzaniga (Ed.), The new cognitive neurosciences (Second Edition, pp. 353\u2013364). The MIT Press. Schrater, P. R., & Kersten, D. (2000). How optimal depth cue integration depends on the task. International Journal of Computer Vision , 40 (1), 71\u201389. https://doi.org/10.1023/A:1026557704054 Bloj, M., Kersten, D., & Hurlbert, A. (1999). 3D shape perception influences colour perception via mutual illumination. Nature , 402 , 877\u2013879. Liu, Z., Kersten, D., & Knill, D. C. (1999). Dissociating stimulus information from internal representation\u2014a case study in object recognition. Vision Research , 39 (3), 603\u2013612. https://doi.org/10.1016/s0042-6989(98)00167-9 Schrater, P. R., & Kersten, D. (1999). Statistical structure and task dependence in visual cue integration. Workshop on statistical and computational theories of vision\u2013modeling, learning, computing, and sampling, fort collins, colorado . Troje, N. F., & Kersten, D. (1999). Viewpoint-dependent recognition of familiar faces. Perception , 28 (4), 483\u2013487. https://doi.org/10.1068/p2901 Braje, W. L., Kersten, D., Tarr, M. J., & Troje, N. F. (1998). Illumination effects in face recognition. Psychobiology , 26 (4), 371\u2013380. https://doi.org/10.3758/BF03330623 Liu, Z., & Kersten, D. (1998). 2D affine transformations cannot account for human 3D object recognition. Sixth international conference on computer vision (ieee cat. No. 98CH36271) , 549\u2013554. IEEE. Mamassian, P., Knill, D. C., & Kersten, D. (1998). The perception of cast shadows. Trends in Cognitive Sciences , 2 (8), 288\u2013295. https://doi.org/10.1016/s1364-6613(98)01204-2 Tarr, M. J., Kersten, D., & B\u00fclthoff, H. H. (1998). Why the visual recognition system might encode the effects of illumination. Vision Research , 38 (15-16), 2259\u20132275. https://doi.org/10.1016/s0042-6989(98)00041-8 Thompson, W. B., Shirley, P., Smits, B., Kersten, D. J., & Madison, C. (1998). Visual glue. University of Utah Technical Reports , UUCS-98-007 . Troje, N. F., & Kersten, D. (1998). Viewer-centered recognition of familiar faces . Kersten, D. (1997a). Inverse 3-d graphics: A metaphor for visual perception. Behavior Research Methods, Instruments, & Computers , 29 (1), 37\u201346. https://doi.org/10.3758/bf03200564 Kersten, D. (1997b). Perceptual categories for spatial layout. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences , 352 (1358), 1155\u20131163. https://doi.org/10.1098/rstb.1997.0099 Kersten, D., Mamassian, P., & Knill, D. C. (1997). Moving cast shadows induce apparent motion in depth. Perception , 26 (2), 171\u2013192. https://doi.org/10.1068/p260171 Knill, D. C., Mamassian, P., & Kersten, D. (1997). Geometry of shadows. JOSA A , 14 (12), 3216\u20133232. https://doi.org/10.1364/josaa.14.003216 D\u2019AVOSSA, G., & Kersten, D. (1996). Evidence in human subjects for independent coding of azimuth and elevation for direction of heading from optic flow. Vision Research , 36 (18), 2915\u20132924. https://doi.org/10.1016/0042-6989(96)00010-7 Kersten, D. (1996). Commentary on: Pattern theory: A unifying perspective. In D. C. Knill & W. Richards (Eds.), Perception as bayesian inference (pp. 213\u2013236). Cambridge University Press. Kersten, D., Knill, D. C., Mamassian, P., & B\u00fclthoff, I. (1996a). Illusory motion from shadows. Nature , 379 (6560), 31. https://doi.org/10.1038/379031a0 Kersten, D., Troje, N. F., & B\u00fclthoff, H. H. (1996b). Phenomenal competition for poses of the human head. Perception , 25 (3), 367\u2013368. https://doi.org/10.1068/p250367 Knill, D. C., Kersten, D., & Yuille, A. (1996). Introduction: A bayesian formulation of visual perception. In D. C. Knill & W. Richards (Eds.), Perception as bayesian inference (pp. 1\u201321). https://doi.org/10.1017/CBO9780511984037.002 Knill, D., Kersten, D., & Mamassian, P. (1996). The bayesian framework for visual information processing: Implications for psychophysics. In D. C. Knill & W. Richards (Eds.), Perception as bayesian inference (pp. 239\u2013286). https://doi.org/10.1017/CBO9780511984037.009 Mamassian, P., & Kersten, D. (1996). Illumination, shading and the perception of local orientation. Vision Research , 36 (15), 2351\u20132367. https://doi.org/10.1016/0042-6989(95)00286-3 Mamassian, P., Kersten, D., & Knill, D. C. (1996). Categorical local-shape perception. Perception , 25 (1), 95\u2013107. https://doi.org/10.1068/p250095 Liu, Z., Knill, D. C., & Kersten, D. (1995). Object classification for human and ideal observers. Vision Research , 35 (4), 549\u2013568. https://doi.org/10.1016/0042-6989(94)00150-k Mamassian, P., B\u00fclthoff, H., & Kersten, D. (1995). Eye-hand coordination for 3-d oriented objects . Tjan, B. S., Braje, W. L., Legge, G. E., & Kersten, D. (1995). Human efficiency for recognizing 3-d objects in luminance noise. Vision Research , 35 (21), 3053\u20133069. https://doi.org/10.1016/0042-6989(95)00070-g Kersten, D., Mamassian, P., & Knill, D. C. (1994). Moving cast shadows and the perception of relative depth . Kersten, D. J., & Madarasmi, S. (1993). The visual perception of surfaces, their properties and relationships. Partitioning data sets , 373\u2013390. Madarasmi, S., Kersten, D., & Pong, T.-C. (1993a). Multi-layer surface segmentation using energy minimization. Proceedings of ieee conference on computer vision and pattern recognition , 774\u2013775. IEEE. Madarasmi, S., Kersten, D., & Pong, T.-C. (1993b). The computation of stereo disparity for transparent and for opaque surfaces. Advances in neural information processing systems , 385\u2013392. Kersten, D., B\u00fclthoff, H. H., Schwartz, B. L., & Kurtz, K. J. (1992). Interaction between transparency and structure from motion. Neural Computation , 4 (4), 573\u2013589. https://doi.org/10.1162/neco.1992.4.4.573 O\u2019toole, A. J., & Kersten, D. J. (1992). Learning to see random-dot stereograms. Perception , 21 (2), 227\u2013243. https://doi.org/10.1068/p210227 Thompson, W. B., Kersten, D., & Knecht, W. R. (1992). Structure-from-motion based on information at surface boundaries. Biological Cybernetics , 66 (4), 327\u2013333. https://doi.org/10.1007/bf00203669 Kersten, D. (1991). Transparency and the cooperative computation of scene attributes. In M. S. Landy, J. A. Movshon, & others (Eds.), Computational models of visual processing . The MIT Press. Knill, D. C., & Kersten, D. (1991). Apparent surface curvature affects lightness perception. Nature , 351 (6323), 228\u2013230. https://doi.org/10.1038/351228a0 Knill, D., & Kersten, D. (1991). Ideal perceptual observers for computation, psychophysics and neural networks. In R. J. Watt (Ed.), Pattern recognition by man and machine (vision and visual dysfunction) . McMillan. Kersten, D. (1990). Statistical limits to image understanding. In C. Blakemore, K. Adler, & M. Pointon (Eds.), Vision: Coding and efficiency (pp. 32\u201344). https://doi.org/10.1017/cbo9780511626197.005 Knill, D. C., Field, D., & Kerstent, D. (1990). Human discrimination of fractal images. JOSA A , 7 (6), 1113\u20131123. https://doi.org/10.1364/josaa.7.001113 Knill, D. C., & Kersten, D. (1990). Learning a near-optimal estimator for surface shape from shading. Computer Vision, Graphics, and Image Processing , 50 (1), 75\u2013100. https://doi.org/10.1016/0734-189x(90)90068-7 Kersten, D., Hess, R., & Plant, G. (1988). Assessing contrast sensitivity behind cloudy media. Clinical Vision Sciences , 2 (3), 143\u2013158. Sereno, M., Kersten, D., & Anderson, J. A. (1988). A neural network model of an aspect of motion perception. Science at the John von Neumann National Supercomputer Center , 173\u2013178. Kersten, D. (1987a). Predictability and redundancy of natural images. JOSA A , 4 (12), 2395\u20132400. https://doi.org/10.1364/josaa.4.002395 Kersten, D. (1987b). Statistical efficiency for the detection of visual noise. Vision Research , 27 (6), 1029\u20131040. https://doi.org/10.1016/0042-6989(87)90016-2 Kersten, D., O\u2019toole, A. J., Sereno, M. E., Knill, D. C., & Anderson, J. A. (1987). Associative learning of scene parameters from images. Applied Optics , 26 (23), 4999\u20135006. https://doi.org/10.1364/ao.26.004999 Legge, G. E., Kersten, D., & Burgess, A. E. (1987). Contrast discrimination in noise. JOSA A , 4 (2), 391\u2013404. https://doi.org/10.1364/josaa.4.000391 Burkhardt, D. A., Gottesman, J., Kersten, D., & Legge, G. E. (1984). Symmetry and constancy in the perception of negative and positive luminance contrast. JOSA A , 1 (3), 309\u2013316. https://doi.org/10.1364/josaa.1.000309 Kersten, D. (1984). Spatial summation in visual noise. Vision Research , 24 (12), 1977\u20131990. https://doi.org/10.1016/0042-6989(84)90033-6 Kersten, D. (1983). A comparison of human and ideal performance for the detection of visual pattern (PhD thesis). University of Minnesota. Kersten, D., & Legge, G. E. (1983). Convergence accommodation. JOSA , 73 (3), 332\u2013338. https://doi.org/10.1364/josa.73.000332 Legge, G. E., & Kersten, D. (1983). Light and dark bars; contrast discrimination. Vision Research , 23 (5), 473\u2013483. https://doi.org/10.1016/0042-6989(83)90122-0","title":"Publications"},{"location":"research/","text":"Research \u00b6 What is computational vision? \u00b6 Vision is used for recognition, planning and motor actions. Exactly how to get from the retinal image intensities to useful actions is a tough problem requiring multiple approaches. A major theoretical challenge is to discover the computational principles required to infer world properties and determine motor output from images. Computational vision searches for these solutions. The empirical challenge is to discover how our visual systems and those of other animals are built to achieve useful actions from the images received. Psychophysics, neurophysiology and brain imaging provide tools to investigate how the visual pathways of the brain transform image information into useful plans and action. Our research addresses problems of vision using computational, psychophysical, and brain imaging approaches. Vision as statistical inference Bayesian approaches \u00b6 Bayesian approaches have enjoyed a great deal of recent success in their application to problems in computer vision. This success has led to an emerging interest in applying Bayesian methods to modeling human visual perception. We consider the implications of a Bayesian view of visual information processing for experimentally investigating human visual perception. We have outlined the elements of a general program of empirical research which results from taking the basic Bayesian formulation seriously not only as a means for objectively modeling image information through ideal observer analysis (e.g. see our work in object recognition), but also as a framework for characterizing human perceptual inference. A major advantage of following such a program is that, because its structure is the same as that of the Bayesian framework for computational modeling, it supports a strong integration of psychophysics and computational theory. In particular, it provides the foundation for a psychophysics of constraints in which one tests hypotheses regarding quantitative and qualitative constraints used in human perceptual inferences. The Bayesian approach also suggests new ways to conceptualize the general problem of perception and to decompose it into isolatable parts for psychophysical investigation; that is, it not only provides a framework for modeling solutions to specific perceptual problems; it also guides the definition of the problems.","title":"Research"},{"location":"research/#research","text":"","title":"Research"},{"location":"research/#what-is-computational-vision","text":"Vision is used for recognition, planning and motor actions. Exactly how to get from the retinal image intensities to useful actions is a tough problem requiring multiple approaches. A major theoretical challenge is to discover the computational principles required to infer world properties and determine motor output from images. Computational vision searches for these solutions. The empirical challenge is to discover how our visual systems and those of other animals are built to achieve useful actions from the images received. Psychophysics, neurophysiology and brain imaging provide tools to investigate how the visual pathways of the brain transform image information into useful plans and action. Our research addresses problems of vision using computational, psychophysical, and brain imaging approaches.","title":"What is computational vision?"},{"location":"research/#vision-as-statistical-inference-bayesian-approaches","text":"Bayesian approaches have enjoyed a great deal of recent success in their application to problems in computer vision. This success has led to an emerging interest in applying Bayesian methods to modeling human visual perception. We consider the implications of a Bayesian view of visual information processing for experimentally investigating human visual perception. We have outlined the elements of a general program of empirical research which results from taking the basic Bayesian formulation seriously not only as a means for objectively modeling image information through ideal observer analysis (e.g. see our work in object recognition), but also as a framework for characterizing human perceptual inference. A major advantage of following such a program is that, because its structure is the same as that of the Bayesian framework for computational modeling, it supports a strong integration of psychophysics and computational theory. In particular, it provides the foundation for a psychophysics of constraints in which one tests hypotheses regarding quantitative and qualitative constraints used in human perceptual inferences. The Bayesian approach also suggests new ways to conceptualize the general problem of perception and to decompose it into isolatable parts for psychophysical investigation; that is, it not only provides a framework for modeling solutions to specific perceptual problems; it also guides the definition of the problems.","title":"Vision as statistical inference Bayesian approaches"},{"location":"courses/PSY5036F2019/","text":"Computational Vision PSY5036F2019 \u00b6 University of Minnesota, Fall Semester, 2019 Instructor: Dan Kersten: kersten@umn.edu Abstract The visual perception of what is in the world is accomplished continually, instantaneously, and usually without conscious thought. The very effortlessness of perception disguises the underlying richness of the problem. We can gain insight into the processes and functions of human vision by studying the relationship between neural mechanisms and visual behavior through computer analysis and simulation. Students will learn about the anatomy and neurophysiology of vision and how they relate to the phenomona of perception. An underlying theme will be to treat vision as a process of statistical inference. There will be in-class programming exercises using the language Mathematica. No prior programming experience is required; however, some familiarity with probability, vector calculus and linear algebra is helpful. Readings \u00b6 Main \u00b6 Lecture notes, Main Readings & Supplementary Material are all available online. Additional readings \u00b6 Math and vision \u00b6 ( EV ) Early Vision. Yuille and Kersten. In From Neuron to Cognition via Computational Neuroscience , M.A. Arbib, James J. Bonaiuto Editors, Cambridge MA: The MIT Press, in 2016 ( preprint pdf ) Understanding Vision: Theory, Models, and Data. Li Zhaoping. 2014.( publisher page ) ( author's web outline ) Functional human vision \u00b6 ( FV ) Foundations of Vision . Wandell ( web ) Neurophysiology \u00b6 ( NVN ) The New Visual Neurosciences . John S. Werner and Leo M. Chalupa, edts. 2014. Software \u00b6 Mathematica \u00b6 Mathematica is the primary programming environment for this course. Students who have registered for the course will have access through the Psychology Department's site license. Alternatives: Mathematica is available in several labs on campus, go to http://www.oit.umn.edu/computer-labs/software/index.htm You may wish to purchase Mathematica for Students see http://www.wolfram.com/products/student/mathforstudents/index.html . For help using Mathematica, see: http://mathematica.stackexchange.com Python/IPython \u00b6 http://ipython.org http://jupyter-notebook-beginner-guide.readthedocs.org/en/latest/index.html http://www.scipy.org For an online course in using Python and PsychoPy for research in human vision see: http://nbviewer.ipython.org/github/gestaltrevision/python_for_visres/blob/master/index.ipynb Writing \u00b6 Gopen, G. D., & Swan, J. A., 1990. The Science of Scientific Writing. <u> American Scientist </u> , <u> 78 </u> , 550-558. Supplementary: The Sense of Style: The Thinking Person's Guide to Writing in the 21 st Century (2014), Pinker, Steven. ( amazon link ) Penrose, A. M., & Katz, S. B. (1998). <u> Writing in the Sciences: Exploring Conventions of Scientific Discourse </u> . New York: St. Martin's Press, Inc. American Psychological Association. (2009). <u> Publication manual of the American Psychological Association </u> (6 th ed.). Washington, DC: American Psychological Association Writing assistance. THE CENTER FOR WRITING offers free one-to-one writing assistance to undergraduate and graduate students, with appointments up to 45 minutes. Nonnative speaker specialists are available. For more information, see http://writing.umn.edu . Psychology department resources: http://writing.psych.umn.edu/student-resources Grade Requirements \u00b6 There will be programming assignments and a final project. The grade weights are: Exercise/programming assignments: 55% Final project in-class presentations: 5 % Final project : 40% (five parts: 2% (title and outline) +5%(first draft) +5% (peer commentary) +8% (cover letter response) + 20% (final draft)) The programming assignments will use the Mathematica programming environment. No prior experience with Mathematica is necessary. Assignment due By the 6 am on the day after the nominal due date. Late Policy: Assignments turned in within 24 hours following the due date will have 15% deducted from the assignment score. Assignments turned in between 24 and 48 hours following the due date will have 30% deducted from the score. Assignments more than 48 hours late will receive a score of zero. Lectures \u00b6 Check this section before each class for recent additions and revisions. Info Lecture notes are in Mathematica Notebook and pdf format. You can download the Mathematica notebook files below to view with Mathematica or Wolfram CDF Player (whichis free). Final Project Assignment \u00b6 Goal: This course integrates the behavioral, neural and computational principles of perception. Students often find the interdisciplinary integration to be the most challenging aspect of the course. Through writing, you will learn to synthesize results from diverse and typically isolated disciplines. By writing about your project work, you will learn to think through the broader implications of your project, and to effectively communicate the rationale and results of your contribution in words. You will do a final page research report in which you will describe, in the form of a scientific paper, the results of an original computer program on a topic in computational vision. Your final project will involve: 1) a computer program and; 2) a 2000-3000 word final paper describing your project. For your computer project, you will do one of the following: 1) Write a program to simulate a model from the computer vision literature ; 2) Design and program a method for solving some problem in perception. 3) Design and program a psychophysical experiment to study an aspect of human visual perception. The results of your final project should be written up in the form of a short scientific paper or Mathematica Notebook, describing the motivation, methods, results, and interpretation. If you choose to write your program in Mathematica, your paper and program can be combined can be formated as a Mathematica notebook. See: Books and Tutorials on Notebooks. If you do your final project using Python, you can turn your paper in as a Jupyter notebook. Your paper will be critiqued and returned for you to revise and resubmit in final form. You should write for an audience consisting of your class peers. Completing the final paper involves 4 steps. Each step requires that you email a document to the teaching assistant. Outline (2% of grade). You will submit a working title and paragraph outline by the deadline noted in the syllabus. These outlines will be critiqued in order to help you find an appropriate focus for your papers. (Consult with the instructor or TA for ideas well ahead of time). Complete draft (5% of grade). A double-spaced, complete draft of the paper must be turned in by the deadline noted in the syllabus. Papers should be between 2000 and 3000 words. In addition to the title, author and date lines, papers must include the following sections: Abstract, Introduction, Methods, Results, Discussion, and Bibliography. Use citations to motivate your problem and to justify your claims. Cite authors by name and date, e.g. (Marr & Poggio, 1979). Citations should be original sources, not wikipedia. Use a standard citation format, such as APA. (The UM library has information on style guides , and in particular APA style .) Papers must be typed, with a page number on each page. Figures should be numbered and have figure captions. This draft will be reviewed by your instructor and one of you class peers. The point break down for the total 5% is: 2 pts for completing Introduction, 2 pts for completing Methods, 1 pt for completing Discussion) Peer commentary (5% of grade) . You will submit a written commentary (200 to 500 words) on a complete draft of one of your class peers. The project drafts and commentaries will be anonymous. The commentary should provide feedback to improve the quality and clarity of the writing. Final draft (20% of grade) and \"Cover letter\" (8% of grade). The final draft must be turned in by the date noted on the syllabus. The \"Cover letter\" should describe how your revision addressed comments from your peer evaluator and from your instructor. It should itemize key criticisms together with a brief description of the changes you made to your draft manuscript.","title":"PSY5036F2019"},{"location":"courses/PSY5036F2019/#computational-vision-psy5036f2019","text":"University of Minnesota, Fall Semester, 2019 Instructor: Dan Kersten: kersten@umn.edu Abstract The visual perception of what is in the world is accomplished continually, instantaneously, and usually without conscious thought. The very effortlessness of perception disguises the underlying richness of the problem. We can gain insight into the processes and functions of human vision by studying the relationship between neural mechanisms and visual behavior through computer analysis and simulation. Students will learn about the anatomy and neurophysiology of vision and how they relate to the phenomona of perception. An underlying theme will be to treat vision as a process of statistical inference. There will be in-class programming exercises using the language Mathematica. No prior programming experience is required; however, some familiarity with probability, vector calculus and linear algebra is helpful.","title":"Computational Vision PSY5036F2019"},{"location":"courses/PSY5036F2019/#readings","text":"","title":"Readings"},{"location":"courses/PSY5036F2019/#main","text":"Lecture notes, Main Readings & Supplementary Material are all available online.","title":"Main"},{"location":"courses/PSY5036F2019/#additional-readings","text":"","title":"Additional readings"},{"location":"courses/PSY5036F2019/#math-and-vision","text":"( EV ) Early Vision. Yuille and Kersten. In From Neuron to Cognition via Computational Neuroscience , M.A. Arbib, James J. Bonaiuto Editors, Cambridge MA: The MIT Press, in 2016 ( preprint pdf ) Understanding Vision: Theory, Models, and Data. Li Zhaoping. 2014.( publisher page ) ( author's web outline )","title":"Math and vision"},{"location":"courses/PSY5036F2019/#functional-human-vision","text":"( FV ) Foundations of Vision . Wandell ( web )","title":"Functional human vision"},{"location":"courses/PSY5036F2019/#neurophysiology","text":"( NVN ) The New Visual Neurosciences . John S. Werner and Leo M. Chalupa, edts. 2014.","title":"Neurophysiology"},{"location":"courses/PSY5036F2019/#software","text":"","title":"Software"},{"location":"courses/PSY5036F2019/#mathematica","text":"Mathematica is the primary programming environment for this course. Students who have registered for the course will have access through the Psychology Department's site license. Alternatives: Mathematica is available in several labs on campus, go to http://www.oit.umn.edu/computer-labs/software/index.htm You may wish to purchase Mathematica for Students see http://www.wolfram.com/products/student/mathforstudents/index.html . For help using Mathematica, see: http://mathematica.stackexchange.com","title":"Mathematica"},{"location":"courses/PSY5036F2019/#pythonipython","text":"http://ipython.org http://jupyter-notebook-beginner-guide.readthedocs.org/en/latest/index.html http://www.scipy.org For an online course in using Python and PsychoPy for research in human vision see: http://nbviewer.ipython.org/github/gestaltrevision/python_for_visres/blob/master/index.ipynb","title":"Python/IPython"},{"location":"courses/PSY5036F2019/#writing","text":"Gopen, G. D., & Swan, J. A., 1990. The Science of Scientific Writing. <u> American Scientist </u> , <u> 78 </u> , 550-558. Supplementary: The Sense of Style: The Thinking Person's Guide to Writing in the 21 st Century (2014), Pinker, Steven. ( amazon link ) Penrose, A. M., & Katz, S. B. (1998). <u> Writing in the Sciences: Exploring Conventions of Scientific Discourse </u> . New York: St. Martin's Press, Inc. American Psychological Association. (2009). <u> Publication manual of the American Psychological Association </u> (6 th ed.). Washington, DC: American Psychological Association Writing assistance. THE CENTER FOR WRITING offers free one-to-one writing assistance to undergraduate and graduate students, with appointments up to 45 minutes. Nonnative speaker specialists are available. For more information, see http://writing.umn.edu . Psychology department resources: http://writing.psych.umn.edu/student-resources","title":"Writing"},{"location":"courses/PSY5036F2019/#grade-requirements","text":"There will be programming assignments and a final project. The grade weights are: Exercise/programming assignments: 55% Final project in-class presentations: 5 % Final project : 40% (five parts: 2% (title and outline) +5%(first draft) +5% (peer commentary) +8% (cover letter response) + 20% (final draft)) The programming assignments will use the Mathematica programming environment. No prior experience with Mathematica is necessary. Assignment due By the 6 am on the day after the nominal due date. Late Policy: Assignments turned in within 24 hours following the due date will have 15% deducted from the assignment score. Assignments turned in between 24 and 48 hours following the due date will have 30% deducted from the score. Assignments more than 48 hours late will receive a score of zero.","title":"Grade Requirements"},{"location":"courses/PSY5036F2019/#lectures","text":"Check this section before each class for recent additions and revisions. Info Lecture notes are in Mathematica Notebook and pdf format. You can download the Mathematica notebook files below to view with Mathematica or Wolfram CDF Player (whichis free).","title":"Lectures"},{"location":"courses/PSY5036F2019/#final-project-assignment","text":"Goal: This course integrates the behavioral, neural and computational principles of perception. Students often find the interdisciplinary integration to be the most challenging aspect of the course. Through writing, you will learn to synthesize results from diverse and typically isolated disciplines. By writing about your project work, you will learn to think through the broader implications of your project, and to effectively communicate the rationale and results of your contribution in words. You will do a final page research report in which you will describe, in the form of a scientific paper, the results of an original computer program on a topic in computational vision. Your final project will involve: 1) a computer program and; 2) a 2000-3000 word final paper describing your project. For your computer project, you will do one of the following: 1) Write a program to simulate a model from the computer vision literature ; 2) Design and program a method for solving some problem in perception. 3) Design and program a psychophysical experiment to study an aspect of human visual perception. The results of your final project should be written up in the form of a short scientific paper or Mathematica Notebook, describing the motivation, methods, results, and interpretation. If you choose to write your program in Mathematica, your paper and program can be combined can be formated as a Mathematica notebook. See: Books and Tutorials on Notebooks. If you do your final project using Python, you can turn your paper in as a Jupyter notebook. Your paper will be critiqued and returned for you to revise and resubmit in final form. You should write for an audience consisting of your class peers. Completing the final paper involves 4 steps. Each step requires that you email a document to the teaching assistant. Outline (2% of grade). You will submit a working title and paragraph outline by the deadline noted in the syllabus. These outlines will be critiqued in order to help you find an appropriate focus for your papers. (Consult with the instructor or TA for ideas well ahead of time). Complete draft (5% of grade). A double-spaced, complete draft of the paper must be turned in by the deadline noted in the syllabus. Papers should be between 2000 and 3000 words. In addition to the title, author and date lines, papers must include the following sections: Abstract, Introduction, Methods, Results, Discussion, and Bibliography. Use citations to motivate your problem and to justify your claims. Cite authors by name and date, e.g. (Marr & Poggio, 1979). Citations should be original sources, not wikipedia. Use a standard citation format, such as APA. (The UM library has information on style guides , and in particular APA style .) Papers must be typed, with a page number on each page. Figures should be numbered and have figure captions. This draft will be reviewed by your instructor and one of you class peers. The point break down for the total 5% is: 2 pts for completing Introduction, 2 pts for completing Methods, 1 pt for completing Discussion) Peer commentary (5% of grade) . You will submit a written commentary (200 to 500 words) on a complete draft of one of your class peers. The project drafts and commentaries will be anonymous. The commentary should provide feedback to improve the quality and clarity of the writing. Final draft (20% of grade) and \"Cover letter\" (8% of grade). The final draft must be turned in by the date noted on the syllabus. The \"Cover letter\" should describe how your revision addressed comments from your peer evaluator and from your instructor. It should itemize key criticisms together with a brief description of the changes you made to your draft manuscript.","title":"Final Project Assignment"},{"location":"courses/PSY5038F2018/","text":"Introduction to Neural Networks PSY5038WF2018 \u00b6 University of Minnesota, Fall Semester, 2018 Instructor: Dan Kersten: kersten@umn.edu Abstract Introduction to large scale parallel distributed processing models in neural and cognitive science. Topics include: linear models, statistical pattern theory, Hebbian rules, self-organization, non-linear models, information optimization, and representation of neural information. Applications to sensory processing, perception, learning, and memory. Prerequisites Linear algebra, multivariate calculus. Software \u00b6 Mathematica \u00b6 Mathematica is the primary programming environment for this course. Students who have registered for the course will have Google Docs access through the Psychology Department\\'s site license . Alternatives: Mathematica is available in several labs on campus, go to http://www.oit.umn.edu/computer-labs/software/index.htm\\ You may wish to purchase Mathematica for Students see http://www.wolfram.com/products/student/mathforstudents/index.html . \\ You can also access Mathematica on the CLA servers: If you never programmed before go here . If you have programming experience, go here . For user help on using Mathematica, see: http://mathematica.stackexchange.com Learning center: http://www.wolfram.com/learningcenter/ Writing \u00b6 Gopen, G. D., & Swan, J. A., 1990. The Science of Scientific Writing. American Scientist, 78, 550-558. ( pdf ) **Supplementary: ** The Sense of Style: The Thinking Person\\'s Guide to Writing in the 21 st Century (2014), Pinker, Steven. ( amazon link ) Penrose, A. M., & Katz, S. B. (1998). Writing in the Sciences: Exploring Conventions of Scientific Discourse. New York: St. Martin\\'s Press, Inc. American Psychological Association. (2009). Publication manual of the American Psychological Association (6 th ed.). Washington, DC: American Psychological Association Writing assistance. THE CENTER FOR WRITING offers free one-to-one writing assistance to undergraduate and graduate students, with appointments up to 45 minutes. Nonnative speaker specialists are available. For more information, see http://writing.umn.edu . Psychology department resources: http://writing.psych.umn.edu/student-resources Grade Requirements \u00b6 There will be programming assignments, as well as a final project . The grade weights are: Exercise/programming assignments: 55% Final project presentations: 5 % Final project : 40% (four parts: 2%+5%+5%+28%)\\ \\ The programming assignments will use the Mathematica programming environment. No prior experience with Mathematica is necessary. \\ \\ Assignment due BEFORE class start time (9:45 am) on the day due. You can use the downloaded Mathematica notebook for the assignment as your template, add your answers, and email your finished assignment to the TA. You can copy and paste any code bits you need from the Lecture notebooks. But of course, you cannot copy and paste code or any other answer materials from someone else. Final Project Assignment \u00b6 This course teaches you how to understand cognitive and perceptual aspects of brain processing in terms of computation. Writing a computer program encourages you to think clearly about the assumptions underlying a given theory. Getting a program to work, however, tests just one level of clear thinking. By writing about your work, you will learn to think through the broader implications of your final project, and to effectively communicate the rationale and results in words. Your final project will involve: 1) a computer simulation and; 2) a 2000-3000 word final paper describing your simulation. For your computer project, you will do one of the following: 1) Devise a novel application for a neural network model studied in the course; 2) Write a program to simulate a model from the neural network literature ; 3) Design and program a method for solving some problem in perception, cognition or motor control. The results of your final project should be written up in the form of a short scientific paper, describing the motivation, methods, results, and interpretation. Your paper will be critiqued and returned for you to revise and resubmit in final form. You should write for an audience consisting of your class peers. You may elect to have your final paper published in the course\\'s web-based electronic journal. Completing the final paper involves 3 steps: Outline . You will submit a working title and paragraph outline by the deadline noted in the syllabus. These outlines will be critiqued in order to help you find an appropriate focus for your papers. ( 2% of grade). (Consult with the instructor or TA for ideas well ahead of time). Complete draft . You will then submit a complete draft of your paper ( 2000-3000 words ). Papers must include the following sections: Abstract, Introduction, Methods, Results, Discussion, and Bibliography. Use citations to motivate your problem and to justify your claims. Figures should be numbered and have figure captions. Cite authors by name and date, e.g. (Marr & Poggio, 1979). Use a standard citation format, such as APA. Papers must be typed, with a page number on each page.Each paper will be reviewed with specific recommendations for improvement. ( 5% of grade) Peer commentary . Each student will submit a paragraph on an anonymous paired project draft ( 5% of grade) Final draft . You will submit a final revision for grading. ( 28% of grade). The final draft must be turned in by the date noted on the syllabus. Students who wish to submit their final papers to be published in the class electronic journal should turn in both paper and electronic copies of their reports. If you choose to write your program in Mathematica, your paper and program can be combined can be formated as a Mathematica notebook. See: Books and Tutorials on Notebooks. Your paper will be critiqued and returned for you to revise and resubmit in final form. You should write for an audience consisting of your class peers.","title":"PSY5038F2018"},{"location":"courses/PSY5038F2018/#introduction-to-neural-networks-psy5038wf2018","text":"University of Minnesota, Fall Semester, 2018 Instructor: Dan Kersten: kersten@umn.edu Abstract Introduction to large scale parallel distributed processing models in neural and cognitive science. Topics include: linear models, statistical pattern theory, Hebbian rules, self-organization, non-linear models, information optimization, and representation of neural information. Applications to sensory processing, perception, learning, and memory. Prerequisites Linear algebra, multivariate calculus.","title":"Introduction to Neural Networks PSY5038WF2018"},{"location":"courses/PSY5038F2018/#software","text":"","title":"Software"},{"location":"courses/PSY5038F2018/#mathematica","text":"Mathematica is the primary programming environment for this course. Students who have registered for the course will have Google Docs access through the Psychology Department\\'s site license . Alternatives: Mathematica is available in several labs on campus, go to http://www.oit.umn.edu/computer-labs/software/index.htm\\ You may wish to purchase Mathematica for Students see http://www.wolfram.com/products/student/mathforstudents/index.html . \\ You can also access Mathematica on the CLA servers: If you never programmed before go here . If you have programming experience, go here . For user help on using Mathematica, see: http://mathematica.stackexchange.com Learning center: http://www.wolfram.com/learningcenter/","title":"Mathematica"},{"location":"courses/PSY5038F2018/#writing","text":"Gopen, G. D., & Swan, J. A., 1990. The Science of Scientific Writing. American Scientist, 78, 550-558. ( pdf ) **Supplementary: ** The Sense of Style: The Thinking Person\\'s Guide to Writing in the 21 st Century (2014), Pinker, Steven. ( amazon link ) Penrose, A. M., & Katz, S. B. (1998). Writing in the Sciences: Exploring Conventions of Scientific Discourse. New York: St. Martin\\'s Press, Inc. American Psychological Association. (2009). Publication manual of the American Psychological Association (6 th ed.). Washington, DC: American Psychological Association Writing assistance. THE CENTER FOR WRITING offers free one-to-one writing assistance to undergraduate and graduate students, with appointments up to 45 minutes. Nonnative speaker specialists are available. For more information, see http://writing.umn.edu . Psychology department resources: http://writing.psych.umn.edu/student-resources","title":"Writing"},{"location":"courses/PSY5038F2018/#grade-requirements","text":"There will be programming assignments, as well as a final project . The grade weights are: Exercise/programming assignments: 55% Final project presentations: 5 % Final project : 40% (four parts: 2%+5%+5%+28%)\\ \\ The programming assignments will use the Mathematica programming environment. No prior experience with Mathematica is necessary. \\ \\ Assignment due BEFORE class start time (9:45 am) on the day due. You can use the downloaded Mathematica notebook for the assignment as your template, add your answers, and email your finished assignment to the TA. You can copy and paste any code bits you need from the Lecture notebooks. But of course, you cannot copy and paste code or any other answer materials from someone else.","title":"Grade Requirements"},{"location":"courses/PSY5038F2018/#final-project-assignment","text":"This course teaches you how to understand cognitive and perceptual aspects of brain processing in terms of computation. Writing a computer program encourages you to think clearly about the assumptions underlying a given theory. Getting a program to work, however, tests just one level of clear thinking. By writing about your work, you will learn to think through the broader implications of your final project, and to effectively communicate the rationale and results in words. Your final project will involve: 1) a computer simulation and; 2) a 2000-3000 word final paper describing your simulation. For your computer project, you will do one of the following: 1) Devise a novel application for a neural network model studied in the course; 2) Write a program to simulate a model from the neural network literature ; 3) Design and program a method for solving some problem in perception, cognition or motor control. The results of your final project should be written up in the form of a short scientific paper, describing the motivation, methods, results, and interpretation. Your paper will be critiqued and returned for you to revise and resubmit in final form. You should write for an audience consisting of your class peers. You may elect to have your final paper published in the course\\'s web-based electronic journal. Completing the final paper involves 3 steps: Outline . You will submit a working title and paragraph outline by the deadline noted in the syllabus. These outlines will be critiqued in order to help you find an appropriate focus for your papers. ( 2% of grade). (Consult with the instructor or TA for ideas well ahead of time). Complete draft . You will then submit a complete draft of your paper ( 2000-3000 words ). Papers must include the following sections: Abstract, Introduction, Methods, Results, Discussion, and Bibliography. Use citations to motivate your problem and to justify your claims. Figures should be numbered and have figure captions. Cite authors by name and date, e.g. (Marr & Poggio, 1979). Use a standard citation format, such as APA. Papers must be typed, with a page number on each page.Each paper will be reviewed with specific recommendations for improvement. ( 5% of grade) Peer commentary . Each student will submit a paragraph on an anonymous paired project draft ( 5% of grade) Final draft . You will submit a final revision for grading. ( 28% of grade). The final draft must be turned in by the date noted on the syllabus. Students who wish to submit their final papers to be published in the class electronic journal should turn in both paper and electronic copies of their reports. If you choose to write your program in Mathematica, your paper and program can be combined can be formated as a Mathematica notebook. See: Books and Tutorials on Notebooks. Your paper will be critiqued and returned for you to revise and resubmit in final form. You should write for an audience consisting of your class peers.","title":"Final Project Assignment"},{"location":"courses/PSY8036SP2018/","text":"Data-driven generative models for perception, dreaming, and imagining PSY8036SP2018 \u00b6 University of Minnesota, Spring Semester, 2018 Topics in Computational Vision Psy 8036 (Kersten) Psy 5993 Section 034 (Schrater) Instructors: Dan Kersten kersten@umn.edu Paul Schrater schrater@umn.edu Abstract It has been proposed that perception is fundamentally a process of \u201canalysis-by-synthesis\u201d in which the sensory input is analyzed bottom-up, with perceptual interpretations tested and refined by top-down predictions of the input, through synthesis. However, while the computational and neural study of the analysis component is well-developed, less is known about the principles and mechanisms that underly synthesis. This seminar will explore recent advances using \u201cdeep\u201d learning algorithms to discover hierarchical statistical regularities in large datasets of natural patterns, and the relevance of the learning results to models of human perception and recognition. These algorithms also provide the basis for the stochastic synthesis of novel, yet familiar patterns, which raises the question of whether the human experiences of dreams and hallucinations, and the ability to imagine, reflect the same statistical regularities that are discoverable using machine learning. The class format will include short introductory lectures by the instructors, and weekly student presentations of current literature. The short lectures will provide historical context as well as tutorials on machine learning (e.g. TensorFlow for neural network simulations). Background \u00b6 There is a long history of theories of perception in which the brain \u201cexplains\u201d sensory input in terms of external, behaviorally relevant causes. A current hypothesis is that this process is implemented in part by cortical feedback mechanisms that synthesize predictions of early data representations in order to test how well the brain's current interpretation of the world corresponds with the sensory data. In this view, perception involves a cycle in which the incoming data triggers a set of explanations, i.e. hypotheses, which are used to measure how far the expected sensory input differs from the actual input. From a computational perspective, such generative models of perceptual inference have a number of advantages over strictly bottom-up inference. A generative model can incorporate measures of \"goodness-of-fit\" to decide whether to accept or reject an interpretation--some explanations are better than others. Discrepancies between sensory data and predictions may also be used to direct attentional resources and signal whether more complex combinations of hypotheses are needed. Further, with sufficient structure, a generative model could provide the basis for the perceptual interpretation of sensory input outside the range of past experience. While computational theories for bottom-up neural mechanisms for perception have received considerable scientific attention, much less is known about top-down mechanisms. This seminar will explore the idea that the brain has hierarchically structured mechanisms that can synthesize patterns of input representations with the following constraints: 1) the mechanisms build on inductive structural biases that are innate; 2) the mechanisms reflect the statistical regularities induced by the physical causes of sensory experience, i.e. they are \"data-driven\"; 3) the need for cognitive processes to access semantic, perceptual content over levels of abstraction. Assumptions 1) and 2) constrain the class of generative models to be \"data-driven\", i.e. models that can be learned from sensory data. Recent computational methods for data-driven pattern synthesis (e.g. VAE, InfoGAN, Adversarial Bayes, StackGAN) will be covered in this seminar. We will also explore the proposal that the same circuitry that may underly feedback in perception is used during imagery, dreams, and hallucinations. Tentative Syllabus \u00b6 Week Topics Background material Discussion topics and papers 1: Jan 16 Background Models of perception Yuille, A., & Kersten, D. (2006). Vision as Bayesian inference: analysis by synthesis? Trends in Cognitive Sciences, 10(7), 301\u2013308. 2: Jan 23 Overview of machine learning Ackley, D. H., Hinton, G. E., & Sejnowski, T. J. (1985). A learning algorithm for Boltzmann machines. Cognitive Science, 9(1), 147\u2013169. 3: Jan 30 Shallow image models, textures Zhu, S. C., Wu, Y., & Mumford, D. (1998). Filters, random fields and maximum entropy (FRAME): Towards a unified theory for texture modeling. International Journal of Computer Vision, 27(2), 107\u2013126. McDermott, J. H., Schemitsch, M., & Simoncelli, E. P. (2013). Summary statistics in auditory perception. Nature Publishing Group, 16(4), 493\u2013498. 4: Feb 6 Hierarchical image models, deep learning Zhu, S.-C., & Mumford, D. (2006). Quest for a stochastic grammar of images. Foundations and Trends\u00ae in Computer Graphics and Vision, 2(4), 259\u2013362. Topic preview: Visual imagery 5: Feb 13 Hierarchical image models, deep learning Topic preview: Auditory imagery 6: Feb 20 Hierarchical image models, deep learning Topic preview: Hypnagogic imagery 7: Feb 27 Dynamic textures, patterns Xie, J., & Zhu, S. C. (n.d.). Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet. arXiv.org. Vondrick, C., Pirsiavash, H., & Torralba, A. (2016). Generating Videos with Scene Dynamics. Advances in Neural Information Processing Systems NIPS, 613\u2013621. Topic preview: Dreams 8: Mar 6 Visual imagery Christophel, T. B., Klink, P. C., Spitzer, B., Roelfsema, P. R., & Haynes, J.-D. (2017). The Distributed Nature of Working Memory. Trends in Cognitive Sciences, 1\u201315. Dijkstra, N., Zeidman, P., Ondobaka, S., Gerven, M. A. J., & Friston, K. (2017). Distinct Top-down and Bottom-up Brain Connectivity During Visual Perception and Imagery. Scientific Reports, 1\u20139. Topic preview: Lucid dreaming Mar 13 Spring Break 9: Mar 20 Auditory, musical imagery Zatorre, R. J., & Halpern, A. R. (2005). Mental Concerts: Musical Imagery and Auditory Cortex. Neuron, 47(1), 9\u201312. Riecke, L., A. J. van Opstal, R. Goebel, and E. Formisano. \u201cHearing Illusory Sounds in Noise: Sensory-Perceptual Transformations in Primary Auditory Cortex.\u201d Journal of Neuroscience 27, no. 46 (November 14, 2007): 12684\u201389. McDermott, Josh H., and Andrew J. Oxenham. \u201cSpectral Completion of Partially Masked Sounds.\u201d Proceedings of the National Academy of Sciences 105, no. 15 (2008): 5939\u20135944. Topic overview: Hallucinations & psychedelics 10: Mar 27 Hypnagogic imagery Schacter, D. L. (1976). The hypnagogic state: a critical review of the literature. Psychological Bulletin. Topic preview: Hallucinations & schizophrenia 11: Apr 3 Dreams Stickgold, R., Hobson, J. A., Fosse, R., & Fosse, M. (2001). Sleep, Learning, and Dreams: Off-line Memory Reprocessing. Science, 294(5544), 1052\u20131057. Crick, F., G. Mitchison., 1983. The function of dream sleep. Nature. Springer Topic preview: Imagination 12: Apr 10 Lucid dreaming Voss, U., Holzmann, R., Tuin, I., , J. A. Hobson., 2009. Lucid dreaming: a state of consciousness with features of both waking and non-lucid dreaming. Sleep. 13: Apr 17 Hallucinations Seri\u00e8s, P., Reichert, D. P., & Storkey, A. J. (2010). Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model, 2020\u20132028. Ermentrout, G. B., & Cowan, J. D. (1979). A mathematical theory of visual hallucination patterns. Biological Cybernetics, 34(3), 137\u2013150 Howard, R. J., Brammer, M. J., David, A., Woodruff, P., & Williams, S. (1998). The anatomy of conscious vision: an fMRI study of visual hallucinations. Nature neuroscience, 1(8), 738-742. 14: Apr 24 Hallucinations Kumar, S., Sedley, W., Barnes, G. R., Teki, S., Friston, K. J., & Griffiths, T. D. (2014). A brain basis for musical hallucinations. Cortex, 52(C), 86\u201397 15: May 1 Imagination, art and design Friston, K. J., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., & Ondobaka, S. (2017). Active Inference, Curiosity and Insight. Neural Computation, 29(10), 2633\u20132683. 16: May 8 Finals week FINAL PROJECT PRESENTATIONS Sample Readings (under construction) \u00b6 Background \u00b6 Ackley, D. H., Hinton, G. E., & Sejnowski, T. J. (1985). A learning algorithm for Boltzmann machines. Cognitive Science, 9(1), 147\u2013169. Bastos, A. M., Usrey, W. M., Adams, R. A., Mangun, G. R., Fries, P., & Friston, K. J. (2012). Canonical Microcircuits for Predictive Coding. Neuron, 76(4), 695\u2013711. Berkes, P., Orban, G., Lengyel, M., & Fiser, J. (2011). Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment. Science, 331(6013), 83\u201387. Dayan, P., Hinton, G. E., Neal, R. M., & Zemel, R. S. (1995). The Helmholtz Machine. Neural Computation, 7(5), 889\u2013904. Ouden, den, H. E. M. (2012). How prediction errors shape perception, attention, and motivation, 1\u201312. Orban, G., Pietro Berkes, Fiser, J., & Lengyel, M. (2016). Neural Variability and Sampling-Based Probabilistic Representations in the Visual Cortex. Neuron, 92(2), 530\u2013543. MacKay, D. M. (1956). Towards an information-flow model of human behaviour. British Journal of Psychology (London, England : 1953), 47(1), 30\u201343. Lake, B. M., Salakhutdinov, R., & Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266), 1332\u20131338. http://doi.org/10.1126/science.aab3050 LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436\u2013444. http://doi.org/10.1038/nature14539 McDermott, Josh H., and Andrew J. Oxenham. \u201cSpectral Completion of Partially Masked Sounds.\u201d Proceedings of the National Academy of Sciences 105, no. 15 (2008): 5939\u20135944.Mumford, D. (1992). On the computational architecture of the neocortex. Biological Cybernetics, 66(3), 241\u2013251. Mumford, D. (1994). Pattern theory: a unifying perspective, 187\u2013224. Rao, R. P. N., & Ballard, D. H. (1999). Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nature Neuroscience, 2, 79\u201387. Tu, Z., Chen, X., Yuille, A. L., & Zhu, S.-C. (2005). Image parsing: Unifying segmentation, detection, and recognition. International Journal of Computer Vision, 63(2), 113\u2013140. Yuille, A., & Kersten, D. (2006). Vision as Bayesian inference: analysis by synthesis? Trends in Cognitive Sciences, 10(7), 301\u2013308. Richards, W. (1971). The Fortification Illusions of Migraines, Scientific American, 1\u201310. Zhu, S.-C., & Mumford, D. (2006). Quest for a stochastic grammar of images. Foundations and Trends\u00ae in Computer Graphics and Vision, 2(4), 259\u2013362. http://doi.org/10.1561/0600000018 Shallow generative models: Texture synthesis \u00b6 Freeman, J., & Simoncelli, E. P. (2011). Metamers of the ventral stream. Nature Publishing Group, 14(9), 1195\u20131201. http://doi.org/10.1038/nn.2889 McDermott, J. H., Schemitsch, M., & Simoncelli, E. P. (2013). Summary statistics in auditory perception. Nature Publishing Group, 16(4), 493\u2013498. McDermott, J. H., & Simoncelli, E. P. (2011). Sound Texture Perception via Statistics of the Auditory Periphery: Evidence from Sound Synthesis. Neuron, 71(5), 926\u2013940. Zhu, S. C., Wu, Y., & Mumford, D. (1998). Filters, random fields and maximum entropy (FRAME): Towards a unified theory for texture modeling. International Journal of Computer Vision, 27(2), 107\u2013126. Hierarchical (deep) data-driven generative models \u00b6 Chen, X., Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., & Abbeel, P. (2016). InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets, 2172\u20132180. Goodfellow, I. (2016, December 31). NIPS 2016 Tutorial: Generative Adversarial Networks. Kulkarni, T. D., Whitney, W. F., Kohli, P., & Tenenbaum, J. (2015). Deep Convolutional Inverse Graphics Network, 2539\u20132547. Rock, J., Issaranon, T., Deshpande, A., & Forsyth, D. (2016, December 5). Authoring image decompositions with generative models. Varol, G., Romero, J., Martin, X., Mahmood, N., Black, M. J., Laptev, I., & Schmid, C. (2017, January 5). Learning from Synthetic Humans. Xie, J., Zhu, S.-C., & Wu, Y. N. (2016, June 3). Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet. Yosinski, J., Clune, J., Nguyen, A., Fuchs, T., & Lipson, H. (2015, June 22). Understanding Neural Networks Through Deep Visualization. Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., & Metaxas, D. (2016, December 10). StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks. Hypnagogic imagery \u00b6 Gurstelle, E. B., & de Oliveira, J. L. (2004). Daytime parahypnagogia: a state of consciousness that occurs when we almost fall asleep. Medical Hypotheses, 62(2), 166\u2013168. http://doi.org/10.1016/S0306-9877(03)00306-2 Holmes, E. A., James, E. L., Coode-Bate, T., & Deeprose, C. (2009). Can Playing the Computer Game \u201cTetris\u201d Reduce the Build-Up of Flashbacks for Trauma? A Proposal from Cognitive Science. PLoS ONE, 4(1), e4153. http://doi.org/10.1371/journal.pone.0004153.t004 Nielsen, T. A. (1995). Describing and modeling hypnagogic imagery using a systematic self-observation procedure. Dreaming, 5(2), 75\u201394. http://doi.org/10.1037/h0094426 Nielsen, T. A. (2016). A Self-Observational Study of Spontaneous Hypnagogic Imagery Using the Upright Napping Procedure. Imagination, Cognition and Personality, 11(4), 353\u2013366. http://doi.org/10.2190/3LVV-L5GY-UR5V-N0TG *Schacter, D. L. (1976). The hypnagogic state: a critical review of the literature. Psychological Bulletin. Stickgold, R. (2000). Replaying the Game: Hypnagogic Images in Normals and Amnesics. Science, 290(5490), 350\u2013353. http://doi.org/10.1126/science.290.5490.350 Dreams \u00b6 Band, J. C. Z. F. A., 2016. (n.d.). Animal \u201cHypnosis\u201d and Waking Nightmares. Anomalistik.De Crick, F., G. Mitchison., 1983. The function of dream sleep. Nature. Springer *Hobson, J. A., & Mccarley, R. W. (197.). The brain as a dream state generator: an activation-synthesis hypothesis of the dream process. The American Journal of Psychiatry. Dresler, M., Koch, S. P., Wehrle, R., Spoormaker, V. I., Holsboer, F., Steiger, A., et al. (2011). Dreamed Movement Elicits Activation in the Sensorimotor Cortex. Current Biology : CB. Stickgold, R., Hobson, J. A., Fosse, R., & Fosse, M. (2001). Sleep, Learning, and Dreams: Off-line Memory Reprocessing. Science, 294(5544), 1052\u20131057. http://doi.org/10.1126/science.1063530 Stickgold, R. (2005). Sleep-dependent memory consolidation. Nature, 437(7063), 1272\u20131278. http://doi.org/10.1038/nature04286 Studies, J. H. J. O. C., 2014. (n.d.). Consciousness, dreams, and inference: the cartesian theatre revisited. Ingentaconnect.com Hallucinations \u00b6 Bressloff, P. C., Cowan, J. D., Golubitsky, M., Thomas, P. J., & Wiener, M. C. (2002). What geometric visual hallucinations tell us about the visual cortex. Neural Computation, 14(3), 473\u2013491. http://doi.org/10.1162/089976602317250861 Cummings, J. L., & Miller, B. L. (1987). Visual hallucinations. Clinical occurrence and use in differential diagnosis. The Western Journal of Medicine, 146(1), 46\u201351. *Ermentrout, G. B., & Cowan, J. D. (1979). A mathematical theory of visual hallucination patterns. Biological Cybernetics, 34(3), 137\u2013150. http://doi.org/10.1007/BF00336965 Merabet, L. B., Maguire, D., Warde, A., Alterescu, K., Stickgold, R., & Pascual-Leone, A. (2004). Visual hallucinations during prolonged blindfolding in sighted subjects. Journal of Neuro-Ophthalmology, 24(2), 109\u2013113. Howard, R. J., Brammer, M. J., David, A., Woodruff, P., & Williams, S. (1998). The anatomy of conscious vision: an fMRI study of visual hallucinations. Nature neuroscience, 1(8), 738-742.Seri\u00e8s, P., Reichert, D. P., & Storkey, A. J. (2010). Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model, 2020\u20132028. Silverstein, S. M. (2016). Visual Perception Disturbances in Schizophrenia: A Unified Model. In The Neuropsychopathology of Schizophrenia: Molecules, Brain Systems, Motivation, and Cognition (3 rd ed., Vol. 63, pp. 77\u2013132). Cham: Springer International Publishing. http://doi.org/10.1007/978-3-319-30596-7_4 Kumar, S., Sedley, W., Barnes, G. R., Teki, S., Friston, K. J., & Griffiths, T. D. (2014). A brain basis for musical hallucinations. Cortex, 52(C), 86\u201397. http://doi.org/10.1016/j.cortex.2013.12.002 Imagery and imagination \u00b6 Chetverikov, A., & Kristj\u00e1nsson, \u00c1. (2016). On the joys of perceiving: Affect as feedback for perceptual predictions. Actpsy, 169(C), 1\u201310. http://doi.org/10.1016/j.actpsy.2016.05.005 Dijkstra, N., Zeidman, P., Ondobaka, S., Gerven, M. A. J., & Friston, K. (2017). Distinct Top-down and Bottom-up Brain Connectivity During Visual Perception and Imagery. Scientific Reports, 1\u20139. http://doi.org/10.1038/s41598-017-05888-8 Friston, K. J., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., & Ondobaka, S. (2017). Active Inference, Curiosity and Insight. Neural Computation, 29(10), 2633\u20132683. http://doi.org/10.1162/neco_a_00999 Kosslyn, S. M., & Thompson, W. L. (2003). When is early visual cortex activated during visual mental imagery? Psychological Bulletin, 129(5), 723\u2013746. http://doi.org/10.1037/0033-2909.129.5.723 Kosslyn, S. M., Alpert, N. M., Thompson, W. L., Maljkovic, V., Weise, S. B., Chabris, C. F., et al. (1993). Visual Mental Imagery Activates Topographically Organized Visual Cortex: PET Investigations. Journal of Cognitive Neuroscience, 5(3), 263\u2013287. http://doi.org/10.1162/jocn.1993.5.3.263 Kosslyn, S., & Ganis, G. (2000). Neural foundations of imagery. Nature Reviews \u2026. Pearson, J., Naselaris, T., Holmes, E. A., & Kosslyn, S. M. (2015). Mental Imagery: Functional Mechanisms and Clinical Applications. Trends in Cognitive Sciences, 19(10), 590\u2013602. http://doi.org/10.1016/j.tics.2015.08.003 Riecke, L., A. J. van Opstal, R. Goebel, and E. Formisano. \u201cHearing Illusory Sounds in Noise: Sensory-Perceptual Transformations in Primary Auditory Cortex.\u201d Journal of Neuroscience 27, no. 46 (November 14, 2007): 12684\u201389. https://doi.org/10.1523/JNEUROSCI.2713-07.2007. Schacter, D. L., Addis, D. R., Hassabis, D., Martin, V. C., Spreng, R. N., & Szpunar, K. K. (2012). The Future of Memory: Remembering, Imagining, and the Brain. Neuron, 76(4), 677\u2013694. http://doi.org/10.1016/j.neuron.2012.11.001 Zatorre, R. J., & Halpern, A. R. (2005). Mental Concerts: Musical Imagery and Auditory Cortex. Neuron, 47(1), 9\u201312. http://doi.org/10.1016/j.neuron.2005.06.013 Imagery and memory \u00b6 Albers, A. M., Kok, P., Toni, I., Dijkerman, H. C., & de Lange, F. P. (2013). Shared Representations for Working Memory and Mental Imagery in Early Visual Cortex. Curbio, 23(15), 1427\u20131431. http://doi.org/10.1016/j.cub.2013.05.065 Christophel, T. B., Klink, P. C., Spitzer, B., Roelfsema, P. R., & Haynes, J.-D. (2017). The Distributed Nature of Working Memory. Trends in Cognitive Sciences, 1\u201315. http://doi.org/10.1016/j.tics.2016.12.007 Naselaris, T., Olman, C. A., Stansbury, D. E., Ugurbil, K., & Gallant, J. L. (2015). A voxel-wise encoding model for early visual areas decodes mental images of remembered scenes. NeuroImage, 105(C), 215\u2013228. http://doi.org/10.1016/j.neuroimage.2014.10.018 Self, M. W., van Kerkoerle, T., & Roelfsema, P. R. (2016). Layer-specificity in the effects of attention and working memory on activity in primary visual cortex. Nature Communications, 8, 1\u201312. http://doi.org/10.1038/ncomms13804 Stickgold, R. (2005). Sleep-dependent memory consolidation. Nature, 437(7063), 1272\u20131278. http://doi.org/10.1038/nature04286","title":"PSY8036SP2018"},{"location":"courses/PSY8036SP2018/#data-driven-generative-models-for-perception-dreaming-and-imagining-psy8036sp2018","text":"University of Minnesota, Spring Semester, 2018 Topics in Computational Vision Psy 8036 (Kersten) Psy 5993 Section 034 (Schrater) Instructors: Dan Kersten kersten@umn.edu Paul Schrater schrater@umn.edu Abstract It has been proposed that perception is fundamentally a process of \u201canalysis-by-synthesis\u201d in which the sensory input is analyzed bottom-up, with perceptual interpretations tested and refined by top-down predictions of the input, through synthesis. However, while the computational and neural study of the analysis component is well-developed, less is known about the principles and mechanisms that underly synthesis. This seminar will explore recent advances using \u201cdeep\u201d learning algorithms to discover hierarchical statistical regularities in large datasets of natural patterns, and the relevance of the learning results to models of human perception and recognition. These algorithms also provide the basis for the stochastic synthesis of novel, yet familiar patterns, which raises the question of whether the human experiences of dreams and hallucinations, and the ability to imagine, reflect the same statistical regularities that are discoverable using machine learning. The class format will include short introductory lectures by the instructors, and weekly student presentations of current literature. The short lectures will provide historical context as well as tutorials on machine learning (e.g. TensorFlow for neural network simulations).","title":"Data-driven generative models for perception, dreaming, and imagining PSY8036SP2018"},{"location":"courses/PSY8036SP2018/#background","text":"There is a long history of theories of perception in which the brain \u201cexplains\u201d sensory input in terms of external, behaviorally relevant causes. A current hypothesis is that this process is implemented in part by cortical feedback mechanisms that synthesize predictions of early data representations in order to test how well the brain's current interpretation of the world corresponds with the sensory data. In this view, perception involves a cycle in which the incoming data triggers a set of explanations, i.e. hypotheses, which are used to measure how far the expected sensory input differs from the actual input. From a computational perspective, such generative models of perceptual inference have a number of advantages over strictly bottom-up inference. A generative model can incorporate measures of \"goodness-of-fit\" to decide whether to accept or reject an interpretation--some explanations are better than others. Discrepancies between sensory data and predictions may also be used to direct attentional resources and signal whether more complex combinations of hypotheses are needed. Further, with sufficient structure, a generative model could provide the basis for the perceptual interpretation of sensory input outside the range of past experience. While computational theories for bottom-up neural mechanisms for perception have received considerable scientific attention, much less is known about top-down mechanisms. This seminar will explore the idea that the brain has hierarchically structured mechanisms that can synthesize patterns of input representations with the following constraints: 1) the mechanisms build on inductive structural biases that are innate; 2) the mechanisms reflect the statistical regularities induced by the physical causes of sensory experience, i.e. they are \"data-driven\"; 3) the need for cognitive processes to access semantic, perceptual content over levels of abstraction. Assumptions 1) and 2) constrain the class of generative models to be \"data-driven\", i.e. models that can be learned from sensory data. Recent computational methods for data-driven pattern synthesis (e.g. VAE, InfoGAN, Adversarial Bayes, StackGAN) will be covered in this seminar. We will also explore the proposal that the same circuitry that may underly feedback in perception is used during imagery, dreams, and hallucinations.","title":"Background"},{"location":"courses/PSY8036SP2018/#tentative-syllabus","text":"Week Topics Background material Discussion topics and papers 1: Jan 16 Background Models of perception Yuille, A., & Kersten, D. (2006). Vision as Bayesian inference: analysis by synthesis? Trends in Cognitive Sciences, 10(7), 301\u2013308. 2: Jan 23 Overview of machine learning Ackley, D. H., Hinton, G. E., & Sejnowski, T. J. (1985). A learning algorithm for Boltzmann machines. Cognitive Science, 9(1), 147\u2013169. 3: Jan 30 Shallow image models, textures Zhu, S. C., Wu, Y., & Mumford, D. (1998). Filters, random fields and maximum entropy (FRAME): Towards a unified theory for texture modeling. International Journal of Computer Vision, 27(2), 107\u2013126. McDermott, J. H., Schemitsch, M., & Simoncelli, E. P. (2013). Summary statistics in auditory perception. Nature Publishing Group, 16(4), 493\u2013498. 4: Feb 6 Hierarchical image models, deep learning Zhu, S.-C., & Mumford, D. (2006). Quest for a stochastic grammar of images. Foundations and Trends\u00ae in Computer Graphics and Vision, 2(4), 259\u2013362. Topic preview: Visual imagery 5: Feb 13 Hierarchical image models, deep learning Topic preview: Auditory imagery 6: Feb 20 Hierarchical image models, deep learning Topic preview: Hypnagogic imagery 7: Feb 27 Dynamic textures, patterns Xie, J., & Zhu, S. C. (n.d.). Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet. arXiv.org. Vondrick, C., Pirsiavash, H., & Torralba, A. (2016). Generating Videos with Scene Dynamics. Advances in Neural Information Processing Systems NIPS, 613\u2013621. Topic preview: Dreams 8: Mar 6 Visual imagery Christophel, T. B., Klink, P. C., Spitzer, B., Roelfsema, P. R., & Haynes, J.-D. (2017). The Distributed Nature of Working Memory. Trends in Cognitive Sciences, 1\u201315. Dijkstra, N., Zeidman, P., Ondobaka, S., Gerven, M. A. J., & Friston, K. (2017). Distinct Top-down and Bottom-up Brain Connectivity During Visual Perception and Imagery. Scientific Reports, 1\u20139. Topic preview: Lucid dreaming Mar 13 Spring Break 9: Mar 20 Auditory, musical imagery Zatorre, R. J., & Halpern, A. R. (2005). Mental Concerts: Musical Imagery and Auditory Cortex. Neuron, 47(1), 9\u201312. Riecke, L., A. J. van Opstal, R. Goebel, and E. Formisano. \u201cHearing Illusory Sounds in Noise: Sensory-Perceptual Transformations in Primary Auditory Cortex.\u201d Journal of Neuroscience 27, no. 46 (November 14, 2007): 12684\u201389. McDermott, Josh H., and Andrew J. Oxenham. \u201cSpectral Completion of Partially Masked Sounds.\u201d Proceedings of the National Academy of Sciences 105, no. 15 (2008): 5939\u20135944. Topic overview: Hallucinations & psychedelics 10: Mar 27 Hypnagogic imagery Schacter, D. L. (1976). The hypnagogic state: a critical review of the literature. Psychological Bulletin. Topic preview: Hallucinations & schizophrenia 11: Apr 3 Dreams Stickgold, R., Hobson, J. A., Fosse, R., & Fosse, M. (2001). Sleep, Learning, and Dreams: Off-line Memory Reprocessing. Science, 294(5544), 1052\u20131057. Crick, F., G. Mitchison., 1983. The function of dream sleep. Nature. Springer Topic preview: Imagination 12: Apr 10 Lucid dreaming Voss, U., Holzmann, R., Tuin, I., , J. A. Hobson., 2009. Lucid dreaming: a state of consciousness with features of both waking and non-lucid dreaming. Sleep. 13: Apr 17 Hallucinations Seri\u00e8s, P., Reichert, D. P., & Storkey, A. J. (2010). Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model, 2020\u20132028. Ermentrout, G. B., & Cowan, J. D. (1979). A mathematical theory of visual hallucination patterns. Biological Cybernetics, 34(3), 137\u2013150 Howard, R. J., Brammer, M. J., David, A., Woodruff, P., & Williams, S. (1998). The anatomy of conscious vision: an fMRI study of visual hallucinations. Nature neuroscience, 1(8), 738-742. 14: Apr 24 Hallucinations Kumar, S., Sedley, W., Barnes, G. R., Teki, S., Friston, K. J., & Griffiths, T. D. (2014). A brain basis for musical hallucinations. Cortex, 52(C), 86\u201397 15: May 1 Imagination, art and design Friston, K. J., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., & Ondobaka, S. (2017). Active Inference, Curiosity and Insight. Neural Computation, 29(10), 2633\u20132683. 16: May 8 Finals week FINAL PROJECT PRESENTATIONS","title":"Tentative Syllabus"},{"location":"courses/PSY8036SP2018/#sample-readings-under-construction","text":"","title":"Sample Readings (under construction)"},{"location":"courses/PSY8036SP2018/#background_1","text":"Ackley, D. H., Hinton, G. E., & Sejnowski, T. J. (1985). A learning algorithm for Boltzmann machines. Cognitive Science, 9(1), 147\u2013169. Bastos, A. M., Usrey, W. M., Adams, R. A., Mangun, G. R., Fries, P., & Friston, K. J. (2012). Canonical Microcircuits for Predictive Coding. Neuron, 76(4), 695\u2013711. Berkes, P., Orban, G., Lengyel, M., & Fiser, J. (2011). Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment. Science, 331(6013), 83\u201387. Dayan, P., Hinton, G. E., Neal, R. M., & Zemel, R. S. (1995). The Helmholtz Machine. Neural Computation, 7(5), 889\u2013904. Ouden, den, H. E. M. (2012). How prediction errors shape perception, attention, and motivation, 1\u201312. Orban, G., Pietro Berkes, Fiser, J., & Lengyel, M. (2016). Neural Variability and Sampling-Based Probabilistic Representations in the Visual Cortex. Neuron, 92(2), 530\u2013543. MacKay, D. M. (1956). Towards an information-flow model of human behaviour. British Journal of Psychology (London, England : 1953), 47(1), 30\u201343. Lake, B. M., Salakhutdinov, R., & Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266), 1332\u20131338. http://doi.org/10.1126/science.aab3050 LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436\u2013444. http://doi.org/10.1038/nature14539 McDermott, Josh H., and Andrew J. Oxenham. \u201cSpectral Completion of Partially Masked Sounds.\u201d Proceedings of the National Academy of Sciences 105, no. 15 (2008): 5939\u20135944.Mumford, D. (1992). On the computational architecture of the neocortex. Biological Cybernetics, 66(3), 241\u2013251. Mumford, D. (1994). Pattern theory: a unifying perspective, 187\u2013224. Rao, R. P. N., & Ballard, D. H. (1999). Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nature Neuroscience, 2, 79\u201387. Tu, Z., Chen, X., Yuille, A. L., & Zhu, S.-C. (2005). Image parsing: Unifying segmentation, detection, and recognition. International Journal of Computer Vision, 63(2), 113\u2013140. Yuille, A., & Kersten, D. (2006). Vision as Bayesian inference: analysis by synthesis? Trends in Cognitive Sciences, 10(7), 301\u2013308. Richards, W. (1971). The Fortification Illusions of Migraines, Scientific American, 1\u201310. Zhu, S.-C., & Mumford, D. (2006). Quest for a stochastic grammar of images. Foundations and Trends\u00ae in Computer Graphics and Vision, 2(4), 259\u2013362. http://doi.org/10.1561/0600000018","title":"Background"},{"location":"courses/PSY8036SP2018/#shallow-generative-models-texture-synthesis","text":"Freeman, J., & Simoncelli, E. P. (2011). Metamers of the ventral stream. Nature Publishing Group, 14(9), 1195\u20131201. http://doi.org/10.1038/nn.2889 McDermott, J. H., Schemitsch, M., & Simoncelli, E. P. (2013). Summary statistics in auditory perception. Nature Publishing Group, 16(4), 493\u2013498. McDermott, J. H., & Simoncelli, E. P. (2011). Sound Texture Perception via Statistics of the Auditory Periphery: Evidence from Sound Synthesis. Neuron, 71(5), 926\u2013940. Zhu, S. C., Wu, Y., & Mumford, D. (1998). Filters, random fields and maximum entropy (FRAME): Towards a unified theory for texture modeling. International Journal of Computer Vision, 27(2), 107\u2013126.","title":"Shallow generative models: Texture synthesis"},{"location":"courses/PSY8036SP2018/#hierarchical-deep-data-driven-generative-models","text":"Chen, X., Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., & Abbeel, P. (2016). InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets, 2172\u20132180. Goodfellow, I. (2016, December 31). NIPS 2016 Tutorial: Generative Adversarial Networks. Kulkarni, T. D., Whitney, W. F., Kohli, P., & Tenenbaum, J. (2015). Deep Convolutional Inverse Graphics Network, 2539\u20132547. Rock, J., Issaranon, T., Deshpande, A., & Forsyth, D. (2016, December 5). Authoring image decompositions with generative models. Varol, G., Romero, J., Martin, X., Mahmood, N., Black, M. J., Laptev, I., & Schmid, C. (2017, January 5). Learning from Synthetic Humans. Xie, J., Zhu, S.-C., & Wu, Y. N. (2016, June 3). Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet. Yosinski, J., Clune, J., Nguyen, A., Fuchs, T., & Lipson, H. (2015, June 22). Understanding Neural Networks Through Deep Visualization. Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., & Metaxas, D. (2016, December 10). StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks.","title":"Hierarchical (deep) data-driven generative models"},{"location":"courses/PSY8036SP2018/#hypnagogic-imagery","text":"Gurstelle, E. B., & de Oliveira, J. L. (2004). Daytime parahypnagogia: a state of consciousness that occurs when we almost fall asleep. Medical Hypotheses, 62(2), 166\u2013168. http://doi.org/10.1016/S0306-9877(03)00306-2 Holmes, E. A., James, E. L., Coode-Bate, T., & Deeprose, C. (2009). Can Playing the Computer Game \u201cTetris\u201d Reduce the Build-Up of Flashbacks for Trauma? A Proposal from Cognitive Science. PLoS ONE, 4(1), e4153. http://doi.org/10.1371/journal.pone.0004153.t004 Nielsen, T. A. (1995). Describing and modeling hypnagogic imagery using a systematic self-observation procedure. Dreaming, 5(2), 75\u201394. http://doi.org/10.1037/h0094426 Nielsen, T. A. (2016). A Self-Observational Study of Spontaneous Hypnagogic Imagery Using the Upright Napping Procedure. Imagination, Cognition and Personality, 11(4), 353\u2013366. http://doi.org/10.2190/3LVV-L5GY-UR5V-N0TG *Schacter, D. L. (1976). The hypnagogic state: a critical review of the literature. Psychological Bulletin. Stickgold, R. (2000). Replaying the Game: Hypnagogic Images in Normals and Amnesics. Science, 290(5490), 350\u2013353. http://doi.org/10.1126/science.290.5490.350","title":"Hypnagogic imagery"},{"location":"courses/PSY8036SP2018/#dreams","text":"Band, J. C. Z. F. A., 2016. (n.d.). Animal \u201cHypnosis\u201d and Waking Nightmares. Anomalistik.De Crick, F., G. Mitchison., 1983. The function of dream sleep. Nature. Springer *Hobson, J. A., & Mccarley, R. W. (197.). The brain as a dream state generator: an activation-synthesis hypothesis of the dream process. The American Journal of Psychiatry. Dresler, M., Koch, S. P., Wehrle, R., Spoormaker, V. I., Holsboer, F., Steiger, A., et al. (2011). Dreamed Movement Elicits Activation in the Sensorimotor Cortex. Current Biology : CB. Stickgold, R., Hobson, J. A., Fosse, R., & Fosse, M. (2001). Sleep, Learning, and Dreams: Off-line Memory Reprocessing. Science, 294(5544), 1052\u20131057. http://doi.org/10.1126/science.1063530 Stickgold, R. (2005). Sleep-dependent memory consolidation. Nature, 437(7063), 1272\u20131278. http://doi.org/10.1038/nature04286 Studies, J. H. J. O. C., 2014. (n.d.). Consciousness, dreams, and inference: the cartesian theatre revisited. Ingentaconnect.com","title":"Dreams"},{"location":"courses/PSY8036SP2018/#hallucinations","text":"Bressloff, P. C., Cowan, J. D., Golubitsky, M., Thomas, P. J., & Wiener, M. C. (2002). What geometric visual hallucinations tell us about the visual cortex. Neural Computation, 14(3), 473\u2013491. http://doi.org/10.1162/089976602317250861 Cummings, J. L., & Miller, B. L. (1987). Visual hallucinations. Clinical occurrence and use in differential diagnosis. The Western Journal of Medicine, 146(1), 46\u201351. *Ermentrout, G. B., & Cowan, J. D. (1979). A mathematical theory of visual hallucination patterns. Biological Cybernetics, 34(3), 137\u2013150. http://doi.org/10.1007/BF00336965 Merabet, L. B., Maguire, D., Warde, A., Alterescu, K., Stickgold, R., & Pascual-Leone, A. (2004). Visual hallucinations during prolonged blindfolding in sighted subjects. Journal of Neuro-Ophthalmology, 24(2), 109\u2013113. Howard, R. J., Brammer, M. J., David, A., Woodruff, P., & Williams, S. (1998). The anatomy of conscious vision: an fMRI study of visual hallucinations. Nature neuroscience, 1(8), 738-742.Seri\u00e8s, P., Reichert, D. P., & Storkey, A. J. (2010). Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model, 2020\u20132028. Silverstein, S. M. (2016). Visual Perception Disturbances in Schizophrenia: A Unified Model. In The Neuropsychopathology of Schizophrenia: Molecules, Brain Systems, Motivation, and Cognition (3 rd ed., Vol. 63, pp. 77\u2013132). Cham: Springer International Publishing. http://doi.org/10.1007/978-3-319-30596-7_4 Kumar, S., Sedley, W., Barnes, G. R., Teki, S., Friston, K. J., & Griffiths, T. D. (2014). A brain basis for musical hallucinations. Cortex, 52(C), 86\u201397. http://doi.org/10.1016/j.cortex.2013.12.002","title":"Hallucinations"},{"location":"courses/PSY8036SP2018/#imagery-and-imagination","text":"Chetverikov, A., & Kristj\u00e1nsson, \u00c1. (2016). On the joys of perceiving: Affect as feedback for perceptual predictions. Actpsy, 169(C), 1\u201310. http://doi.org/10.1016/j.actpsy.2016.05.005 Dijkstra, N., Zeidman, P., Ondobaka, S., Gerven, M. A. J., & Friston, K. (2017). Distinct Top-down and Bottom-up Brain Connectivity During Visual Perception and Imagery. Scientific Reports, 1\u20139. http://doi.org/10.1038/s41598-017-05888-8 Friston, K. J., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., & Ondobaka, S. (2017). Active Inference, Curiosity and Insight. Neural Computation, 29(10), 2633\u20132683. http://doi.org/10.1162/neco_a_00999 Kosslyn, S. M., & Thompson, W. L. (2003). When is early visual cortex activated during visual mental imagery? Psychological Bulletin, 129(5), 723\u2013746. http://doi.org/10.1037/0033-2909.129.5.723 Kosslyn, S. M., Alpert, N. M., Thompson, W. L., Maljkovic, V., Weise, S. B., Chabris, C. F., et al. (1993). Visual Mental Imagery Activates Topographically Organized Visual Cortex: PET Investigations. Journal of Cognitive Neuroscience, 5(3), 263\u2013287. http://doi.org/10.1162/jocn.1993.5.3.263 Kosslyn, S., & Ganis, G. (2000). Neural foundations of imagery. Nature Reviews \u2026. Pearson, J., Naselaris, T., Holmes, E. A., & Kosslyn, S. M. (2015). Mental Imagery: Functional Mechanisms and Clinical Applications. Trends in Cognitive Sciences, 19(10), 590\u2013602. http://doi.org/10.1016/j.tics.2015.08.003 Riecke, L., A. J. van Opstal, R. Goebel, and E. Formisano. \u201cHearing Illusory Sounds in Noise: Sensory-Perceptual Transformations in Primary Auditory Cortex.\u201d Journal of Neuroscience 27, no. 46 (November 14, 2007): 12684\u201389. https://doi.org/10.1523/JNEUROSCI.2713-07.2007. Schacter, D. L., Addis, D. R., Hassabis, D., Martin, V. C., Spreng, R. N., & Szpunar, K. K. (2012). The Future of Memory: Remembering, Imagining, and the Brain. Neuron, 76(4), 677\u2013694. http://doi.org/10.1016/j.neuron.2012.11.001 Zatorre, R. J., & Halpern, A. R. (2005). Mental Concerts: Musical Imagery and Auditory Cortex. Neuron, 47(1), 9\u201312. http://doi.org/10.1016/j.neuron.2005.06.013","title":"Imagery and imagination"},{"location":"courses/PSY8036SP2018/#imagery-and-memory","text":"Albers, A. M., Kok, P., Toni, I., Dijkerman, H. C., & de Lange, F. P. (2013). Shared Representations for Working Memory and Mental Imagery in Early Visual Cortex. Curbio, 23(15), 1427\u20131431. http://doi.org/10.1016/j.cub.2013.05.065 Christophel, T. B., Klink, P. C., Spitzer, B., Roelfsema, P. R., & Haynes, J.-D. (2017). The Distributed Nature of Working Memory. Trends in Cognitive Sciences, 1\u201315. http://doi.org/10.1016/j.tics.2016.12.007 Naselaris, T., Olman, C. A., Stansbury, D. E., Ugurbil, K., & Gallant, J. L. (2015). A voxel-wise encoding model for early visual areas decodes mental images of remembered scenes. NeuroImage, 105(C), 215\u2013228. http://doi.org/10.1016/j.neuroimage.2014.10.018 Self, M. W., van Kerkoerle, T., & Roelfsema, P. R. (2016). Layer-specificity in the effects of attention and working memory on activity in primary visual cortex. Nature Communications, 8, 1\u201312. http://doi.org/10.1038/ncomms13804 Stickgold, R. (2005). Sleep-dependent memory consolidation. Nature, 437(7063), 1272\u20131278. http://doi.org/10.1038/nature04286","title":"Imagery and memory"},{"location":"courses/PSY8036SP2019/","text":"Deep networks: Behavior, Brain and Theory PSY8036SP2019 \u00b6 University of Minnesota, Spring Semester, 2019 Topics in Computational Vision PSY 8036 PSY 5993 Section 034 Instructors: Dan Kersten: kersten@umn.edu Paul Schrater: schrater@umn.edu Abstract Recent rapid advances in deep learning networks have provided the means to produce \u201cimage computable models\u201d of human vision\u2013models that take natural images as input and produce accurate predictions of perceptual decisions. However, the current and future value of deep network research for understanding the brain\u2019s visual system faces both methodological and conceptual challenges. What are the best methods to compare deep networks to perceptual behavior and to the brain? And how can we achieve a conceptual understanding of the networks, to determine which elements are important and which are not? We will read and discuss empirical papers that compare network models of object recognition to behavior and the brain. The seminar will also review work that is helping to understand what functions networks can compute, and the limitations on learning to generalize. Finally, we will discuss advances that will be needed to understand the human ability to interpret virtually any image\u2013an ability that spans a wide range of visual tasks. The class format will include short introductory lectures by the instructors to provide historical context and weekly student presentations of current literature. Students will have the opportunity to collaborate on final programming projects. The course will also introduce and use Julia, a rapidly developing language for scientific programming, which is fast, flexible, and relatively easy to learn and use. Background \u00b6 Warning !!!DRAFT!!! There are good online resources for learning about artificial neural networks, and in particular deep convolutional neural networks. For video content, there is the Neural Networks for Machine Learning from Geoff Hinton\u2019s 2016 coursera lectures and Fei-Fei Li\u2019s Stanford 231n course, Convolutional Neural Networks for Visual Recognition . For books, see Ian Goodfellow\u2019s Deep Learning free online or for purchase , and Visual Cortex and Deep Networks: Learning Invariant Representations for purchase by Tomaso Poggio and Fabio Anselmi. For an excellent basic background and review, watch the first 9 lectures of the Stanford 231n course . Introduction \u00b6 Week 1 Deep architectures \u00b6 The first class will cover background and overview of the problems of human image understanding and visual recognition and how these problems have been approached. We\u2019ll go over the goals of the seminar in the context of three questions: 1) Is computational vision close to producing biologically consistent, predictive, models of human visual recognition performance? 2) assuming that candidate models exist; how well do we understand them for example, to decide when two networks are equivalent? 3) What is missing from current theories, in terms of conceptual understanding, behavioral functionality and their neural bases? To address the first question above, we\u2019ll review empirical papers, from psychophysics and neuroscience aimed at understanding the basic-level or \u201ccore\u201d function of rapid object identification with the goal of determining best ways of comparing network models to behavior and the brain. To address the second question, we will review theoretical work that seeks to understand what functions networks can compute, and how efficiently they can learn the parameters (e.g. \u201cweights\u201d) of those functions. Finally, we will assess where research may need to go to understand human ability to interpret virtually any image\u2013a challenge that will require advances in dynamic neural architectures that allow task flexibility. Background: For a short video Introduction to deep networks, see Lecture 1 from the MIT short course (6.S191): Introduction to Deep Learning. To run Julia programs and Jupyter notebooks locally on your computer, first install Julia , and then use the anaconda distribution to install Jupyter. For video instructions see: installing Julia and Jupyter . But the quickest and easiest way to start learning and using the Julia language is to sign in to JuliaBox where you\u2019ll immediately be able to create notebooks and access tutorials. Week 3 Deep networks and the brain & Intro to Julia programming \u00b6 Background: Video introduction to Julia programming. Introduction to Julia for Data Science and Scientific Computing Readings: Kietzmann, T. C., McClure, P., & Kriegeskorte, N. (2018, June 5). Deep Neural Networks In Computational Neuroscience. bioRxiv. doi: 10.1101/133504 Week 4 Deep networks and the brain reviews & Simulations with the Julia language \u00b6 Readings: Turner, M. H., Sanchez Giraldo, L. G., Schwartz, O., & Rieke, F. (2019, January). Stimulus- and goal-oriented frameworks for understanding natural vision. Nature Neuroscience, 22(1), 15\u201324. doi: 10.1038/s41593-018-0284-0 Jacobs, R. A. & Bates, C. J. (2018, November 27). Comparing the Visual Representations and Performance of Humans and Deep Neural Networks. Current Directions in Psychological Science, 0963721418801342. doi: 10.1177/0963721418801342 Kay, K. N. (2018, October 15). Principles for models of neural information processing. NeuroImage. New Advances in Encoding and Decoding of Brain Signals, 180, 101\u2013109. doi: 10.1016/j.neuroimage.2017.08.016 Empirical tests \u00b6 The most basic perceptual test is \u201clook and see\u201d which is what is often what is done by modelers. One can treat \u201cadversarial examples\u201d as behavioral tests to guide model development. However, one can be systematic and ask what parametric manipulations can be made of test images that we expect humans to generalize \u201cfor free\u201d. How best to design \u201cadversarial tasks\" for a model network? For example, based on typical experience, we expect little cost to testing on certain families of novel image variations. These manipulations can be based on 3D variables (e.g. object transformations such as 3D rotation, cast shadows, occlusion), or image variables (e.g. lower contrast, blur, noise). Manipulations can be based on highly artificial variations that we already know humans have generalization abilities, if limited, such as reverse contrast, non-linear histograms, morphs, and atypical occlusions. Given these manipulations, what are good quantitative measures that can be applied to both human and model observers? When does more model training solve the problem, and can we understand when to rule out a class of network architectures? Most current perceptual tests have been with feedforward DCNNs, and that is where we start. Week 5 Empirical tests of DCNNs: human object recognition \u00b6 Methodological challenges and the design of adversarial tasks. Readings: Ullman, S., Assif, L., Fetaya, E., & Harari, D. (2016, March 8). Atoms of recognition in human and computer vision. Proceedings of the National Academy of Sciences, 113(10), 2744\u20132749. doi: 10.1073/pnas.1513198113 . pmid: 26884200 Zhang, R., Isola, P., Efros, A. A., Shechtman, E., & Wang, O. (2018). The Unreasonable Effectiveness of Deep Features as a Perceptual Metric, 10. Retrieved from http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0299.pdf Elsayed, G. F., Shankar, S., Cheung, B., Papernot, N., Kurakin, A., Goodfellow, I., & Sohl-Dickstein, J. (2018, February 22). Adversarial Examples that Fool both Computer Vision and Time-Limited Humans. arXiv: 1802.08195 [cs, q-bio, stat] . Retrieved November 13, 2018, from http://arxiv.org/abs/1802.08195 Geirhos, R., Temme, C. R. M., Rauber, J., Sch\ufffdtt, H. H., Bethge, M., & Wichmann, F. A. (2018). Generalisation in humans and deep neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 31 (pp. 7549\u20137561). Curran Associates, Inc. Retrieved January 28, 2019, from http://papers.nips.cc/paper/7982-generalisation-in-humans-and-deep-neural-networks.pdf Week 6 Empirical tests of DCNNs and methods of study \u00b6 Including discussion of generative models, manifold discovery, posterior estimation \u2013 useful tools? Readings: Hill, M. Q., Parde, C. J., Castillo, C. D., Colon, Y. I., Ranjan, R., Chen, J.-C., \u2026, & O\u2019Toole, A. J. (2018, December 28). Deep Convolutional Neural Networks in the Face of Caricature: Identity and Image Revealed. arXiv: 1812.10902 [cs] . Retrieved January 23, 2019, from http://arxiv.org/abs/1812.10902 Zhang, M., Feng, J., Ma, K. T., Lim, J. H., Zhao, Q., & Kreiman, G. (2018, December). Finding any Waldo with zero-shot invariant and efficient visual search. Nature Communications, 9(1). doi: 10.1038/s41467-018-06217-x Ricci, M., Kim, J., & Serre, T. (2018, February 9). Same-different problems strain convolutional neural networks. arXiv: 1802.03390 [cs, q-bio] . Retrieved December 11, 2018, from http://arxiv.org/abs/1802.03390 Luo, W., Li, Y., Urtasun, R., & Zemel, R. (n.d.). Understanding the Effective Receptive Field in Deep Convolutional Neural Networks, 9 Week 7 DCNNs and the brain \u00b6 Zhou, B., Bau, D., Oliva, A., & Torralba, A. (2018). Interpreting Deep Visual Representations via Network Dissection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1\u20131. doi: 10.1109/TPAMI.2018.2858759 Readings: Rajalingham, R., Issa, E. B., Bashivan, P., Kar, K., Schmidt, K., & DiCarlo, J. J. (2018, February 12). Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks. doi: 10.1101/240614 Schrimpf, M., Kubilius, J., Hong, H., Majaj, N. J., Rajalingham, R., Issa, E. B., \u2026, & DiCarlo, J. J. (2018, September 5). Brain-Score: Which Artificial Neural Network for Object Recognition is most Brain-Like? doi: 10.1101/407007 Breedlove, J. L., St-Yves, G., Olman, C. A., & Naselaris, T. (2018, November 9). Human brain activity during mental imagery exhibits signatures of inference in a hierarchical generative model. doi: 10.1101/462226 Understanding deep networks \u00b6 Week 8 Theory: shallow vs. deep \u00b6 Universal approximators. Hierarchy as a solution to over-fitting. Poggio, T., Mhaskar, H., Rosasco, L., Miranda, B., & Liao, Q. (2017, October 1). Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review. International Journal of Automation and Computing, 14(5), 503\u2013519. doi: 10.1007/s11633-017-1054-2 Garriga-Alonso, A., Aitchison, L., & Rasmussen, C. E. (2019). DEEP CONVOLUTIONAL NETWORKS AS SHALLOW GAUSSIAN PROCESSES, 16 Lin, H. & Jegelka, S. (2018, June 28). ResNet with one-neuron hidden layers is a Universal Approximator. arXiv: 1806.10909 [cs, stat] . Retrieved January 18, 2019, from http://arxiv.org/abs/1806.10909 Week 9 Theory: DCNNs and implicit generative models \u00b6 Feedforward DCNNs, implicit generative models, texture, and maximum entropy. Readings: Xie, J., Zhu, S.-C., & Wu, Y. N. (2017). Synthesizing dynamic patterns by spatial-temporal generative convnet. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 7093\u20137101). Retrieved from http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Synthesizing_Dynamic_Patterns_CVPR_2017_paper.pdf Zhang, Q., Wu, Y. N., & Zhu, S.-C. (2017, October 2). Interpretable Convolutional Neural Networks. arXiv: 1710.00935 [cs] . Retrieved February 22, 2018, from http://arxiv.org/abs/1710.00935 WU, Y. N., XIE, J., LU, Y., & ZHU, S.-C. (2018). Sparse and Deep Generalizations of the FRAME Model. Retrieved from https://www.intlpress.com/site/pub/files/_fulltext/journals/amsa/2018/0003/0001/AMSA-2018-0003-0001-a007.pdf Week 10 Theory: invariance \u00b6 Readings: Tacchetti, A., Isik, L., & Poggio, T. A. (2018, September 15). Invariant Recognition Shapes Neural Representations of Visual Input. Annual Review of Vision Science, 4(1), 403\u2013422. doi: 10.1146/annurev-vision-091517-034103 Leibo, J. Z., Liao, Q., Anselmi, F., & Poggio, T. (2015, October 23). The Invariance Hypothesis Implies Domain-Specific Regions in Visual Cortex. PLOS Computational Biology, 11(10), e1004390. doi: 10.1371/journal.pcbi.1004390 Azulay, A. & Weiss, Y. (2018, May 30). Why do deep convolutional networks generalize so poorly to small image transformations? arXiv: 1805.12177 [cs] . Retrieved October 4, 2018, from http://arxiv.org/abs/1805.12177 Week 11 Theory: normalization \u00b6 The value and types of normalization: batch, spatial, temporal, and channel normalization. Liao, Q., Kawaguchi, K., & Poggio, T. (2016, October 19). Streaming Normalization: Towards Simpler and More Biologically-plausible Normalizations for Online and Recurrent Learning. arXiv: 1610.06160 [cs] . Retrieved December 7, 2018, from http://arxiv.org/abs/1610.06160 What\u2019s missing? \u00b6 Week 12 Compositionality and semantic accessibility \u00b6 The problems of occlusion and articulation (e.g. body pose). Computing spatial relationships,... Readings: Burgess, C. P., Matthey, L., Watters, N., Kabra, R., Higgins, I., Botvinick, M., & Lerchner, A. (2019, January 22). MONet: Unsupervised Scene Decomposition and Representation. arXiv: 1901.11390 [cs, stat] . Retrieved February 5, 2019, from http://arxiv.org/abs/1901.11390 Tang, H., Schrimpf, M., Lotter, W., Moerman, C., Paredes, A., Ortega Caro, J., \u2026, & Kreiman, G. (2018, August 28). Recurrent computations for visual pattern completion. Proceedings of the National Academy of Sciences, 115(35), 8835\u20138840. doi: 10.1073/pnas.1719397115 Zhang, Z., Xie, C., Wang, J.","title":"PSY8036SP2019"},{"location":"courses/PSY8036SP2019/#deep-networks-behavior-brain-and-theory-psy8036sp2019","text":"University of Minnesota, Spring Semester, 2019 Topics in Computational Vision PSY 8036 PSY 5993 Section 034 Instructors: Dan Kersten: kersten@umn.edu Paul Schrater: schrater@umn.edu Abstract Recent rapid advances in deep learning networks have provided the means to produce \u201cimage computable models\u201d of human vision\u2013models that take natural images as input and produce accurate predictions of perceptual decisions. However, the current and future value of deep network research for understanding the brain\u2019s visual system faces both methodological and conceptual challenges. What are the best methods to compare deep networks to perceptual behavior and to the brain? And how can we achieve a conceptual understanding of the networks, to determine which elements are important and which are not? We will read and discuss empirical papers that compare network models of object recognition to behavior and the brain. The seminar will also review work that is helping to understand what functions networks can compute, and the limitations on learning to generalize. Finally, we will discuss advances that will be needed to understand the human ability to interpret virtually any image\u2013an ability that spans a wide range of visual tasks. The class format will include short introductory lectures by the instructors to provide historical context and weekly student presentations of current literature. Students will have the opportunity to collaborate on final programming projects. The course will also introduce and use Julia, a rapidly developing language for scientific programming, which is fast, flexible, and relatively easy to learn and use.","title":"Deep networks: Behavior, Brain and Theory PSY8036SP2019"},{"location":"courses/PSY8036SP2019/#background","text":"Warning !!!DRAFT!!! There are good online resources for learning about artificial neural networks, and in particular deep convolutional neural networks. For video content, there is the Neural Networks for Machine Learning from Geoff Hinton\u2019s 2016 coursera lectures and Fei-Fei Li\u2019s Stanford 231n course, Convolutional Neural Networks for Visual Recognition . For books, see Ian Goodfellow\u2019s Deep Learning free online or for purchase , and Visual Cortex and Deep Networks: Learning Invariant Representations for purchase by Tomaso Poggio and Fabio Anselmi. For an excellent basic background and review, watch the first 9 lectures of the Stanford 231n course .","title":"Background"},{"location":"courses/PSY8036SP2019/#introduction","text":"","title":"Introduction"},{"location":"courses/PSY8036SP2019/#week-1-deep-architectures","text":"The first class will cover background and overview of the problems of human image understanding and visual recognition and how these problems have been approached. We\u2019ll go over the goals of the seminar in the context of three questions: 1) Is computational vision close to producing biologically consistent, predictive, models of human visual recognition performance? 2) assuming that candidate models exist; how well do we understand them for example, to decide when two networks are equivalent? 3) What is missing from current theories, in terms of conceptual understanding, behavioral functionality and their neural bases? To address the first question above, we\u2019ll review empirical papers, from psychophysics and neuroscience aimed at understanding the basic-level or \u201ccore\u201d function of rapid object identification with the goal of determining best ways of comparing network models to behavior and the brain. To address the second question, we will review theoretical work that seeks to understand what functions networks can compute, and how efficiently they can learn the parameters (e.g. \u201cweights\u201d) of those functions. Finally, we will assess where research may need to go to understand human ability to interpret virtually any image\u2013a challenge that will require advances in dynamic neural architectures that allow task flexibility. Background: For a short video Introduction to deep networks, see Lecture 1 from the MIT short course (6.S191): Introduction to Deep Learning. To run Julia programs and Jupyter notebooks locally on your computer, first install Julia , and then use the anaconda distribution to install Jupyter. For video instructions see: installing Julia and Jupyter . But the quickest and easiest way to start learning and using the Julia language is to sign in to JuliaBox where you\u2019ll immediately be able to create notebooks and access tutorials.","title":"Week 1 Deep architectures"},{"location":"courses/PSY8036SP2019/#week-3-deep-networks-and-the-brain-intro-to-julia-programming","text":"Background: Video introduction to Julia programming. Introduction to Julia for Data Science and Scientific Computing Readings: Kietzmann, T. C., McClure, P., & Kriegeskorte, N. (2018, June 5). Deep Neural Networks In Computational Neuroscience. bioRxiv. doi: 10.1101/133504","title":"Week 3 Deep networks and the brain &amp; Intro to Julia programming"},{"location":"courses/PSY8036SP2019/#week-4-deep-networks-and-the-brain-reviews-simulations-with-the-julia-language","text":"Readings: Turner, M. H., Sanchez Giraldo, L. G., Schwartz, O., & Rieke, F. (2019, January). Stimulus- and goal-oriented frameworks for understanding natural vision. Nature Neuroscience, 22(1), 15\u201324. doi: 10.1038/s41593-018-0284-0 Jacobs, R. A. & Bates, C. J. (2018, November 27). Comparing the Visual Representations and Performance of Humans and Deep Neural Networks. Current Directions in Psychological Science, 0963721418801342. doi: 10.1177/0963721418801342 Kay, K. N. (2018, October 15). Principles for models of neural information processing. NeuroImage. New Advances in Encoding and Decoding of Brain Signals, 180, 101\u2013109. doi: 10.1016/j.neuroimage.2017.08.016","title":"Week 4 Deep networks and the brain reviews &amp; Simulations with the Julia language"},{"location":"courses/PSY8036SP2019/#empirical-tests","text":"The most basic perceptual test is \u201clook and see\u201d which is what is often what is done by modelers. One can treat \u201cadversarial examples\u201d as behavioral tests to guide model development. However, one can be systematic and ask what parametric manipulations can be made of test images that we expect humans to generalize \u201cfor free\u201d. How best to design \u201cadversarial tasks\" for a model network? For example, based on typical experience, we expect little cost to testing on certain families of novel image variations. These manipulations can be based on 3D variables (e.g. object transformations such as 3D rotation, cast shadows, occlusion), or image variables (e.g. lower contrast, blur, noise). Manipulations can be based on highly artificial variations that we already know humans have generalization abilities, if limited, such as reverse contrast, non-linear histograms, morphs, and atypical occlusions. Given these manipulations, what are good quantitative measures that can be applied to both human and model observers? When does more model training solve the problem, and can we understand when to rule out a class of network architectures? Most current perceptual tests have been with feedforward DCNNs, and that is where we start.","title":"Empirical tests"},{"location":"courses/PSY8036SP2019/#week-5-empirical-tests-of-dcnns-human-object-recognition","text":"Methodological challenges and the design of adversarial tasks. Readings: Ullman, S., Assif, L., Fetaya, E., & Harari, D. (2016, March 8). Atoms of recognition in human and computer vision. Proceedings of the National Academy of Sciences, 113(10), 2744\u20132749. doi: 10.1073/pnas.1513198113 . pmid: 26884200 Zhang, R., Isola, P., Efros, A. A., Shechtman, E., & Wang, O. (2018). The Unreasonable Effectiveness of Deep Features as a Perceptual Metric, 10. Retrieved from http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0299.pdf Elsayed, G. F., Shankar, S., Cheung, B., Papernot, N., Kurakin, A., Goodfellow, I., & Sohl-Dickstein, J. (2018, February 22). Adversarial Examples that Fool both Computer Vision and Time-Limited Humans. arXiv: 1802.08195 [cs, q-bio, stat] . Retrieved November 13, 2018, from http://arxiv.org/abs/1802.08195 Geirhos, R., Temme, C. R. M., Rauber, J., Sch\ufffdtt, H. H., Bethge, M., & Wichmann, F. A. (2018). Generalisation in humans and deep neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 31 (pp. 7549\u20137561). Curran Associates, Inc. Retrieved January 28, 2019, from http://papers.nips.cc/paper/7982-generalisation-in-humans-and-deep-neural-networks.pdf","title":"Week 5 Empirical tests of DCNNs: human object recognition"},{"location":"courses/PSY8036SP2019/#week-6-empirical-tests-of-dcnns-and-methods-of-study","text":"Including discussion of generative models, manifold discovery, posterior estimation \u2013 useful tools? Readings: Hill, M. Q., Parde, C. J., Castillo, C. D., Colon, Y. I., Ranjan, R., Chen, J.-C., \u2026, & O\u2019Toole, A. J. (2018, December 28). Deep Convolutional Neural Networks in the Face of Caricature: Identity and Image Revealed. arXiv: 1812.10902 [cs] . Retrieved January 23, 2019, from http://arxiv.org/abs/1812.10902 Zhang, M., Feng, J., Ma, K. T., Lim, J. H., Zhao, Q., & Kreiman, G. (2018, December). Finding any Waldo with zero-shot invariant and efficient visual search. Nature Communications, 9(1). doi: 10.1038/s41467-018-06217-x Ricci, M., Kim, J., & Serre, T. (2018, February 9). Same-different problems strain convolutional neural networks. arXiv: 1802.03390 [cs, q-bio] . Retrieved December 11, 2018, from http://arxiv.org/abs/1802.03390 Luo, W., Li, Y., Urtasun, R., & Zemel, R. (n.d.). Understanding the Effective Receptive Field in Deep Convolutional Neural Networks, 9","title":"Week 6 Empirical tests of DCNNs and methods of study"},{"location":"courses/PSY8036SP2019/#week-7-dcnns-and-the-brain","text":"Zhou, B., Bau, D., Oliva, A., & Torralba, A. (2018). Interpreting Deep Visual Representations via Network Dissection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1\u20131. doi: 10.1109/TPAMI.2018.2858759 Readings: Rajalingham, R., Issa, E. B., Bashivan, P., Kar, K., Schmidt, K., & DiCarlo, J. J. (2018, February 12). Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks. doi: 10.1101/240614 Schrimpf, M., Kubilius, J., Hong, H., Majaj, N. J., Rajalingham, R., Issa, E. B., \u2026, & DiCarlo, J. J. (2018, September 5). Brain-Score: Which Artificial Neural Network for Object Recognition is most Brain-Like? doi: 10.1101/407007 Breedlove, J. L., St-Yves, G., Olman, C. A., & Naselaris, T. (2018, November 9). Human brain activity during mental imagery exhibits signatures of inference in a hierarchical generative model. doi: 10.1101/462226","title":"Week 7 DCNNs and the brain"},{"location":"courses/PSY8036SP2019/#understanding-deep-networks","text":"","title":"Understanding deep networks"},{"location":"courses/PSY8036SP2019/#week-8-theory-shallow-vs-deep","text":"Universal approximators. Hierarchy as a solution to over-fitting. Poggio, T., Mhaskar, H., Rosasco, L., Miranda, B., & Liao, Q. (2017, October 1). Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review. International Journal of Automation and Computing, 14(5), 503\u2013519. doi: 10.1007/s11633-017-1054-2 Garriga-Alonso, A., Aitchison, L., & Rasmussen, C. E. (2019). DEEP CONVOLUTIONAL NETWORKS AS SHALLOW GAUSSIAN PROCESSES, 16 Lin, H. & Jegelka, S. (2018, June 28). ResNet with one-neuron hidden layers is a Universal Approximator. arXiv: 1806.10909 [cs, stat] . Retrieved January 18, 2019, from http://arxiv.org/abs/1806.10909","title":"Week 8 Theory: shallow vs. deep"},{"location":"courses/PSY8036SP2019/#week-9-theory-dcnns-and-implicit-generative-models","text":"Feedforward DCNNs, implicit generative models, texture, and maximum entropy. Readings: Xie, J., Zhu, S.-C., & Wu, Y. N. (2017). Synthesizing dynamic patterns by spatial-temporal generative convnet. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 7093\u20137101). Retrieved from http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Synthesizing_Dynamic_Patterns_CVPR_2017_paper.pdf Zhang, Q., Wu, Y. N., & Zhu, S.-C. (2017, October 2). Interpretable Convolutional Neural Networks. arXiv: 1710.00935 [cs] . Retrieved February 22, 2018, from http://arxiv.org/abs/1710.00935 WU, Y. N., XIE, J., LU, Y., & ZHU, S.-C. (2018). Sparse and Deep Generalizations of the FRAME Model. Retrieved from https://www.intlpress.com/site/pub/files/_fulltext/journals/amsa/2018/0003/0001/AMSA-2018-0003-0001-a007.pdf","title":"Week 9 Theory: DCNNs and implicit generative models"},{"location":"courses/PSY8036SP2019/#week-10-theory-invariance","text":"Readings: Tacchetti, A., Isik, L., & Poggio, T. A. (2018, September 15). Invariant Recognition Shapes Neural Representations of Visual Input. Annual Review of Vision Science, 4(1), 403\u2013422. doi: 10.1146/annurev-vision-091517-034103 Leibo, J. Z., Liao, Q., Anselmi, F., & Poggio, T. (2015, October 23). The Invariance Hypothesis Implies Domain-Specific Regions in Visual Cortex. PLOS Computational Biology, 11(10), e1004390. doi: 10.1371/journal.pcbi.1004390 Azulay, A. & Weiss, Y. (2018, May 30). Why do deep convolutional networks generalize so poorly to small image transformations? arXiv: 1805.12177 [cs] . Retrieved October 4, 2018, from http://arxiv.org/abs/1805.12177","title":"Week 10 Theory: invariance"},{"location":"courses/PSY8036SP2019/#week-11-theory-normalization","text":"The value and types of normalization: batch, spatial, temporal, and channel normalization. Liao, Q., Kawaguchi, K., & Poggio, T. (2016, October 19). Streaming Normalization: Towards Simpler and More Biologically-plausible Normalizations for Online and Recurrent Learning. arXiv: 1610.06160 [cs] . Retrieved December 7, 2018, from http://arxiv.org/abs/1610.06160","title":"Week 11 Theory: normalization"},{"location":"courses/PSY8036SP2019/#whats-missing","text":"","title":"What\u2019s missing?"},{"location":"courses/PSY8036SP2019/#week-12-compositionality-and-semantic-accessibility","text":"The problems of occlusion and articulation (e.g. body pose). Computing spatial relationships,... Readings: Burgess, C. P., Matthey, L., Watters, N., Kabra, R., Higgins, I., Botvinick, M., & Lerchner, A. (2019, January 22). MONet: Unsupervised Scene Decomposition and Representation. arXiv: 1901.11390 [cs, stat] . Retrieved February 5, 2019, from http://arxiv.org/abs/1901.11390 Tang, H., Schrimpf, M., Lotter, W., Moerman, C., Paredes, A., Ortega Caro, J., \u2026, & Kreiman, G. (2018, August 28). Recurrent computations for visual pattern completion. Proceedings of the National Academy of Sciences, 115(35), 8835\u20138840. doi: 10.1073/pnas.1719397115 Zhang, Z., Xie, C., Wang, J.","title":"Week 12 Compositionality and semantic accessibility"},{"location":"datasets/camouflage/camouflage/","text":"Download \u00b6 The digital embryo camouflage challenge dataset: download This dataset provides 20 images of 9 objects. Each image has a novel camouflage albedo texture map, and a novel background of other digital embryos, also with a novel arrangements and camouflage patterns. The target object is always in front, i.e. unoccluded and \"in plain view\". The test set consists of 20 additional camouflaged images of the 9 objects. It is difficult to perceive the target object in any given image without experience. As an illustration of the effectiveness of the camouflage, note that on initial viewing a target object in the video below only becomes visible once it moves. Try pausing, then restarting the video. After a while, you may learn to recognize and segment the target object even in a static view. Making Digital Embryos \u00b6 Digital embryos are are created using a stochastic process which is analogous to embryological development. The novelty of the objects arises from the random initialization of growth \"hormones\" to the vertices of the polygon \"egge\". Because of the embryological nature of the algorithm, digital embryos bear some resemble to living forms, although they do not usually look like any particular plant or animal species. Digital embryos may have applications in art, entertainment, research, or anywhere novel 3D shapes are useful. Our interest is in vision research to understand how we learn novel objects. the embryos are used to study camouflage, visual learning, and object recognition. Brady, M. J., & Kersten, D. (2003). Bootstrapped learning of novel objects. J Vis, 3(6), 413-422.( pdf ) For more information on the algorithm, see: Brady, M. J. (1999). Psychophysical investigations of incomplete forms and forms with background (Order No. 9921424). Available from Dissertations & Theses @ CIC Institutions; ProQuest Dissertations & Theses A&I. (304522737) For more on the development and applications of digital embryos, see: Jay Hegd\u00e9's web page","title":"Camouflage"},{"location":"datasets/camouflage/camouflage/#download","text":"The digital embryo camouflage challenge dataset: download This dataset provides 20 images of 9 objects. Each image has a novel camouflage albedo texture map, and a novel background of other digital embryos, also with a novel arrangements and camouflage patterns. The target object is always in front, i.e. unoccluded and \"in plain view\". The test set consists of 20 additional camouflaged images of the 9 objects. It is difficult to perceive the target object in any given image without experience. As an illustration of the effectiveness of the camouflage, note that on initial viewing a target object in the video below only becomes visible once it moves. Try pausing, then restarting the video. After a while, you may learn to recognize and segment the target object even in a static view.","title":"Download"},{"location":"datasets/camouflage/camouflage/#making-digital-embryos","text":"Digital embryos are are created using a stochastic process which is analogous to embryological development. The novelty of the objects arises from the random initialization of growth \"hormones\" to the vertices of the polygon \"egge\". Because of the embryological nature of the algorithm, digital embryos bear some resemble to living forms, although they do not usually look like any particular plant or animal species. Digital embryos may have applications in art, entertainment, research, or anywhere novel 3D shapes are useful. Our interest is in vision research to understand how we learn novel objects. the embryos are used to study camouflage, visual learning, and object recognition. Brady, M. J., & Kersten, D. (2003). Bootstrapped learning of novel objects. J Vis, 3(6), 413-422.( pdf ) For more information on the algorithm, see: Brady, M. J. (1999). Psychophysical investigations of incomplete forms and forms with background (Order No. 9921424). Available from Dissertations & Theses @ CIC Institutions; ProQuest Dissertations & Theses A&I. (304522737) For more on the development and applications of digital embryos, see: Jay Hegd\u00e9's web page","title":"Making Digital Embryos"},{"location":"demos/lightness-shape/","text":"Lightness and shape \u00b6 The gradually shaded left and right halves of the flat object on the left are physically identical. The central regions of the left and right cylinders on the right also have identical intensity profiles. Further, all four of gradients are identical to across the central regions. The horizontal light intensity profiles are illustrated by the red lines. Your browser does not support HTML5 video. Knill, D. C., & Kersten, D. (1991). Apparent surface curvature affects lightness perception. Nature, 351, 228-230.","title":"Lightness and Shape"},{"location":"demos/lightness-shape/#lightness-and-shape","text":"The gradually shaded left and right halves of the flat object on the left are physically identical. The central regions of the left and right cylinders on the right also have identical intensity profiles. Further, all four of gradients are identical to across the central regions. The horizontal light intensity profiles are illustrated by the red lines. Your browser does not support HTML5 video. Knill, D. C., & Kersten, D. (1991). Apparent surface curvature affects lightness perception. Nature, 351, 228-230.","title":"Lightness and shape"},{"location":"demos/matte-shiny/","text":"Material from Motion \u00b6 Shiny or Matte? \u00b6 See: Hartung, B., & Kersten, D. (2002). Distinguishing shiny from matte [Abstract]. Journal of Vision, 2(7), 551a abstract . DOI 10.1167/2.7.551. This video shows how motion can affect material appearance. The first half of the movie simulates a chrome teapot rotating in mid-air. The appearance is what one might expect, that of a shiny chrome teapot. Half way through the movie, the reflection gets painted on to the teapot making a \"sticky reflection\". The painted-on pattern mimics what one would see in the reflection of a perfectly reflecting chrome teapot. Now the body of the teapot appears more or less like a painted matte object during the second half of the movie. Try stopping the movie. The teapot reverts to a more shiny appearance. Now with a croissant-shaped object: Your browser does not support the video tag. Many observers report that this polygonal object seems to look transparent rather than shiny-opaque. Your browser does not support the video tag. And the last one shows the \"wrong\" optic flow, in which the environment is moving about a different axis than the teapot. Your browser does not support the video tag. The highlights at points of high curvature in this last one conflict to some extent with the matte interpretation, and thus one can see these points as from a shiny object. Also, note the lack of contact information (inter-reflections are not modeled in this rendering) for the handle and spout. Your browser does not support the video tag. Movies were created by Bruce Hartung in collaboration with Dan Kersten and advice from Ted Adelson. The measured illumination map was obtained from http://www.debevec.org/Probes/ . This study was reported at the 2002 meeting of the Vision ScienceS Society in Sarasota, Florida: Hartung, B., & Kersten, D. (2002). Distinguishing Shiny from Matte. Presented at Vision SciencieS, Sarasota, Florida. For more on human perception of shiny objects, see: Doerschner, K., Fleming, R. W., Yilmaz, O., Schrater, P. R., Hartung, B., & Kersten, D. (2011). Visual Motion and the Perception of Surface Material. Current Biology, 21(23), 2010\u20132016. https://doi.org/10.1016/j.cub.2011.10.036 R.W. Fleming, R.O. Dror, and E.H. Adelson. How do Humans Determine Reflectance Properties under Unknown Illumination? Proceedings of the Workshop on Identifying Objects Across Variations in Lighting at CVPR, Hawaii, December, 2001.","title":"Shiny or Matte?"},{"location":"demos/matte-shiny/#material-from-motion","text":"","title":"Material from Motion"},{"location":"demos/matte-shiny/#shiny-or-matte","text":"See: Hartung, B., & Kersten, D. (2002). Distinguishing shiny from matte [Abstract]. Journal of Vision, 2(7), 551a abstract . DOI 10.1167/2.7.551. This video shows how motion can affect material appearance. The first half of the movie simulates a chrome teapot rotating in mid-air. The appearance is what one might expect, that of a shiny chrome teapot. Half way through the movie, the reflection gets painted on to the teapot making a \"sticky reflection\". The painted-on pattern mimics what one would see in the reflection of a perfectly reflecting chrome teapot. Now the body of the teapot appears more or less like a painted matte object during the second half of the movie. Try stopping the movie. The teapot reverts to a more shiny appearance. Now with a croissant-shaped object: Your browser does not support the video tag. Many observers report that this polygonal object seems to look transparent rather than shiny-opaque. Your browser does not support the video tag. And the last one shows the \"wrong\" optic flow, in which the environment is moving about a different axis than the teapot. Your browser does not support the video tag. The highlights at points of high curvature in this last one conflict to some extent with the matte interpretation, and thus one can see these points as from a shiny object. Also, note the lack of contact information (inter-reflections are not modeled in this rendering) for the handle and spout. Your browser does not support the video tag. Movies were created by Bruce Hartung in collaboration with Dan Kersten and advice from Ted Adelson. The measured illumination map was obtained from http://www.debevec.org/Probes/ . This study was reported at the 2002 meeting of the Vision ScienceS Society in Sarasota, Florida: Hartung, B., & Kersten, D. (2002). Distinguishing Shiny from Matte. Presented at Vision SciencieS, Sarasota, Florida. For more on human perception of shiny objects, see: Doerschner, K., Fleming, R. W., Yilmaz, O., Schrater, P. R., Hartung, B., & Kersten, D. (2011). Visual Motion and the Perception of Surface Material. Current Biology, 21(23), 2010\u20132016. https://doi.org/10.1016/j.cub.2011.10.036 R.W. Fleming, R.O. Dror, and E.H. Adelson. How do Humans Determine Reflectance Properties under Unknown Illumination? Proceedings of the Workshop on Identifying Objects Across Variations in Lighting at CVPR, Hawaii, December, 2001.","title":"Shiny or Matte?"},{"location":"demos/retinotopy/","text":"Illustration of log-polar mapping \u00b6 Retinal stimulation Corresponding V1 activity Mathematica code to do a cartesian to log-polar mapping download","title":"Retinotopy"},{"location":"demos/retinotopy/#illustration-of-log-polar-mapping","text":"Retinal stimulation Corresponding V1 activity Mathematica code to do a cartesian to log-polar mapping download","title":"Illustration of log-polar mapping"},{"location":"demos/shadows/","text":"Cast shadows and depth \u00b6 The strength of a cast shadow as a cue for depth can be illustrated by moving a shadow without moving the object casting the shadow. In the demo below, the green square doesn't move in the image, but it appears to move in depth, and in the image: See: Kersten, D., Knill, D., Mamassian, P. et al. Illusory motion from shadows. Nature 379, 31 (1996). local link . Nature site: (https://doi.org/10.1038/379031a0.). In the next demonstration, the image size of the ball is unchanged, but the position of the ball changes. The ball follows a diagonal trajectory inside a box. The ball's shadow first moves diagonally in a trajectory parallel to the ball, then it moves horizontally. The ball's trajectory is the same for both segments of the animation. The apparent motion in depth of the ball is strikingly different in the two cases. When the shadow is diagonal, the ball appears to slide along the floor to the back of the box. When the shadow trajectory is horizontal, the ball appears to rise above the floor of the box. See: Kersten, D., Mamassian, P., & Knill, D. C. (1997). Moving cast shadows induce apparent motion in depth. Perception, 26(2), 171-192. link Shadows and the bouncing ball \u00b6 The next demo provides a simple demonstration of vision's preference to interpret motion of a ball and its shadow as consistent with a stationary light source. 12 minutes worth of assorted shadow demonstrations \u00b6 Developed at the Max Planck Institute for Biological Cybernetics with Pascal Mamassian, Isabelle B\u00fclthoff, David Knill and Heinrich B\u00fclthoff in 1993. The video includes demonstrations of: how cast shadows can affect perceived size through depth; how motion of the light source are interpreted as changes in depth; apparent non-linear motion from cast shadows; how additional cues for light source motion don't help in interpreting ball motion as due to light source rather than depth changes; how cast shadows can reduce shape ambiguity; influence perceived rigidity; how the shape of an object doesn't need to match the shape or color of the \"shadow\" patch; and how the reversing the contrast of the \"shadow\"-i.e. a light rather than dark shadow--reduces the effect; and how illumination from below also can reduce the strength of the depth illusion. Shadows and sterescopic depth \u00b6 In this video, as in the above green square demo, the size of the green square doesn't change. But a stereoscopic cue is introduced that is in conflict with the shadow cue. The demonstration should be viewed with crossed-fusion--i.e. the left image to the right eye, and the right image to the left eye. The shadows are still effective at conveying a change in depth, despite the stereo disparity cue consistent with a depth change in the opposite direction\u2014at least initially. After a while, the disparity seems to win out and the green square move appears to move towards the checkerboard background and \"squeeze\" the dark shadow away, then recede from the background and \"suck\" the shadow under itself.","title":"Shadows"},{"location":"demos/shadows/#cast-shadows-and-depth","text":"The strength of a cast shadow as a cue for depth can be illustrated by moving a shadow without moving the object casting the shadow. In the demo below, the green square doesn't move in the image, but it appears to move in depth, and in the image: See: Kersten, D., Knill, D., Mamassian, P. et al. Illusory motion from shadows. Nature 379, 31 (1996). local link . Nature site: (https://doi.org/10.1038/379031a0.). In the next demonstration, the image size of the ball is unchanged, but the position of the ball changes. The ball follows a diagonal trajectory inside a box. The ball's shadow first moves diagonally in a trajectory parallel to the ball, then it moves horizontally. The ball's trajectory is the same for both segments of the animation. The apparent motion in depth of the ball is strikingly different in the two cases. When the shadow is diagonal, the ball appears to slide along the floor to the back of the box. When the shadow trajectory is horizontal, the ball appears to rise above the floor of the box. See: Kersten, D., Mamassian, P., & Knill, D. C. (1997). Moving cast shadows induce apparent motion in depth. Perception, 26(2), 171-192. link","title":"Cast shadows and depth"},{"location":"demos/shadows/#shadows-and-the-bouncing-ball","text":"The next demo provides a simple demonstration of vision's preference to interpret motion of a ball and its shadow as consistent with a stationary light source.","title":"Shadows and the bouncing ball"},{"location":"demos/shadows/#12-minutes-worth-of-assorted-shadow-demonstrations","text":"Developed at the Max Planck Institute for Biological Cybernetics with Pascal Mamassian, Isabelle B\u00fclthoff, David Knill and Heinrich B\u00fclthoff in 1993. The video includes demonstrations of: how cast shadows can affect perceived size through depth; how motion of the light source are interpreted as changes in depth; apparent non-linear motion from cast shadows; how additional cues for light source motion don't help in interpreting ball motion as due to light source rather than depth changes; how cast shadows can reduce shape ambiguity; influence perceived rigidity; how the shape of an object doesn't need to match the shape or color of the \"shadow\" patch; and how the reversing the contrast of the \"shadow\"-i.e. a light rather than dark shadow--reduces the effect; and how illumination from below also can reduce the strength of the depth illusion.","title":"12 minutes worth of assorted shadow demonstrations"},{"location":"demos/shadows/#shadows-and-sterescopic-depth","text":"In this video, as in the above green square demo, the size of the green square doesn't change. But a stereoscopic cue is introduced that is in conflict with the shadow cue. The demonstration should be viewed with crossed-fusion--i.e. the left image to the right eye, and the right image to the left eye. The shadows are still effective at conveying a change in depth, despite the stereo disparity cue consistent with a depth change in the opposite direction\u2014at least initially. After a while, the disparity seems to win out and the green square move appears to move towards the checkerboard background and \"squeeze\" the dark shadow away, then recede from the background and \"suck\" the shadow under itself.","title":"Shadows and sterescopic depth"},{"location":"demos/transparency/","text":"Transparency and structure from motion \u00b6 Bistability in Transparency and rigidity \u00b6 When you initially view, you will most likely see one transparent (darker) square surface in front of a (lighter) opaque square surface, together rotating about a single axis. This percept corresponds an interpretation of the front and back faces of a rotating cube viewed from slightly above, similar to: 1) two faces rotating about a single central axis. but with a transparent rather than opaque front face. However, if you look long enough at the first transparency video, eventually your perceptual interpretation will suddenly flip, and you will see the two surfaces apparently slipping and sliding over one another, where the lighter upper surface now appears transparent and in front, while the lower darker surface now appears opaque and in back. This percept looks like 2): It is as if your \"visual brain\" \"knows\" about the physics of rotation and surface transparency, coming up with the split axes interpretation even before you are cognitively aware that this is a second possible physical explanation for the rotation. Kersten, D., B\u00fclthoff, H. H., Schwartz, B., & Kurtz, K. (1992). Interaction between transparency and structure from motion. Neural Computation, 4(4), 573-589. These quicktime video demos were prepared by Gina Albanese.","title":"Transparency"},{"location":"demos/transparency/#transparency-and-structure-from-motion","text":"","title":"Transparency and structure from motion"},{"location":"demos/transparency/#bistability-in-transparency-and-rigidity","text":"When you initially view, you will most likely see one transparent (darker) square surface in front of a (lighter) opaque square surface, together rotating about a single axis. This percept corresponds an interpretation of the front and back faces of a rotating cube viewed from slightly above, similar to: 1) two faces rotating about a single central axis. but with a transparent rather than opaque front face. However, if you look long enough at the first transparency video, eventually your perceptual interpretation will suddenly flip, and you will see the two surfaces apparently slipping and sliding over one another, where the lighter upper surface now appears transparent and in front, while the lower darker surface now appears opaque and in back. This percept looks like 2): It is as if your \"visual brain\" \"knows\" about the physics of rotation and surface transparency, coming up with the split axes interpretation even before you are cognitively aware that this is a second possible physical explanation for the rotation. Kersten, D., B\u00fclthoff, H. H., Schwartz, B., & Kurtz, K. (1992). Interaction between transparency and structure from motion. Neural Computation, 4(4), 573-589. These quicktime video demos were prepared by Gina Albanese.","title":"Bistability in Transparency and rigidity"}]}