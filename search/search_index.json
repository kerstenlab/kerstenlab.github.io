{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kersten Computational Vision Lab # The Computational Vision Lab combines computational theory with behavioral and brain imaging experiments to understand how we see the world around us. Welcome! You can learn who we are and read more about our research and publications . Selected publications # Fan, X., Wang, L., Shao, H., Kersten, D., & He, S. (2016). Temporally flexible feedback signal to foveal cortex for peripheral object recognition. Proceedings of the National Academy of Sciences , 113 (41), 11627\u201311632. https://doi.org/10.1073/pnas.1606137113 Qiu, C., Burton, P. C., Kersten, D., & Olman, C. A. (2016). Responses in early visual areas to contour integration are context dependent. Journal of Vision , 16 (8), 19\u201319. https://doi.org/10.1167/16.8.19 Kersten, D., & Yuille, A. (2014). Inferential models of the visual cortical hierarchy. In M. S. Gazzaniga & G. R. Mangun (Eds.), The cognitive neurosciences (Fifth Edition, pp. 398\u2013404). /fulltext/Kersten2014.pdf Kersten, D., Mamassian, P., & Yuille, A. (2004). Object Perception as Bayesian Inference. Annu. Rev. Psychol. , 55 , 271\u2013304. https://doi.org/10.1146/annurev.psych.55.090902.142005 more","title":"Home"},{"location":"#kersten-computational-vision-lab","text":"The Computational Vision Lab combines computational theory with behavioral and brain imaging experiments to understand how we see the world around us. Welcome! You can learn who we are and read more about our research and publications .","title":"Kersten Computational Vision Lab"},{"location":"#selected-publications","text":"Fan, X., Wang, L., Shao, H., Kersten, D., & He, S. (2016). Temporally flexible feedback signal to foveal cortex for peripheral object recognition. Proceedings of the National Academy of Sciences , 113 (41), 11627\u201311632. https://doi.org/10.1073/pnas.1606137113 Qiu, C., Burton, P. C., Kersten, D., & Olman, C. A. (2016). Responses in early visual areas to contour integration are context dependent. Journal of Vision , 16 (8), 19\u201319. https://doi.org/10.1167/16.8.19 Kersten, D., & Yuille, A. (2014). Inferential models of the visual cortical hierarchy. In M. S. Gazzaniga & G. R. Mangun (Eds.), The cognitive neurosciences (Fifth Edition, pp. 398\u2013404). /fulltext/Kersten2014.pdf Kersten, D., Mamassian, P., & Yuille, A. (2004). Object Perception as Bayesian Inference. Annu. Rev. Psychol. , 55 , 271\u2013304. https://doi.org/10.1146/annurev.psych.55.090902.142005 more","title":"Selected publications"},{"location":"contact/","text":"Daniel J Kersten, PhD Department of Psychology kersten@umn.edu 612-625-2589 Lab Location N13 Elliott Hall 75 East River Road Minneapolis, MN 55455 Call Lab Phone: 612-625-1337 for access. Once in Elliott Hall, take the elevator on the North end of the building to the basement. The lab is in room N13. Mailing address Computational Vision Lab N218 Elliott Hall 75 East River Road Minneapolis, MN 55455","title":"Contact"},{"location":"people/","text":"People # The Computational Vision Lab involves people from several departments at the University of Minnesota, as well as colleagues in other laboratories in the US and overseas. Director # Daniel J Kersten, PhD Department of Psychology N218 Elliott Hall kersten@umn.edu 612-625-2589 scholar | homepage For Dan Kersten's academic genealogy see: https://neurotree.org/beta/tree.php?pid=415 Graduate Students # Doug Addleman Department of Psychology N218 Elliott Hall addle005@umn.edu scholar | homepage Alexander Bratch Department of Psychology N218 Elliott Hall bratc006@umn.edu scholar Yijun Ge Department of Psychology N10 Elliott Hall gexxx119@umn.edu Jiaqi Liu liu00687@umn.edu Siyun Liu Department of Psychology N218 Elliott Hall liux4433@umn.edu Ziwei Liu Department of Psychology N218 Elliott Hall liu00964@umn.edu Link Swanson Cognitive Science S309 Elliott Hall link@umn.edu scholar | homepage | github Undergraduates # Nicholos Schwartz Past Members Graduates and Postdocs # Gina Albanese Bruce Hartung Giovanni d'Avossa Scott Murray Peter Battaglia Evgeniy Bart Wendy Braje Stacy Ziegenhagen Charlie Benson Mark Brady Cheryl Olman Liming Shen Fang Fang Geon AE Eom Evangelos Theodoru Katja Doerschner Cindee Madison Alice O'Toole Huseyin Boyaci Erik J. Schlicht Neng Preeyakorn C. Shawn Green Bosco Tjan Paul Schrater Jay Hegd\u00e9 Brenton McMenamin Frank J\u00e4kel Margaret Sereno Jennifer Schumacher Pascal Mamassian Brian Stankiewicz David C. Knill Sara Lawson Serena Thompson Zili Liu Justin Potter An incomplete academic genealogy of past and current lab members is available at Neurotree . Collaborators Past and Present # Horace Barlow Peter Belhumeur Marina Bloj Heinrich B\u00fclthoff Isabelle B\u00fclthoff David J. Field Roland Fleming Wilson S Geisler Sheng He Xiaoping Hu Anya Hurlbert David Kriegman Gordon Legge Damien Mannion Bruno Olshausen Cheng Qiu Peter Shirley Michael Tarr William Thompson Nikolaus Troje Kamil Ugurbil David Woods Alan Yuille","title":"People"},{"location":"people/#people","text":"The Computational Vision Lab involves people from several departments at the University of Minnesota, as well as colleagues in other laboratories in the US and overseas.","title":"People"},{"location":"people/#director","text":"Daniel J Kersten, PhD Department of Psychology N218 Elliott Hall kersten@umn.edu 612-625-2589 scholar | homepage For Dan Kersten's academic genealogy see: https://neurotree.org/beta/tree.php?pid=415","title":"Director"},{"location":"people/#graduate-students","text":"Doug Addleman Department of Psychology N218 Elliott Hall addle005@umn.edu scholar | homepage Alexander Bratch Department of Psychology N218 Elliott Hall bratc006@umn.edu scholar Yijun Ge Department of Psychology N10 Elliott Hall gexxx119@umn.edu Jiaqi Liu liu00687@umn.edu Siyun Liu Department of Psychology N218 Elliott Hall liux4433@umn.edu Ziwei Liu Department of Psychology N218 Elliott Hall liu00964@umn.edu Link Swanson Cognitive Science S309 Elliott Hall link@umn.edu scholar | homepage | github","title":"Graduate Students"},{"location":"people/#undergraduates","text":"Nicholos Schwartz","title":"Undergraduates"},{"location":"people/#past-members-graduates-and-postdocs","text":"Gina Albanese Bruce Hartung Giovanni d'Avossa Scott Murray Peter Battaglia Evgeniy Bart Wendy Braje Stacy Ziegenhagen Charlie Benson Mark Brady Cheryl Olman Liming Shen Fang Fang Geon AE Eom Evangelos Theodoru Katja Doerschner Cindee Madison Alice O'Toole Huseyin Boyaci Erik J. Schlicht Neng Preeyakorn C. Shawn Green Bosco Tjan Paul Schrater Jay Hegd\u00e9 Brenton McMenamin Frank J\u00e4kel Margaret Sereno Jennifer Schumacher Pascal Mamassian Brian Stankiewicz David C. Knill Sara Lawson Serena Thompson Zili Liu Justin Potter An incomplete academic genealogy of past and current lab members is available at Neurotree .","title":"Past Members Graduates and Postdocs"},{"location":"people/#collaborators-past-and-present","text":"Horace Barlow Peter Belhumeur Marina Bloj Heinrich B\u00fclthoff Isabelle B\u00fclthoff David J. Field Roland Fleming Wilson S Geisler Sheng He Xiaoping Hu Anya Hurlbert David Kriegman Gordon Legge Damien Mannion Bruno Olshausen Cheng Qiu Peter Shirley Michael Tarr William Thompson Nikolaus Troje Kamil Ugurbil David Woods Alan Yuille","title":"Collaborators Past and Present"},{"location":"publications/","text":"Akin, Burak, Ceylan Ozdem, Seda Eroglu, Dudu Taslak Keskin, Fang Fang, Katja Doerschner, Daniel Kersten, and Huseyin Boyaci. 2014. \u201cAttention Modulates Neuronal Correlates of Interhemispheric Integration and Global Motion Perception.\u201d Journal of Vision 14 (12): 30\u201330. https://doi.org/10.1167/14.12.30 . Battaglia, Peter W, Massimiliano Di Luca, Marc O Ernst, Paul R Schrater, Tonja Machulla, and Daniel Kersten. 2010. \u201cWithin-and Cross-Modal Distance Information Disambiguate Visual Size-Change Perception.\u201d PLoS Computational Biology 6 (3). https://doi.org/10.1371/journal.pcbi.1000697 . Battaglia, Peter W, Daniel Kersten, and Paul R Schrater. 2011a. \u201cHow Haptic Size Sensations Improve Distance Perception.\u201d PLoS Computational Biology 7 (6). https://doi.org/10.1371/journal.pcbi.1002080 . \u2014\u2014\u2014. 2011b. \u201cThe Role of Generative Knowledge in Object Perception.\u201d In Sensory Cue Integration , edited by Julia Trommershauser, Konrad Kording, and Michael S Landy, 46\u201362. Oxford University Press. https://doi.org/10.1093/acprof:oso/9780195387247.003.0003 . Battaglia, Peter W, Paul R Schrater, and Daniel Kersten. 2005. \u201cAuxiliary Object Knowledge Influences Visually-Guided Interception Behavior.\u201d In Proceedings of the 2nd Symposium on Applied Perception in Graphics and Visualization , 145\u201352. https://doi.org/10.1145/1080402.1080430 . Bloj, M, DJ Kersten, and AC Hurlbert. 1999. \u201c3d Shape Perception Influences Colour Perception via Mutual Illumination.\u201d Nature 402: 877\u201379. https://doi.org/10.1038/47245 . Boyaci, Huseyin, Fang Fang, Scott O Murray, and Daniel Kersten. 2007. \u201cResponses to Lightness Variations in Early Human Visual Cortex.\u201d Current Biology 17 (11): 989\u201393. https://doi.org/10.1016/j.cub.2007.05.005 . \u2014\u2014\u2014. 2010. \u201cPerceptual Grouping-Dependent Lightness Processing in Human Early Visual Cortex.\u201d Journal of Vision 10 (9): 4\u20134. https://doi.org/10.1167/10.9.4 . Brady, Mark J, and Daniel Kersten. 2003. \u201cBootstrapped Learning of Novel Objects.\u201d Journal of Vision 3 (6): 2\u20132. https://doi.org/10.1167/3.6.2 . Braje, Wendy L, Daniel Kersten, Michael J Tarr, and Nikolaus F Troje. 1998. \u201cIllumination Effects in Face Recognition.\u201d Psychobiology 26 (4): 371\u201380. https://doi.org/10.3758/BF03330623 . Braje, Wendy L, Gordon E Legge, and Daniel Kersten. 2000. \u201cInvariant Recognition of Natural Objects in the Presence of Shadows.\u201d Perception 29 (4): 383\u201398. https://doi.org/10.1068/p3051 . Bratch, Alexander, Yixiong Chen, Stephen A Engel, and Daniel J Kersten. 2021. \u201cVisual Adaptation Selective for Individual Limbs Reveals Hierarchical Human Body Representation.\u201d Journal of Vision 21 (5): 18\u201318. https://doi.org/10.1167/jov.21.5.18 . Burkhardt, Dwight A, Jon Gottesman, Daniel Kersten, and Gordon E Legge. 1984. \u201cSymmetry and Constancy in the Perception of Negative and Positive Luminance Contrast.\u201d JOSA A 1 (3): 309\u201316. https://doi.org/10.1364/josaa.1.000309 . D\u2019AVOSSA, GIOVANNI, and Daniel Kersten. 1996. \u201cEvidence in Human Subjects for Independent Coding of Azimuth and Elevation for Direction of Heading from Optic Flow.\u201d Vision Research 36 (18): 2915\u201324. https://doi.org/10.1016/0042-6989(96)00010-7 . Doerschner, Katja, Roland W Fleming, Ozgur Yilmaz, Paul R Schrater, Bruce Hartung, and Daniel Kersten. 2011. \u201cVisual Motion and the Perception of Surface Material.\u201d Current Biology 21 (23): 2010\u201316. https://doi.org/10.1016/j.cub.2011.10.036 . Doerschner, Katja, Dan Kersten, and Paul Schrater. 2009. \u201cRapid Classification of Surface Reflectance from Image Velocities.\u201d In International Conference on Computer Analysis of Images and Patterns , 856\u201364. Springer. https://doi.org/10.1007/978-3-642-03767-2_104 . Doerschner, Katja, D Kersten, and Paul R Schrater. 2011. \u201cRapid Classification of Specular and Diffuse Reflection from Image Velocities.\u201d Pattern Recognition 44 (9): 1874\u201384. https://doi.org/10.1016/j.patcog.2010.09.007 . Fan, Xiaoxu, Lan Wang, Hanyu Shao, Daniel Kersten, and Sheng He. 2016. \u201cTemporally Flexible Feedback Signal to Foveal Cortex for Peripheral Object Recognition.\u201d Proceedings of the National Academy of Sciences 113 (41): 11627\u201332. https://doi.org/10.1073/pnas.1606137113 . Fang, Fang, Huseyin Boyaci, and Daniel Kersten. 2009. \u201cBorder Ownership Selectivity in Human Early Visual Cortex and Its Modulation by Attention.\u201d Journal of Neuroscience 29 (2): 460\u201365. https://doi.org/10.1523/jneurosci.4628-08.2009 . Fang, Fang, Huseyin Boyaci, Daniel Kersten, and Scott O Murray. 2008. \u201cAttention-Dependent Representation of a Size Illusion in Human V1.\u201d Current Biology 18 (21): 1707\u201312. https://doi.org/10.1016/j.cub.2008.09.025 . Fang, Fang, Daniel Kersten, and Scott O Murray. 2008. \u201cPerceptual Grouping and Inverse fMRI Activity Patterns in Human Visual Cortex.\u201d Journal of Vision 8 (7): 2\u20132. https://doi.org/10.1167/8.7.2 . Fang, Fang, Daniel Kersten, Paul R Schrater, and Alan L Yuille. 2004. \u201cHuman and Ideal Observers for Detecting Image Curves.\u201d In Advances in Neural Information Processing Systems , 1459\u201366. https://kerstenlab.psych.umn.edu/fulltext/Fang2004.pdf . Fang, Fang, Scott O Murray, Daniel Kersten, and Sheng He. 2005. \u201cOrientation-Tuned fMRI Adaptation in Human Visual Cortex.\u201d Journal of Neurophysiology 94 (6): 4188\u201395. https://doi.org/10.1152/jn.00378.2005 . Geisler, Wilson S, and Daniel Kersten. 2002. \u201cIllusions, Perception and Bayes.\u201d Nature Neuroscience 5 (6): 508. https://doi.org/10.1038/nn0602-508 . Gold, Jason M, Craig Abbey, Bosco S Tjan, and Daniel Kersten. 2009. \u201cIdeal Observers and Efficiency: Commemorating 50 Years of Tanner and Birdsall: Introduction.\u201d JOSA A 26 (11): IO1\u20132. https://doi.org/10.1364/josaa.26.000io1 . Green, C Shawn, Florian Kattner, Max H Siegel, Daniel Kersten, and Paul R Schrater. 2015. \u201cDifferences in Perceptual Learning Transfer as a Function of Training Task.\u201d Journal of Vision 15 (10): 5\u20135. https://doi.org/10.1167/15.10.5 . Green, CS, C Benson, D Kersten, and P Schrater. 2010. \u201cAlterations in Choice Behavior by Manipulations of World Model.\u201d Proceedings of the National Academy of Sciences 107 (37): 16401\u20136. https://doi.org/10.1073/pnas.1001709107 . Hartung, Bruce, Paul R Schrater, Heinrich H B\u00fclthoff, Daniel Kersten, and Volker H Franz. 2005. \u201cIs Prior Knowledge of Object Geometry Used in Visually Guided Reaching?\u201d Journal of Vision 5 (6): 2\u20132. https://doi.org/10.1167/5.6.2 . Hauffen, Karin, Eugene Bart, Mark Brady, Daniel Kersten, and Jay Hegd\u00e9. 2012. \u201cCreating Objects and Object Categories for Studying Perception and Perceptual Learning.\u201d JoVE (Journal of Visualized Experiments) , no. 69: e3358. https://doi.org/10.3791/3358 . He, Dongjun, Daniel Kersten, and Fang Fang. 2012. \u201cOpposite Modulation of High-and Low-Level Visual Aftereffects by Perceptual Grouping.\u201d Current Biology 22 (11): 1040\u201345. https://doi.org/10.1016/j.cub.2012.04.026 . Hegd\u00e9, Jay, Evgeniy Bart, and Daniel Kersten. 2008. \u201cFragment-Based Learning of Visual Object Categories.\u201d Current Biology 18 (8): 597\u2013601. https://doi.org/10.1016/j.cub.2008.03.058 . Hegd\u00e9, Jay, Fang Fang, Scott O Murray, and Daniel Kersten. 2008. \u201cPreferential Responses to Occluded Objects in the Human Visual Cortex.\u201d Journal of Vision 8 (4): 16\u201316. https://doi.org/10.1167/8.4.16 . Hegd\u00e9, Jay, and Daniel Kersten. 2010. \u201cA Link Between Visual Disambiguation and Visual Memory.\u201d Journal of Neuroscience 30 (45): 15124\u201333. https://doi.org/10.1523/jneurosci.4415-09.2010 . Hegd\u00e9, Jay, Serena K Thompson, Mark Brady, and Daniel Kersten. 2012. \u201cObject Recognition in Clutter: Cortical Responses Depend on the Type of Learning.\u201d Frontiers in Human Neuroscience 6: 170. https://doi.org/10.3389/fnhum.2012.00170 . Kam, Tae-Eui, Damien J Mannion, Seong-Whan Lee, Katja Doerschner, and Daniel J Kersten. 2015. \u201cHuman Visual Cortical Responses to Specular and Matte Motion Flows.\u201d Frontiers in Human Neuroscience 9: 579. https://doi.org/10.3389/fnhum.2015.00579 . Kersten, Daniel. 1983. \u201cA Comparison of Human and Ideal Performance for the Detection of Visual Pattern.\u201d PhD thesis, University of Minnesota. \u2014\u2014\u2014. 1984. \u201cSpatial Summation in Visual Noise.\u201d Vision Research 24 (12): 1977\u201390. https://doi.org/10.1016/0042-6989(84)90033-6 . \u2014\u2014\u2014. 1987a. \u201cPredictability and Redundancy of Natural Images.\u201d JOSA A 4 (12): 2395\u20132400. https://doi.org/10.1364/josaa.4.002395 . \u2014\u2014\u2014. 1987b. \u201cStatistical Efficiency for the Detection of Visual Noise.\u201d Vision Research 27 (6): 1029\u201340. https://doi.org/10.1016/0042-6989(87)90016-2 . \u2014\u2014\u2014. 1990. \u201cStatistical Limits to Image Understanding.\u201d In Vision: Coding and Efficiency , edited by Colin Blakemore, K Adler, and M Pointon, 32\u201344. Cambridge University Press Cambridge. https://doi.org/10.1017/cbo9780511626197.005 . \u2014\u2014\u2014. 1991. \u201cTransparency and the Cooperative Computation of Scene Attributes.\u201d In Computational Models of Visual Processing , edited by Michael S Landy, J Anthony Movshon, et al. The MIT Press. https://kerstenlab.psych.umn.edu/fulltext/Kersten1991.pdf . \u2014\u2014\u2014. 1996. \u201cCommentary on: Pattern Theory: A Unifying Perspective.\u201d In Perception as Bayesian Inference , edited by David C Knill and Whitman Richards, 213\u201336. Cambridge University Press. \u2014\u2014\u2014. 1997a. \u201cInverse 3-d Graphics: A Metaphor for Visual Perception.\u201d Behavior Research Methods, Instruments, & Computers 29 (1): 37\u201346. https://doi.org/10.3758/bf03200564 . \u2014\u2014\u2014. 1997b. \u201cPerceptual Categories for Spatial Layout.\u201d Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences 352 (1358): 1155\u201363. https://doi.org/10.1098/rstb.1997.0099 . \u2014\u2014\u2014. 2000. \u201cHigh-Level Vision as Statistical Inference.\u201d In The New Cognitive Neurosciences , edited by Michael Gazzaniga, Second Edition, 353\u201364. The MIT Press. https://kerstenlab.psych.umn.edu/fulltext/Kersten2000.pdf . \u2014\u2014\u2014. 2002. \u201cObject Perception: Generative Image Models and Bayesian Inference.\u201d In International Workshop on Biologically Motivated Computer Vision , 207\u201318. Springer. https://doi.org/10.1007/3-540-36181-2_21 . Kersten, Daniel J, and Suthep Madarasmi. 1993. \u201cThe Visual Perception of Surfaces, Their Properties and Relationships.\u201d In Partitioning Data Sets , 373\u201390. Kersten, Daniel, Heinrich H B\u00fclthoff, Bennett L Schwartz, and Kenneth J Kurtz. 1992. \u201cInteraction Between Transparency and Structure from Motion.\u201d Neural Computation 4 (4): 573\u201389. https://doi.org/10.1162/neco.1992.4.4.573 . Kersten, Daniel, David C Knill, Pascal Mamassian, and Isabelle B\u00fclthoff. 1996. \u201cIllusory Motion from Shadows.\u201d Nature 379 (6560): 31. https://doi.org/10.1038/379031a0 . Kersten, Daniel, and Gordon E Legge. 1983. \u201cConvergence Accommodation.\u201d JOSA 73 (3): 332\u201338. https://doi.org/10.1364/josa.73.000332 . Kersten, Daniel, and Pascal Mamassian. 2009. \u201cIdeal Observer Theory.\u201d In Encyclopedia of Neuroscience , edited by Larry R Squire, NF Dronkers, and JV Baldo, 89\u201395. Elsevier. https://doi.org/10.1016/b978-008045046-9.01435-2 . \u2014\u2014\u2014. 2016. \u201cCast Shadow Illusions.\u201d In The Oxford Compendium of Visual Illusions , edited by Arthur G Shapiro and Dejan Todorovic, 214\u201320. Oxford University Press. https://doi.org/10.1093/acprof:oso/9780199794607.003.0020 . Kersten, Daniel, Pascal Mamassian, and David C Knill. 1994. \u201cMoving Cast Shadows and the Perception of Relative Depth.\u201d \u2014\u2014\u2014. 1997. \u201cMoving Cast Shadows Induce Apparent Motion in Depth.\u201d Perception 26 (2): 171\u201392. https://doi.org/10.1068/p260171 . Kersten, Daniel, Pascal Mamassian, and Alan Yuille. 2004. \u201cObject Perception as Bayesian Inference.\u201d Annu. Rev. Psychol. 55: 271\u2013304. https://doi.org/10.1146/annurev.psych.55.090902.142005 . Kersten, Daniel, and Scott O Murray. 2010. \u201cVision: When Does Looking Bigger Mean Seeing Better?\u201d Current Biology 20 (9): R398\u201399. https://doi.org/10.1016/j.cub.2010.03.021 . Kersten, Daniel, Alice J O\u2019toole, Margaret E Sereno, David C Knill, and James A Anderson. 1987. \u201cAssociative Learning of Scene Parameters from Images.\u201d Applied Optics 26 (23): 4999\u20135006. https://doi.org/10.1364/ao.26.004999 . Kersten, Daniel, and Paul Schrater. 2002. \u201cPattern Inference Theory: A Probabilistic Approach to Vision.\u201d In Perception and the Physical World: Psychological and Philosophical Issues in Perception , edited by Dieter Heyer and Rainer Mausfeld. Wiley. https://doi.org/10.1002/0470013427.ch7 . Kersten, Daniel, Robert Shakespeare, and William Thompson. 2013. \u201cPredicting Visibility in Designs of Public Spaces.\u201d University of Utah Technical Reports UUCS 13-001. https://www.cs.utah.edu/docs/techreports/2013/pdf/UUCS-13-001.pdf . Kersten, Daniel, Nikolaus F Troje, and Heinrich H B\u00fclthoff. 1996. \u201cPhenomenal Competition for Poses of the Human Head.\u201d Perception 25 (3): 367\u201368. https://doi.org/10.1068/p250367 . Kersten, Daniel, and Alan Yuille. 2003. \u201cBayesian Models of Object Perception.\u201d Current Opinion in Neurobiology 13 (2): 150\u201358. https://doi.org/10.1016/s0959-4388(03)00042-4 . \u2014\u2014\u2014. 2014a. \u201cInferential Models of the Visual Cortical Hierarchy.\u201d In The Cognitive Neurosciences , edited by Michael S Gazzaniga and George R Mangun, Fifth Edition, 398\u2013404. The MIT Press. https://kerstenlab.psych.umn.edu/fulltext/KerstenYuilleNewCogNeuro2014.pdf . \u2014\u2014\u2014. 2014b. \u201cVision: Bayesian Inference and Beyond.\u201d In The New Visual Neurosciences , edited by John Simon Werner and Leo M Chalupa, 1263\u201378. The MIT Press. https://kerstenlab.psych.umn.edu/fulltext/Kersten2014a.pdf . Kersten, D, RF Hess, and GT Plant. 1988. \u201cAssessing Contrast Sensitivity Behind Cloudy Media.\u201d Clinical Vision Sciences 2 (3): 143\u201358. https://kerstenlab.psych.umn.edu/fulltext/Kersten1988.pdf . Knill, David C, David Field, and Daniel Kerstent. 1990. \u201cHuman Discrimination of Fractal Images.\u201d JOSA A 7 (6): 1113\u201323. https://doi.org/10.1364/josaa.7.001113 . Knill, David C., and Daniel Kersten. 2004. \u201cVisuomotor Sensitivity to Visual Information about Surface Orientation.\u201d Journal of Neurophysiology 91 (3): 1350\u201366. https://doi.org/10.1152/jn.00184.2003 . Knill, David C, and Daniel Kersten. 1990. \u201cLearning a Near-Optimal Estimator for Surface Shape from Shading.\u201d Computer Vision, Graphics, and Image Processing 50 (1): 75\u2013100. https://doi.org/10.1016/0734-189x(90)90068-7 . \u2014\u2014\u2014. 1991. \u201cApparent Surface Curvature Affects Lightness Perception.\u201d Nature 351 (6323): 228\u201330. https://doi.org/10.1038/351228a0 . Knill, David C, Daniel Kersten, and Alan Yuille. 1996. \u201cIntroduction: A Bayesian Formulation of Visual Perception.\u201d In Perception as Bayesian Inference , edited by David C Knill and Whitman Richards, 1\u201321. Cambridge University Press. https://doi.org/10.1017/CBO9780511984037.002 . Knill, David C, Pascal Mamassian, and Daniel Kersten. 1997. \u201cGeometry of Shadows.\u201d JOSA A 14 (12): 3216\u201332. https://doi.org/10.1364/josaa.14.003216 . Knill, DC, and Daniel Kersten. 1991. \u201cIdeal Perceptual Observers for Computation, Psychophysics and Neural Networks.\u201d In Pattern Recognition by Man and Machine (Vision and Visual Dysfunction) , edited by Roger J Watt. McMillan. Knill, DC, D Kersten, and P Mamassian. 1996. \u201cThe Bayesian Framework for Visual Information Processing: Implications for Psychophysics.\u201d In Perception as Bayesian Inference , edited by David C Knill and Whitman Richards, 239\u201386. Cambridge University Press. https://doi.org/10.1017/CBO9780511984037.009 . Legge, Gordon E, and Daniel Kersten. 1983. \u201cLight and Dark Bars; Contrast Discrimination.\u201d Vision Research 23 (5): 473\u201383. https://doi.org/10.1016/0042-6989(83)90122-0 . Legge, Gordon E, Daniel Kersten, and Arthur E Burgess. 1987. \u201cContrast Discrimination in Noise.\u201d JOSA A 4 (2): 391\u2013404. https://doi.org/10.1364/josaa.4.000391 . Liu, Siyun, Yichen Liu, Daniel J Kersten, Robert A Shakespeare, William B Thompson, and Gordon E Legge. 2021. \u201cValidating a Model of Architectural Hazard Visibility with Low-Vision Observers.\u201d PloS One 16 (11): e0260267. https://doi.org/10.1371/journal.pone.0260267 . Liu, Zili, and Daniel Kersten. 2003. \u201cThree-Dimensional Symmetric Shapes Are Discriminated More Efficiently Than Asymmetric Ones.\u201d JOSA A 20 (7): 1331\u201340. https://doi.org/10.1364/josaa.20.001331 . Liu, Zili, Daniel Kersten, and David C Knill. 1999. \u201cDissociating Stimulus Information from Internal Representation\u2014a Case Study in Object Recognition.\u201d Vision Research 39 (3): 603\u201312. https://doi.org/10.1016/s0042-6989(98)00167-9 . Liu, Zili, David C Knill, and Daniel Kersten. 1995. \u201cObject Classification for Human and Ideal Observers.\u201d Vision Research 35 (4): 549\u201368. https://doi.org/10.1016/0042-6989(94)00150-k . Liu, Z., and D. Kersten. 1998. \u201c2d Affine Transformations Cannot Account for Human 3d Object Recognition.\u201d In Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271) , 549\u201354. https://doi.org/10.1109/ICCV.1998.710770 . Madarasmi, S., Ting-Chuen Pong, and D. Kersten. 1994. \u201cIllusory Contour Detection Using MRF Models.\u201d In Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN94) , 7:4343\u20134348 vol.7. IEEE. https://doi.org/10.1109/icnn.1994.374966 . Madarasmi, Suthep, Daniel Kersten, and Ting-Chuen Pong. 1993a. \u201cThe Computation of Stereo Disparity for Transparent and for Opaque Surfaces.\u201d In Advances in Neural Information Processing Systems , 385\u201392. https://kerstenlab.psych.umn.edu/fulltext/MadarasmiKerstenComputationStereoTransparency1993.pdf . Madarasmi, Suthep, D Kersten, and Ting-Chuen Pong. 1993b. \u201cMulti-Layer Surface Segmentation Using Energy Minimization.\u201d In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition , 774\u201375. IEEE. MadarasmiKerstenPong1993.pdf . Madison, Cindee, William Thompson, Daniel Kersten, Peter Shirley, and Brian Smits. 2001. \u201cUse of Interreflection and Shadow for Surface Contact.\u201d Perception & Psychophysics 63 (2): 187\u201394. https://doi.org/10.3758/bf03194461 . Mamassian, Pascal, HH B\u00fclthoff, and Daniel Kersten. 1995. \u201cEye-Hand Coordination for 3-d Oriented Objects.\u201d https://kerstenlab.psych.umn.edu/fulltext/Mamassian1995.pdf . Mamassian, Pascal, and Daniel Kersten. 1996. \u201cIllumination, Shading and the Perception of Local Orientation.\u201d Vision Research 36 (15): 2351\u201367. https://doi.org/10.1016/0042-6989(95)00286-3 . Mamassian, Pascal, Daniel Kersten, and David C Knill. 1996. \u201cCategorical Local-Shape Perception.\u201d Perception 25 (1): 95\u2013107. https://doi.org/10.1068/p250095 . Mamassian, Pascal, David C Knill, and Daniel Kersten. 1998. \u201cThe Perception of Cast Shadows.\u201d Trends in Cognitive Sciences 2 (8): 288\u201395. https://doi.org/10.1016/s1364-6613(98)01204-2 . Mannion, Damien J, Daniel J Kersten, and Cheryl A Olman. 2014. \u201cRegions of Mid-Level Human Visual Cortex Sensitive to the Global Coherence of Local Image Patches.\u201d Journal of Cognitive Neuroscience 26 (8): 1764\u201374. https://doi.org/10.1162/jocn_a_00588 . \u2014\u2014\u2014. 2015. \u201cScene Coherence Can Affect the Local Response to Natural Images in Human V1.\u201d European Journal of Neuroscience 42 (11): 2895\u20132903. https://doi.org/10.1111/ejn.13082 . Mannion, Damien J, Daniel Kersten, and Cheryl A Olman. 2013. \u201cConsequences of Polar Form Coherence for fMRI Responses in Human Visual Cortex.\u201d NeuroImage 78: 152\u201358. https://doi.org/10.1016/j.neuroimage.2013.04.036 . McMenamin, Brenton W, Jasmine Radue, Joanna Trask, Kristin Huskamp, Daniel Kersten, and Chad J Marsolek. 2013. \u201cThe Diagnosticity of Color for Emotional Objects.\u201d Motivation and Emotion 37 (3): 609\u201322. https://doi.org/10.1007/s11031-012-9319-0 . Morgenstern, Yaniv, and Daniel J Kersten. 2017. \u201cThe Perceptual Dimensions of Natural Dynamic Flow.\u201d Journal of Vision 17 (12): 7\u20137. https://doi.org/10.1167/17.12.7 . Murray, Scott O, Huseyin Boyaci, and Daniel Kersten. 2006. \u201cThe Representation of Perceived Angular Size in Human Primary Visual Cortex.\u201d Nature Neuroscience 9 (3): 429\u201334. https://doi.org/10.1038/nn1641 . Murray, Scott O, Daniel Kersten, Bruno A Olshausen, Paul Schrater, and David L Woods. 2002. \u201cShape Perception Reduces Activity in Human Primary Visual Cortex.\u201d Proceedings of the National Academy of Sciences 99 (23): 15164\u201369. https://doi.org/10.1073/pnas.192579399 . Murray, Scott O, Cheryl A Olman, and Daniel Kersten. 2006. \u201cSpatially Specific fMRI Repetition Effects in Human Visual Cortex.\u201d Journal of Neurophysiology 95 (4): 2439\u201345. https://doi.org/10.1152/jn.01236.2005 . Murray, Scott O, Paul Schrater, and Daniel Kersten. 2004. \u201cPerceptual Grouping and the Interactions Between Visual Cortical Areas.\u201d Neural Networks 17 (5-6): 695\u2013705. https://doi.org/10.1016/j.neunet.2004.03.010 . Naor-Raz, Galit, Michael J Tarr, and Daniel Kersten. 2003. \u201cIs Color an Intrinsic Property of Object Representation?\u201d Perception 32 (6): 667\u201380. https://doi.org/10.1068/p5050 . O\u2019toole, Alice J, and Daniel J Kersten. 1992. \u201cLearning to See Random-Dot Stereograms.\u201d Perception 21 (2): 227\u201343. https://doi.org/10.1068/p210227 . Olman, Cheryl A, Kamil Ugurbil, Paul Schrater, and Daniel Kersten. 2004. \u201cBOLD fMRI and Psychophysical Measurements of Contrast Response to Broadband Images.\u201d Vision Research 44 (7): 669\u201383. https://doi.org/10.1016/j.visres.2003.10.022 . Olman, Cheryl, and Daniel Kersten. 2004. \u201cClassification Objects, Ideal Observers & Generative Models.\u201d Cognitive Science 28 (2): 227\u201339. https://doi.org/10.1207/s15516709cog2802_5 . Peterson, Lindsay M, Daniel J Kersten, and Damien J Mannion. 2018. \u201cSurface Curvature from Kinetic Depth Can Affect Lightness.\u201d Journal of Experimental Psychology: Human Perception and Performance 44 (12): 1856. https://doi.org/10.1037/xhp0000575 . Qiu, Cheng, Philip C Burton, Daniel Kersten, and Cheryl A Olman. 2016. \u201cResponses in Early Visual Areas to Contour Integration Are Context Dependent.\u201d Journal of Vision 16 (8): 19\u201319. https://doi.org/10.1167/16.8.19 . Qiu, Cheng, Daniel Kersten, and Cheryl A Olman. 2013. \u201cSegmentation Decreases the Magnitude of the Tilt Illusion.\u201d Journal of Vision 13 (13): 19\u201319. https://doi.org/10.1167/13.13.19 . Schrater, Paul R, and Daniel Kersten. 1999. \u201cStatistical Structure and Task Dependence in Visual Cue Integration.\u201d In Workshop on Statistical and Computational Theories of Vision\u2013Modeling, Learning, Computing, and Sampling, Fort Collins, Colorado . https://kerstenlab.psych.umn.edu/fulltext/Schrater1999.pdf . \u2014\u2014\u2014. 2000. \u201cHow Optimal Depth Cue Integration Depends on the Task.\u201d International Journal of Computer Vision 40 (1): 71\u201389. https://doi.org/10.1023/A:1026557704054 . Schrater, Paul, and Daniel Kersten. 2002. \u201cVision, Psychophysics and Bayes.\u201d In Probabilistic Models of the Brain: Perception and Neural Function , edited by Rajesh PN Rao, Bruno A Olshausen, and Michael S Lewicki, 37\u201360. The MIT press. Sereno, ME, Daniel Kersten, and J. A. Anderson. 1988. \u201cA Neural Network Model of an Aspect of Motion Perception.\u201d Science at the John von Neumann National Supercomputer Center , 173\u201378. Tarr, Michael J, Daniel Kersten, and Heinrich H B\u00fclthoff. 1998. \u201cWhy the Visual Recognition System Might Encode the Effects of Illumination.\u201d Vision Research 38 (15-16): 2259\u201375. https://doi.org/10.1016/s0042-6989(98)00041-8 . Thompson, William B, Daniel Kersten, and William R Knecht. 1992. \u201cStructure-from-Motion Based on Information at Surface Boundaries.\u201d Biological Cybernetics 66 (4): 327\u201333. https://doi.org/10.1007/bf00203669 . Thompson, William B, Gordon E Legge, Daniel J Kersten, Robert A Shakespeare, and Quan Lei. 2017. \u201cSimulating Visibility Under Reduced Acuity and Contrast Sensitivity.\u201d JOSA A 34 (4): 583\u201393. https://doi.org/10.1364/josaa.34.000583 . Thompson, William B, Robert A Shakespeare, Siyun Liu, Sarah H Creem-Regehr, Daniel J Kersten, and Gordon E Legge. 2021. \u201cEvaluating the Visibility of Architectural Features for People with Low Vision\u2013a Quantitative Approach.\u201d Leukos , 1\u201319. https://doi.org/10.1080/15502724.2021.1890115 . Thompson, William B, Peter Shirley, Brian Smits, Daniel J Kersten, and Cindee Madison. 1998. \u201cVisual Glue.\u201d University of Utah Technical Reports UUCS-98-007. https://kerstenlab.psych.umn.edu/fulltext/Thompson1998.pdf . Tjan, Bosco S, Wendy L Braje, Gordon E Legge, and Daniel Kersten. 1995. \u201cHuman Efficiency for Recognizing 3-d Objects in Luminance Noise.\u201d Vision Research 35 (21): 3053\u201369. https://doi.org/10.1016/0042-6989(95)00070-g . Troje, Nikolaus F, and Daniel Kersten. 1998. \u201cViewer-Centered Recognition of Familiar Faces.\u201d http://hdl.handle.net/11858/00-001M-0000-0013-E8F9-6 . \u2014\u2014\u2014. 1999. \u201cViewpoint-Dependent Recognition of Familiar Faces.\u201d Perception 28 (4): 483\u201387. https://doi.org/10.1068/p2901 . Vizioli, Luca, Federico De Martino, Lucy S Petro, Daniel Kersten, Kamil Ugurbil, Essa Yacoub, and Lars Muckli. 2019. \u201cMultivoxel Pattern of Blood Oxygen Level Dependent Activity Can Be Sensitive to Stimulus Specific Fine Scale Responses.\u201d bioRxiv . https://doi.org/10.1101/798306 . \u2014\u2014\u2014. 2020. \u201cMultivoxel Pattern of Blood Oxygen Level Dependent Activity Can Be Sensitive to Stimulus Specific Fine Scale Responses.\u201d Scientific Reports 10 (1): 1\u201318. https://doi.org/doi.org/10.1167/jov.21.5.18 . Yuille, Alan, and Daniel Kersten. 2006. \u201cVision as Bayesian Inference: Analysis by Synthesis?\u201d Trends in Cognitive Sciences 10 (7): 301\u20138. https://doi.org/10.1016/j.tics.2006.05.002 . \u2014\u2014\u2014. 2016. \u201cEarly Vision.\u201d In From Neuron to Cognition via Computational Neuroscience , edited by Michael A Arbib and James J Bonaiuto, 345\u2013408. MIT Press. https://kerstenlab.psych.umn.edu/fulltext/Yuille2016.pdf . Zhu, Hongru, Alan Yuille, and Daniel Kersten. 2021. \u201cThree-Dimensional Pose Discrimination in Natural Images of Humans.\u201d In Proceedings of the Annual Meeting of the Cognitive Science Society . Vol. 43. 43. https://escholarship.org/uc/item/0ns293tt .","title":"Publications"},{"location":"research/","text":"Research # The Computational Vision Lab combines computational theory with behavioral and brain image experiments to understand how we see the world around us. Vision is used for recognition, planning and motor actions. Exactly how to get from the retinal image intensities to useful actions is a tough problem requiring multiple approaches. A major theoretical challenge is to discover the computational principles required to infer world properties and determine motor output from images. Computational vision searches for these solutions. The empirical challenge is to discover how our visual systems and those of other animals are built to achieve useful actions from the images received. Approach # Our research uses computational, psychophysical, and brain imaging approaches to investigate how the visual pathways of the brain transform image information into useful plans and action. Methodologies # psychophysics brain imaging (fMRI) neurophysiology computational modeling machine learning and computer vision theoretical neuroscience Topics of interest # perceptual inference perceptual organization object perception perceptual learning vision and action cue integration cortical processing top-down (feedback) and bottom-up (feed-forward) interaction inhibition suppression adaptation Phenomena of interest # natural scenes context effects illusion aftereffects shape, size, surface reflectance color motion occlusion, crowding, clutter shadow camouflage Theory and computation # statistical (Bayesian) inference 1 2 ideal observer models analysis by synthesis predictive coding neural networks hierarchical generative models Innovation # Vision as statistical inference Bayesian approaches # Bayesian approaches have enjoyed a great deal of recent success in their application to problems in computer vision. This success has led to an emerging interest in applying Bayesian methods to modeling human visual perception. We consider the implications of a Bayesian view of visual information processing for experimentally investigating human visual perception. We have outlined the elements of a general program of empirical research which results from taking the basic Bayesian formulation seriously not only as a means for objectively modeling image information through ideal observer analysis (e.g. see our work in object recognition), but also as a framework for characterizing human perceptual inference. A major advantage of following such a program is that, because its structure is the same as that of the Bayesian framework for computational modeling, it supports a strong integration of psychophysics and computational theory. In particular, it provides the foundation for a psychophysics of constraints in which one tests hypotheses regarding quantitative and qualitative constraints used in human perceptual inferences. The Bayesian approach also suggests new ways to conceptualize the general problem of perception and to decompose it into isolable parts for psychophysical investigation; that is, it not only provides a framework for modeling solutions to specific perceptual problems; it also guides the definition of the problems. Analysis by synthesis Analysis by synthesis. (a) Low-level processing can extract edge features, such as bars, and use conjunctions of these features to make bottom-up proposals to access the higher-level models of objects. (b) The high-level objects access the image top-down to validate or reject the bottom-up proposals. In this example, the low-level cues propose that the image can be interpreted as an E, an F, or a set of parallel bars. But interpreting it as an F explains almost all the features in the image and is preferred. 1 2 Alan Yuille and Daniel Kersten. Vision as bayesian inference: analysis by synthesis? Trends in cognitive sciences , 10(7):301\u2013308, 2006. doi:10.1016/j.tics.2006.05.002 . \u21a9 \u21a9 Alan Yuille and Daniel Kersten. Vision as bayesian inference: analysis by synthesis? Trends in cognitive sciences , 10(7):301\u2013308, 2006. doi:10.1016/j.tics.2006.05.002 . \u21a9 \u21a9","title":"Research"},{"location":"research/#research","text":"The Computational Vision Lab combines computational theory with behavioral and brain image experiments to understand how we see the world around us. Vision is used for recognition, planning and motor actions. Exactly how to get from the retinal image intensities to useful actions is a tough problem requiring multiple approaches. A major theoretical challenge is to discover the computational principles required to infer world properties and determine motor output from images. Computational vision searches for these solutions. The empirical challenge is to discover how our visual systems and those of other animals are built to achieve useful actions from the images received.","title":"Research"},{"location":"research/#approach","text":"Our research uses computational, psychophysical, and brain imaging approaches to investigate how the visual pathways of the brain transform image information into useful plans and action.","title":"Approach"},{"location":"research/#methodologies","text":"psychophysics brain imaging (fMRI) neurophysiology computational modeling machine learning and computer vision theoretical neuroscience","title":"Methodologies"},{"location":"research/#topics-of-interest","text":"perceptual inference perceptual organization object perception perceptual learning vision and action cue integration cortical processing top-down (feedback) and bottom-up (feed-forward) interaction inhibition suppression adaptation","title":"Topics of interest"},{"location":"research/#phenomena-of-interest","text":"natural scenes context effects illusion aftereffects shape, size, surface reflectance color motion occlusion, crowding, clutter shadow camouflage","title":"Phenomena of interest"},{"location":"research/#theory-and-computation","text":"statistical (Bayesian) inference 1 2 ideal observer models analysis by synthesis predictive coding neural networks hierarchical generative models","title":"Theory and computation"},{"location":"research/#innovation","text":"","title":"Innovation"},{"location":"research/#vision-as-statistical-inference-bayesian-approaches","text":"Bayesian approaches have enjoyed a great deal of recent success in their application to problems in computer vision. This success has led to an emerging interest in applying Bayesian methods to modeling human visual perception. We consider the implications of a Bayesian view of visual information processing for experimentally investigating human visual perception. We have outlined the elements of a general program of empirical research which results from taking the basic Bayesian formulation seriously not only as a means for objectively modeling image information through ideal observer analysis (e.g. see our work in object recognition), but also as a framework for characterizing human perceptual inference. A major advantage of following such a program is that, because its structure is the same as that of the Bayesian framework for computational modeling, it supports a strong integration of psychophysics and computational theory. In particular, it provides the foundation for a psychophysics of constraints in which one tests hypotheses regarding quantitative and qualitative constraints used in human perceptual inferences. The Bayesian approach also suggests new ways to conceptualize the general problem of perception and to decompose it into isolable parts for psychophysical investigation; that is, it not only provides a framework for modeling solutions to specific perceptual problems; it also guides the definition of the problems. Analysis by synthesis Analysis by synthesis. (a) Low-level processing can extract edge features, such as bars, and use conjunctions of these features to make bottom-up proposals to access the higher-level models of objects. (b) The high-level objects access the image top-down to validate or reject the bottom-up proposals. In this example, the low-level cues propose that the image can be interpreted as an E, an F, or a set of parallel bars. But interpreting it as an F explains almost all the features in the image and is preferred. 1 2 Alan Yuille and Daniel Kersten. Vision as bayesian inference: analysis by synthesis? Trends in cognitive sciences , 10(7):301\u2013308, 2006. doi:10.1016/j.tics.2006.05.002 . \u21a9 \u21a9 Alan Yuille and Daniel Kersten. Vision as bayesian inference: analysis by synthesis? Trends in cognitive sciences , 10(7):301\u2013308, 2006. doi:10.1016/j.tics.2006.05.002 . \u21a9 \u21a9","title":"Vision as statistical inference Bayesian approaches"},{"location":"courses/PSY5036F2019/","text":"Computational Vision PSY5036F2019 # University of Minnesota, Fall Semester, 2019 Instructor: Dan Kersten: kersten@umn.edu Abstract The visual perception of what is in the world is accomplished continually, instantaneously, and usually without conscious thought. The very effortlessness of perception disguises the underlying richness of the problem. We can gain insight into the processes and functions of human vision by studying the relationship between neural mechanisms and visual behavior through computer analysis and simulation. Students will learn about the anatomy and neurophysiology of vision and how they relate to the phenomona of perception. An underlying theme will be to treat vision as a process of statistical inference. There will be in-class programming exercises using the language Mathematica. No prior programming experience is required; however, some familiarity with probability, vector calculus and linear algebra is helpful. Readings # Main # Lecture notes, Main Readings & Supplementary Material are all available online. Additional readings # Math and vision # ( EV ) Early Vision. Yuille and Kersten. In From Neuron to Cognition via Computational Neuroscience , M.A. Arbib, James J. Bonaiuto Editors, Cambridge MA: The MIT Press, in 2016 ( preprint pdf ) Understanding Vision: Theory, Models, and Data. Li Zhaoping. 2014.( publisher page ) ( author's web outline ) Functional human vision # ( FV ) Foundations of Vision . Wandell ( web ) Neurophysiology # ( NVN ) The New Visual Neurosciences . John S. Werner and Leo M. Chalupa, edts. 2014. Software # Mathematica # Mathematica is the primary programming environment for this course. Students who have registered for the course will have access through the Psychology Department's site license. Alternatives: Mathematica is available in several labs on campus, go to http://www.oit.umn.edu/computer-labs/software/index.htm You may wish to purchase Mathematica for Students see http://www.wolfram.com/products/student/mathforstudents/index.html . For help using Mathematica, see: http://mathematica.stackexchange.com Python/IPython # http://ipython.org http://jupyter-notebook-beginner-guide.readthedocs.org/en/latest/index.html http://www.scipy.org For an online course in using Python and PsychoPy for research in human vision see: http://nbviewer.ipython.org/github/gestaltrevision/python_for_visres/blob/master/index.ipynb Writing # Gopen, G. D., & Swan, J. A., 1990. The Science of Scientific Writing. <u> American Scientist </u> , <u> 78 </u> , 550-558. Supplementary: The Sense of Style: The Thinking Person's Guide to Writing in the 21st Century (2014), Pinker, Steven. ( amazon link ) Penrose, A. M., & Katz, S. B. (1998). <u> Writing in the Sciences: Exploring Conventions of Scientific Discourse </u> . New York: St. Martin's Press, Inc. American Psychological Association. (2009). <u> Publication manual of the American Psychological Association </u> (6th ed.). Washington, DC: American Psychological Association Writing assistance. THE CENTER FOR WRITING offers free one-to-one writing assistance to undergraduate and graduate students, with appointments up to 45 minutes. Nonnative speaker specialists are available. For more information, see http://writing.umn.edu . Psychology department resources: http://writing.psych.umn.edu/student-resources Grade Requirements # There will be programming assignments and a final project. The grade weights are: Exercise/programming assignments: 55% Final project in-class presentations: 5 % Final project : 40% (five parts: 2% (title and outline) +5%(first draft) +5% (peer commentary) +8% (cover letter response) + 20% (final draft)) The programming assignments will use the Mathematica programming environment. No prior experience with Mathematica is necessary. Assignment due By the 6 am on the day after the nominal due date. Late Policy: Assignments turned in within 24 hours following the due date will have 15% deducted from the assignment score. Assignments turned in between 24 and 48 hours following the due date will have 30% deducted from the score. Assignments more than 48 hours late will receive a score of zero. Lectures # Check this section before each class for recent additions and revisions. Info Lecture notes are in Mathematica Notebook and pdf format. You can download the Mathematica notebook files below to view with Mathematica or Wolfram CDF Player (whichis free). Final Project Assignment # Goal: This course integrates the behavioral, neural and computational principles of perception. Students often find the interdisciplinary integration to be the most challenging aspect of the course. Through writing, you will learn to synthesize results from diverse and typically isolated disciplines. By writing about your project work, you will learn to think through the broader implications of your project, and to effectively communicate the rationale and results of your contribution in words. You will do a final page research report in which you will describe, in the form of a scientific paper, the results of an original computer program on a topic in computational vision. Your final project will involve: 1) a computer program and; 2) a 2000-3000 word final paper describing your project. For your computer project, you will do one of the following: 1) Write a program to simulate a model from the computer vision literature ; 2) Design and program a method for solving some problem in perception. 3) Design and program a psychophysical experiment to study an aspect of human visual perception. The results of your final project should be written up in the form of a short scientific paper or Mathematica Notebook, describing the motivation, methods, results, and interpretation. If you choose to write your program in Mathematica, your paper and program can be combined can be formated as a Mathematica notebook. See: Books and Tutorials on Notebooks. If you do your final project using Python, you can turn your paper in as a Jupyter notebook. Your paper will be critiqued and returned for you to revise and resubmit in final form. You should write for an audience consisting of your class peers. Completing the final paper involves 4 steps. Each step requires that you email a document to the teaching assistant. Outline (2% of grade). You will submit a working title and paragraph outline by the deadline noted in the syllabus. These outlines will be critiqued in order to help you find an appropriate focus for your papers. (Consult with the instructor or TA for ideas well ahead of time). Complete draft (5% of grade). A double-spaced, complete draft of the paper must be turned in by the deadline noted in the syllabus. Papers should be between 2000 and 3000 words. In addition to the title, author and date lines, papers must include the following sections: Abstract, Introduction, Methods, Results, Discussion, and Bibliography. Use citations to motivate your problem and to justify your claims. Cite authors by name and date, e.g. (Marr & Poggio, 1979). Citations should be original sources, not wikipedia. Use a standard citation format, such as APA. (The UM library has information on style guides , and in particular APA style .) Papers must be typed, with a page number on each page. Figures should be numbered and have figure captions. This draft will be reviewed by your instructor and one of you class peers. The point break down for the total 5% is: 2 pts for completing Introduction, 2 pts for completing Methods, 1 pt for completing Discussion) Peer commentary (5% of grade) . You will submit a written commentary (200 to 500 words) on a complete draft of one of your class peers. The project drafts and commentaries will be anonymous. The commentary should provide feedback to improve the quality and clarity of the writing. Final draft (20% of grade) and \"Cover letter\" (8% of grade). The final draft must be turned in by the date noted on the syllabus. The \"Cover letter\" should describe how your revision addressed comments from your peer evaluator and from your instructor. It should itemize key criticisms together with a brief description of the changes you made to your draft manuscript.","title":"Computational Vision <small>PSY5036F2019</small>"},{"location":"courses/PSY5036F2019/#computational-vision-psy5036f2019","text":"University of Minnesota, Fall Semester, 2019 Instructor: Dan Kersten: kersten@umn.edu Abstract The visual perception of what is in the world is accomplished continually, instantaneously, and usually without conscious thought. The very effortlessness of perception disguises the underlying richness of the problem. We can gain insight into the processes and functions of human vision by studying the relationship between neural mechanisms and visual behavior through computer analysis and simulation. Students will learn about the anatomy and neurophysiology of vision and how they relate to the phenomona of perception. An underlying theme will be to treat vision as a process of statistical inference. There will be in-class programming exercises using the language Mathematica. No prior programming experience is required; however, some familiarity with probability, vector calculus and linear algebra is helpful.","title":"Computational Vision PSY5036F2019"},{"location":"courses/PSY5036F2019/#readings","text":"","title":"Readings"},{"location":"courses/PSY5036F2019/#main","text":"Lecture notes, Main Readings & Supplementary Material are all available online.","title":"Main"},{"location":"courses/PSY5036F2019/#additional-readings","text":"","title":"Additional readings"},{"location":"courses/PSY5036F2019/#math-and-vision","text":"( EV ) Early Vision. Yuille and Kersten. In From Neuron to Cognition via Computational Neuroscience , M.A. Arbib, James J. Bonaiuto Editors, Cambridge MA: The MIT Press, in 2016 ( preprint pdf ) Understanding Vision: Theory, Models, and Data. Li Zhaoping. 2014.( publisher page ) ( author's web outline )","title":"Math and vision"},{"location":"courses/PSY5036F2019/#functional-human-vision","text":"( FV ) Foundations of Vision . Wandell ( web )","title":"Functional human vision"},{"location":"courses/PSY5036F2019/#neurophysiology","text":"( NVN ) The New Visual Neurosciences . John S. Werner and Leo M. Chalupa, edts. 2014.","title":"Neurophysiology"},{"location":"courses/PSY5036F2019/#software","text":"","title":"Software"},{"location":"courses/PSY5036F2019/#mathematica","text":"Mathematica is the primary programming environment for this course. Students who have registered for the course will have access through the Psychology Department's site license. Alternatives: Mathematica is available in several labs on campus, go to http://www.oit.umn.edu/computer-labs/software/index.htm You may wish to purchase Mathematica for Students see http://www.wolfram.com/products/student/mathforstudents/index.html . For help using Mathematica, see: http://mathematica.stackexchange.com","title":"Mathematica"},{"location":"courses/PSY5036F2019/#pythonipython","text":"http://ipython.org http://jupyter-notebook-beginner-guide.readthedocs.org/en/latest/index.html http://www.scipy.org For an online course in using Python and PsychoPy for research in human vision see: http://nbviewer.ipython.org/github/gestaltrevision/python_for_visres/blob/master/index.ipynb","title":"Python/IPython"},{"location":"courses/PSY5036F2019/#writing","text":"Gopen, G. D., & Swan, J. A., 1990. The Science of Scientific Writing. <u> American Scientist </u> , <u> 78 </u> , 550-558. Supplementary: The Sense of Style: The Thinking Person's Guide to Writing in the 21st Century (2014), Pinker, Steven. ( amazon link ) Penrose, A. M., & Katz, S. B. (1998). <u> Writing in the Sciences: Exploring Conventions of Scientific Discourse </u> . New York: St. Martin's Press, Inc. American Psychological Association. (2009). <u> Publication manual of the American Psychological Association </u> (6th ed.). Washington, DC: American Psychological Association Writing assistance. THE CENTER FOR WRITING offers free one-to-one writing assistance to undergraduate and graduate students, with appointments up to 45 minutes. Nonnative speaker specialists are available. For more information, see http://writing.umn.edu . Psychology department resources: http://writing.psych.umn.edu/student-resources","title":"Writing"},{"location":"courses/PSY5036F2019/#grade-requirements","text":"There will be programming assignments and a final project. The grade weights are: Exercise/programming assignments: 55% Final project in-class presentations: 5 % Final project : 40% (five parts: 2% (title and outline) +5%(first draft) +5% (peer commentary) +8% (cover letter response) + 20% (final draft)) The programming assignments will use the Mathematica programming environment. No prior experience with Mathematica is necessary. Assignment due By the 6 am on the day after the nominal due date. Late Policy: Assignments turned in within 24 hours following the due date will have 15% deducted from the assignment score. Assignments turned in between 24 and 48 hours following the due date will have 30% deducted from the score. Assignments more than 48 hours late will receive a score of zero.","title":"Grade Requirements"},{"location":"courses/PSY5036F2019/#lectures","text":"Check this section before each class for recent additions and revisions. Info Lecture notes are in Mathematica Notebook and pdf format. You can download the Mathematica notebook files below to view with Mathematica or Wolfram CDF Player (whichis free).","title":"Lectures"},{"location":"courses/PSY5036F2019/#final-project-assignment","text":"Goal: This course integrates the behavioral, neural and computational principles of perception. Students often find the interdisciplinary integration to be the most challenging aspect of the course. Through writing, you will learn to synthesize results from diverse and typically isolated disciplines. By writing about your project work, you will learn to think through the broader implications of your project, and to effectively communicate the rationale and results of your contribution in words. You will do a final page research report in which you will describe, in the form of a scientific paper, the results of an original computer program on a topic in computational vision. Your final project will involve: 1) a computer program and; 2) a 2000-3000 word final paper describing your project. For your computer project, you will do one of the following: 1) Write a program to simulate a model from the computer vision literature ; 2) Design and program a method for solving some problem in perception. 3) Design and program a psychophysical experiment to study an aspect of human visual perception. The results of your final project should be written up in the form of a short scientific paper or Mathematica Notebook, describing the motivation, methods, results, and interpretation. If you choose to write your program in Mathematica, your paper and program can be combined can be formated as a Mathematica notebook. See: Books and Tutorials on Notebooks. If you do your final project using Python, you can turn your paper in as a Jupyter notebook. Your paper will be critiqued and returned for you to revise and resubmit in final form. You should write for an audience consisting of your class peers. Completing the final paper involves 4 steps. Each step requires that you email a document to the teaching assistant. Outline (2% of grade). You will submit a working title and paragraph outline by the deadline noted in the syllabus. These outlines will be critiqued in order to help you find an appropriate focus for your papers. (Consult with the instructor or TA for ideas well ahead of time). Complete draft (5% of grade). A double-spaced, complete draft of the paper must be turned in by the deadline noted in the syllabus. Papers should be between 2000 and 3000 words. In addition to the title, author and date lines, papers must include the following sections: Abstract, Introduction, Methods, Results, Discussion, and Bibliography. Use citations to motivate your problem and to justify your claims. Cite authors by name and date, e.g. (Marr & Poggio, 1979). Citations should be original sources, not wikipedia. Use a standard citation format, such as APA. (The UM library has information on style guides , and in particular APA style .) Papers must be typed, with a page number on each page. Figures should be numbered and have figure captions. This draft will be reviewed by your instructor and one of you class peers. The point break down for the total 5% is: 2 pts for completing Introduction, 2 pts for completing Methods, 1 pt for completing Discussion) Peer commentary (5% of grade) . You will submit a written commentary (200 to 500 words) on a complete draft of one of your class peers. The project drafts and commentaries will be anonymous. The commentary should provide feedback to improve the quality and clarity of the writing. Final draft (20% of grade) and \"Cover letter\" (8% of grade). The final draft must be turned in by the date noted on the syllabus. The \"Cover letter\" should describe how your revision addressed comments from your peer evaluator and from your instructor. It should itemize key criticisms together with a brief description of the changes you made to your draft manuscript.","title":"Final Project Assignment"},{"location":"courses/PSY5038F2018/","text":"Introduction to Neural Networks PSY5038WF2018 # University of Minnesota, Fall Semester, 2018 Instructor: Dan Kersten: kersten@umn.edu Abstract Introduction to large scale parallel distributed processing models in neural and cognitive science. Topics include: linear models, statistical pattern theory, Hebbian rules, self-organization, non-linear models, information optimization, and representation of neural information. Applications to sensory processing, perception, learning, and memory. Prerequisites linear algebra multivariate calculus Software # Mathematica # Mathematica is the primary programming environment for this course. Students who have registered for the course will have Google Docs access through the Psychology Department's site license . Alternatives: Mathematica is available in several labs on campus, go to http://www.oit.umn.edu/computer-labs/software/index.htm You may wish to purchase Mathematica for Students see http://www.wolfram.com/products/student/mathforstudents/index.html . You can also access Mathematica on the CLA servers: If you never programmed before go here . If you have programming experience, go here . For user help on using Mathematica, see: http://mathematica.stackexchange.com Learning center: http://www.wolfram.com/learningcenter/ Writing # Gopen, G. D., & Swan, J. A., 1990. The Science of Scientific Writing. American Scientist, 78, 550-558. ( pdf ) Supplementary: The Sense of Style: The Thinking Person's Guide to Writing in the 21st Century (2014), Pinker, Steven. ( amazon link ) Penrose, A. M., & Katz, S. B. (1998). Writing in the Sciences: Exploring Conventions of Scientific Discourse. New York: St. Martin's Press, Inc. American Psychological Association. (2009). Publication manual of the American Psychological Association (6th ed.). Washington, DC: American Psychological Association Writing assistance. THE CENTER FOR WRITING offers free one-to-one writing assistance to undergraduate and graduate students, with appointments up to 45 minutes. Nonnative speaker specialists are available. For more information, see http://writing.umn.edu . Psychology department resources: http://writing.psych.umn.edu/student-resources Grade Requirements # There will be programming assignments, as well as a final project . The grade weights are: Exercise/programming assignments: 55% Final project presentations: 5 % Final project : 40% (four parts: 2%+5%+5%+28%) The programming assignments will use the Mathematica programming environment. No prior experience with Mathematica is necessary. [ ](http://www2.publabs.umn.edu/publab/text/locations.html)Assignment due BEFORE class start time (9:45 am) on the day due. You can use the downloaded Mathematica notebook for the assignment as your template, add your answers, and email your finished assignment to the TA. You can copy and paste any code bits you need from the Lecture notebooks. But of course, you cannot copy and paste code or any other answer materials from someone else. Final Project Assignment # This course teaches you how to understand cognitive and perceptual aspects of brain processing in terms of computation. Writing a computer program encourages you to think clearly about the assumptions underlying a given theory. Getting a program to work, however, tests just one level of clear thinking. By writing about your work, you will learn to think through the broader implications of your final project, and to effectively communicate the rationale and results in words. Your final project will involve: 1) a computer simulation and; 2) a 2000-3000 word final paper describing your simulation. For your computer project, you will do one of the following: 1) Devise a novel application for a neural network model studied in the course; 2) Write a program to simulate a model from the neural network literature ; 3) Design and program a method for solving some problem in perception, cognition or motor control. The results of your final project should be written up in the form of a short scientific paper, describing the motivation, methods, results, and interpretation. Your paper will be critiqued and returned for you to revise and resubmit in final form. You should write for an audience consisting of your class peers. You may elect to have your final paper published in the course's web-based electronic journal. Completing the final paper involves 3 steps: Outline . You will submit a working title and paragraph outline by the deadline noted in the syllabus. These outlines will be critiqued in order to help you find an appropriate focus for your papers. ( 2% of grade). (Consult with the instructor or TA for ideas well ahead of time). Complete draft . You will then submit a complete draft of your paper ( 2000-3000 words ). Papers must include the following sections: Abstract, Introduction, Methods, Results, Discussion, and Bibliography. Use citations to motivate your problem and to justify your claims. Figures should be numbered and have figure captions. Cite authors by name and date, e.g. (Marr & Poggio, 1979). Use a standard citation format, such as APA. Papers must be typed, with a page number on each page.Each paper will be reviewed with specific recommendations for improvement. ( 5% of grade) Peer commentary . Each student will submit a paragraph on an anonymous paired project draft ( 5% of grade) Final draft . You will submit a final revision for grading. ( 28% of grade). The final draft must be turned in by the date noted on the syllabus. Students who wish to submit their final papers to be published in the class electronic journal should turn in both paper and electronic copies of their reports. If you choose to write your program in Mathematica, your paper and program can be combined can be formated as a Mathematica notebook. See: Books and Tutorials on Notebooks. Your paper will be critiqued and returned for you to revise and resubmit in final form. You should write for an audience consisting of your class peers.","title":"PSY5038F2018"},{"location":"courses/PSY5038F2018/#introduction-to-neural-networks-psy5038wf2018","text":"University of Minnesota, Fall Semester, 2018 Instructor: Dan Kersten: kersten@umn.edu Abstract Introduction to large scale parallel distributed processing models in neural and cognitive science. Topics include: linear models, statistical pattern theory, Hebbian rules, self-organization, non-linear models, information optimization, and representation of neural information. Applications to sensory processing, perception, learning, and memory. Prerequisites linear algebra multivariate calculus","title":"Introduction to Neural Networks PSY5038WF2018"},{"location":"courses/PSY5038F2018/#software","text":"","title":"Software"},{"location":"courses/PSY5038F2018/#mathematica","text":"Mathematica is the primary programming environment for this course. Students who have registered for the course will have Google Docs access through the Psychology Department's site license . Alternatives: Mathematica is available in several labs on campus, go to http://www.oit.umn.edu/computer-labs/software/index.htm You may wish to purchase Mathematica for Students see http://www.wolfram.com/products/student/mathforstudents/index.html . You can also access Mathematica on the CLA servers: If you never programmed before go here . If you have programming experience, go here . For user help on using Mathematica, see: http://mathematica.stackexchange.com Learning center: http://www.wolfram.com/learningcenter/","title":"Mathematica"},{"location":"courses/PSY5038F2018/#writing","text":"Gopen, G. D., & Swan, J. A., 1990. The Science of Scientific Writing. American Scientist, 78, 550-558. ( pdf ) Supplementary: The Sense of Style: The Thinking Person's Guide to Writing in the 21st Century (2014), Pinker, Steven. ( amazon link ) Penrose, A. M., & Katz, S. B. (1998). Writing in the Sciences: Exploring Conventions of Scientific Discourse. New York: St. Martin's Press, Inc. American Psychological Association. (2009). Publication manual of the American Psychological Association (6th ed.). Washington, DC: American Psychological Association Writing assistance. THE CENTER FOR WRITING offers free one-to-one writing assistance to undergraduate and graduate students, with appointments up to 45 minutes. Nonnative speaker specialists are available. For more information, see http://writing.umn.edu . Psychology department resources: http://writing.psych.umn.edu/student-resources","title":"Writing"},{"location":"courses/PSY5038F2018/#grade-requirements","text":"There will be programming assignments, as well as a final project . The grade weights are: Exercise/programming assignments: 55% Final project presentations: 5 % Final project : 40% (four parts: 2%+5%+5%+28%) The programming assignments will use the Mathematica programming environment. No prior experience with Mathematica is necessary. [ ](http://www2.publabs.umn.edu/publab/text/locations.html)Assignment due BEFORE class start time (9:45 am) on the day due. You can use the downloaded Mathematica notebook for the assignment as your template, add your answers, and email your finished assignment to the TA. You can copy and paste any code bits you need from the Lecture notebooks. But of course, you cannot copy and paste code or any other answer materials from someone else.","title":"Grade Requirements"},{"location":"courses/PSY5038F2018/#final-project-assignment","text":"This course teaches you how to understand cognitive and perceptual aspects of brain processing in terms of computation. Writing a computer program encourages you to think clearly about the assumptions underlying a given theory. Getting a program to work, however, tests just one level of clear thinking. By writing about your work, you will learn to think through the broader implications of your final project, and to effectively communicate the rationale and results in words. Your final project will involve: 1) a computer simulation and; 2) a 2000-3000 word final paper describing your simulation. For your computer project, you will do one of the following: 1) Devise a novel application for a neural network model studied in the course; 2) Write a program to simulate a model from the neural network literature ; 3) Design and program a method for solving some problem in perception, cognition or motor control. The results of your final project should be written up in the form of a short scientific paper, describing the motivation, methods, results, and interpretation. Your paper will be critiqued and returned for you to revise and resubmit in final form. You should write for an audience consisting of your class peers. You may elect to have your final paper published in the course's web-based electronic journal. Completing the final paper involves 3 steps: Outline . You will submit a working title and paragraph outline by the deadline noted in the syllabus. These outlines will be critiqued in order to help you find an appropriate focus for your papers. ( 2% of grade). (Consult with the instructor or TA for ideas well ahead of time). Complete draft . You will then submit a complete draft of your paper ( 2000-3000 words ). Papers must include the following sections: Abstract, Introduction, Methods, Results, Discussion, and Bibliography. Use citations to motivate your problem and to justify your claims. Figures should be numbered and have figure captions. Cite authors by name and date, e.g. (Marr & Poggio, 1979). Use a standard citation format, such as APA. Papers must be typed, with a page number on each page.Each paper will be reviewed with specific recommendations for improvement. ( 5% of grade) Peer commentary . Each student will submit a paragraph on an anonymous paired project draft ( 5% of grade) Final draft . You will submit a final revision for grading. ( 28% of grade). The final draft must be turned in by the date noted on the syllabus. Students who wish to submit their final papers to be published in the class electronic journal should turn in both paper and electronic copies of their reports. If you choose to write your program in Mathematica, your paper and program can be combined can be formated as a Mathematica notebook. See: Books and Tutorials on Notebooks. Your paper will be critiqued and returned for you to revise and resubmit in final form. You should write for an audience consisting of your class peers.","title":"Final Project Assignment"},{"location":"courses/PSY8036SP2018/","text":"Data-driven generative models for perception, dreaming, and imagining PSY8036SP2018 # University of Minnesota, Spring Semester, 2018 Topics in Computational Vision Psy 8036 (Kersten) Psy 5993 Section 034 (Schrater) Instructors: Dan Kersten kersten@umn.edu Paul Schrater schrater@umn.edu Abstract It has been proposed that perception is fundamentally a process of \u201canalysis-by-synthesis\u201d in which the sensory input is analyzed bottom-up, with perceptual interpretations tested and refined by top-down predictions of the input, through synthesis. However, while the computational and neural study of the analysis component is well-developed, less is known about the principles and mechanisms that underly synthesis. This seminar will explore recent advances using \u201cdeep\u201d learning algorithms to discover hierarchical statistical regularities in large datasets of natural patterns, and the relevance of the learning results to models of human perception and recognition. These algorithms also provide the basis for the stochastic synthesis of novel, yet familiar patterns, which raises the question of whether the human experiences of dreams and hallucinations, and the ability to imagine, reflect the same statistical regularities that are discoverable using machine learning. The class format will include short introductory lectures by the instructors, and weekly student presentations of current literature. The short lectures will provide historical context as well as tutorials on machine learning (e.g. TensorFlow for neural network simulations). Background # There is a long history of theories of perception in which the brain \u201cexplains\u201d sensory input in terms of external, behaviorally relevant causes. A current hypothesis is that this process is implemented in part by cortical feedback mechanisms that synthesize predictions of early data representations in order to test how well the brain's current interpretation of the world corresponds with the sensory data. In this view, perception involves a cycle in which the incoming data triggers a set of explanations, i.e. hypotheses, which are used to measure how far the expected sensory input differs from the actual input. From a computational perspective, such generative models of perceptual inference have a number of advantages over strictly bottom-up inference. A generative model can incorporate measures of \"goodness-of-fit\" to decide whether to accept or reject an interpretation--some explanations are better than others. Discrepancies between sensory data and predictions may also be used to direct attentional resources and signal whether more complex combinations of hypotheses are needed. Further, with sufficient structure, a generative model could provide the basis for the perceptual interpretation of sensory input outside the range of past experience. While computational theories for bottom-up neural mechanisms for perception have received considerable scientific attention, much less is known about top-down mechanisms. This seminar will explore the idea that the brain has hierarchically structured mechanisms that can synthesize patterns of input representations with the following constraints: 1) the mechanisms build on inductive structural biases that are innate; 2) the mechanisms reflect the statistical regularities induced by the physical causes of sensory experience, i.e. they are \"data-driven\"; 3) the need for cognitive processes to access semantic, perceptual content over levels of abstraction. Assumptions 1) and 2) constrain the class of generative models to be \"data-driven\", i.e. models that can be learned from sensory data. Recent computational methods for data-driven pattern synthesis (e.g. VAE, InfoGAN, Adversarial Bayes, StackGAN) will be covered in this seminar. We will also explore the proposal that the same circuitry that may underly feedback in perception is used during imagery, dreams, and hallucinations. Tentative Syllabus # Week Topics Background material Discussion topics and papers 1: Jan 16 Background Models of perception Yuille, A., & Kersten, D. (2006). Vision as Bayesian inference: analysis by synthesis? Trends in Cognitive Sciences, 10(7), 301\u2013308. 2: Jan 23 Overview of machine learning Ackley, D. H., Hinton, G. E., & Sejnowski, T. J. (1985). A learning algorithm for Boltzmann machines. Cognitive Science, 9(1), 147\u2013169. 3: Jan 30 Shallow image models, textures Zhu, S. C., Wu, Y., & Mumford, D. (1998). Filters, random fields and maximum entropy (FRAME): Towards a unified theory for texture modeling. International Journal of Computer Vision, 27(2), 107\u2013126. McDermott, J. H., Schemitsch, M., & Simoncelli, E. P. (2013). Summary statistics in auditory perception. Nature Publishing Group, 16(4), 493\u2013498. 4: Feb 6 Hierarchical image models, deep learning Zhu, S.-C., & Mumford, D. (2006). Quest for a stochastic grammar of images. Foundations and Trends\u00ae in Computer Graphics and Vision, 2(4), 259\u2013362. Topic preview: Visual imagery 5: Feb 13 Hierarchical image models, deep learning Topic preview: Auditory imagery 6: Feb 20 Hierarchical image models, deep learning Topic preview: Hypnagogic imagery 7: Feb 27 Dynamic textures, patterns Xie, J., & Zhu, S. C. (n.d.). Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet. arXiv.org. Vondrick, C., Pirsiavash, H., & Torralba, A. (2016). Generating Videos with Scene Dynamics. Advances in Neural Information Processing Systems NIPS, 613\u2013621. Topic preview: Dreams 8: Mar 6 Visual imagery Christophel, T. B., Klink, P. C., Spitzer, B., Roelfsema, P. R., & Haynes, J.-D. (2017). The Distributed Nature of Working Memory. Trends in Cognitive Sciences, 1\u201315. Dijkstra, N., Zeidman, P., Ondobaka, S., Gerven, M. A. J., & Friston, K. (2017). Distinct Top-down and Bottom-up Brain Connectivity During Visual Perception and Imagery. Scientific Reports, 1\u20139. Topic preview: Lucid dreaming Mar 13 Spring Break 9: Mar 20 Auditory, musical imagery Zatorre, R. J., & Halpern, A. R. (2005). Mental Concerts: Musical Imagery and Auditory Cortex. Neuron, 47(1), 9\u201312. Riecke, L., A. J. van Opstal, R. Goebel, and E. Formisano. \u201cHearing Illusory Sounds in Noise: Sensory-Perceptual Transformations in Primary Auditory Cortex.\u201d Journal of Neuroscience 27, no. 46 (November 14, 2007): 12684\u201389. McDermott, Josh H., and Andrew J. Oxenham. \u201cSpectral Completion of Partially Masked Sounds.\u201d Proceedings of the National Academy of Sciences 105, no. 15 (2008): 5939\u20135944. Topic overview: Hallucinations & psychedelics 10: Mar 27 Hypnagogic imagery Schacter, D. L. (1976). The hypnagogic state: a critical review of the literature. Psychological Bulletin. Topic preview: Hallucinations & schizophrenia 11: Apr 3 Dreams Stickgold, R., Hobson, J. A., Fosse, R., & Fosse, M. (2001). Sleep, Learning, and Dreams: Off-line Memory Reprocessing. Science, 294(5544), 1052\u20131057. Crick, F., G. Mitchison., 1983. The function of dream sleep. Nature. Springer Topic preview: Imagination 12: Apr 10 Lucid dreaming Voss, U., Holzmann, R., Tuin, I., , J. A. Hobson., 2009. Lucid dreaming: a state of consciousness with features of both waking and non-lucid dreaming. Sleep. 13: Apr 17 Hallucinations Seri\u00e8s, P., Reichert, D. P., & Storkey, A. J. (2010). Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model, 2020\u20132028. Ermentrout, G. B., & Cowan, J. D. (1979). A mathematical theory of visual hallucination patterns. Biological Cybernetics, 34(3), 137\u2013150 Howard, R. J., Brammer, M. J., David, A., Woodruff, P., & Williams, S. (1998). The anatomy of conscious vision: an fMRI study of visual hallucinations. Nature neuroscience, 1(8), 738-742. 14: Apr 24 Hallucinations Kumar, S., Sedley, W., Barnes, G. R., Teki, S., Friston, K. J., & Griffiths, T. D. (2014). A brain basis for musical hallucinations. Cortex, 52(C), 86\u201397 15: May 1 Imagination, art and design Friston, K. J., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., & Ondobaka, S. (2017). Active Inference, Curiosity and Insight. Neural Computation, 29(10), 2633\u20132683. 16: May 8 Finals week FINAL PROJECT PRESENTATIONS Sample Readings (under construction) # Background # Ackley, D. H., Hinton, G. E., & Sejnowski, T. J. (1985). A learning algorithm for Boltzmann machines. Cognitive Science, 9(1), 147\u2013169. Bastos, A. M., Usrey, W. M., Adams, R. A., Mangun, G. R., Fries, P., & Friston, K. J. (2012). Canonical Microcircuits for Predictive Coding. Neuron, 76(4), 695\u2013711. Berkes, P., Orban, G., Lengyel, M., & Fiser, J. (2011). Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment. Science, 331(6013), 83\u201387. Dayan, P., Hinton, G. E., Neal, R. M., & Zemel, R. S. (1995). The Helmholtz Machine. Neural Computation, 7(5), 889\u2013904. Ouden, den, H. E. M. (2012). How prediction errors shape perception, attention, and motivation, 1\u201312. Orban, G., Pietro Berkes, Fiser, J., & Lengyel, M. (2016). Neural Variability and Sampling-Based Probabilistic Representations in the Visual Cortex. Neuron, 92(2), 530\u2013543. MacKay, D. M. (1956). Towards an information-flow model of human behaviour. British Journal of Psychology (London, England : 1953), 47(1), 30\u201343. Lake, B. M., Salakhutdinov, R., & Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266), 1332\u20131338. http://doi.org/10.1126/science.aab3050 LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436\u2013444. http://doi.org/10.1038/nature14539 McDermott, Josh H., and Andrew J. Oxenham. \u201cSpectral Completion of Partially Masked Sounds.\u201d Proceedings of the National Academy of Sciences 105, no. 15 (2008): 5939\u20135944.Mumford, D. (1992). On the computational architecture of the neocortex. Biological Cybernetics, 66(3), 241\u2013251. Mumford, D. (1994). Pattern theory: a unifying perspective, 187\u2013224. Rao, R. P. N., & Ballard, D. H. (1999). Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nature Neuroscience, 2, 79\u201387. Tu, Z., Chen, X., Yuille, A. L., & Zhu, S.-C. (2005). Image parsing: Unifying segmentation, detection, and recognition. International Journal of Computer Vision, 63(2), 113\u2013140. Yuille, A., & Kersten, D. (2006). Vision as Bayesian inference: analysis by synthesis? Trends in Cognitive Sciences, 10(7), 301\u2013308. Richards, W. (1971). The Fortification Illusions of Migraines, Scientific American, 1\u201310. Zhu, S.-C., & Mumford, D. (2006). Quest for a stochastic grammar of images. Foundations and Trends\u00ae in Computer Graphics and Vision, 2(4), 259\u2013362. http://doi.org/10.1561/0600000018 Shallow generative models: Texture synthesis # Freeman, J., & Simoncelli, E. P. (2011). Metamers of the ventral stream. Nature Publishing Group, 14(9), 1195\u20131201. http://doi.org/10.1038/nn.2889 McDermott, J. H., Schemitsch, M., & Simoncelli, E. P. (2013). Summary statistics in auditory perception. Nature Publishing Group, 16(4), 493\u2013498. McDermott, J. H., & Simoncelli, E. P. (2011). Sound Texture Perception via Statistics of the Auditory Periphery: Evidence from Sound Synthesis. Neuron, 71(5), 926\u2013940. Zhu, S. C., Wu, Y., & Mumford, D. (1998). Filters, random fields and maximum entropy (FRAME): Towards a unified theory for texture modeling. International Journal of Computer Vision, 27(2), 107\u2013126. Hierarchical (deep) data-driven generative models # Chen, X., Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., & Abbeel, P. (2016). InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets, 2172\u20132180. Goodfellow, I. (2016, December 31). NIPS 2016 Tutorial: Generative Adversarial Networks. Kulkarni, T. D., Whitney, W. F., Kohli, P., & Tenenbaum, J. (2015). Deep Convolutional Inverse Graphics Network, 2539\u20132547. Rock, J., Issaranon, T., Deshpande, A., & Forsyth, D. (2016, December 5). Authoring image decompositions with generative models. Varol, G., Romero, J., Martin, X., Mahmood, N., Black, M. J., Laptev, I., & Schmid, C. (2017, January 5). Learning from Synthetic Humans. Xie, J., Zhu, S.-C., & Wu, Y. N. (2016, June 3). Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet. Yosinski, J., Clune, J., Nguyen, A., Fuchs, T., & Lipson, H. (2015, June 22). Understanding Neural Networks Through Deep Visualization. Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., & Metaxas, D. (2016, December 10). StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks. Hypnagogic imagery # Gurstelle, E. B., & de Oliveira, J. L. (2004). Daytime parahypnagogia: a state of consciousness that occurs when we almost fall asleep. Medical Hypotheses, 62(2), 166\u2013168. http://doi.org/10.1016/S0306-9877(03)00306-2 Holmes, E. A., James, E. L., Coode-Bate, T., & Deeprose, C. (2009). Can Playing the Computer Game \u201cTetris\u201d Reduce the Build-Up of Flashbacks for Trauma? A Proposal from Cognitive Science. PLoS ONE, 4(1), e4153. http://doi.org/10.1371/journal.pone.0004153.t004 Nielsen, T. A. (1995). Describing and modeling hypnagogic imagery using a systematic self-observation procedure. Dreaming, 5(2), 75\u201394. http://doi.org/10.1037/h0094426 Nielsen, T. A. (2016). A Self-Observational Study of Spontaneous Hypnagogic Imagery Using the Upright Napping Procedure. Imagination, Cognition and Personality, 11(4), 353\u2013366. http://doi.org/10.2190/3LVV-L5GY-UR5V-N0TG *Schacter, D. L. (1976). The hypnagogic state: a critical review of the literature. Psychological Bulletin. Stickgold, R. (2000). Replaying the Game: Hypnagogic Images in Normals and Amnesics. Science, 290(5490), 350\u2013353. http://doi.org/10.1126/science.290.5490.350 Dreams # Band, J. C. Z. F. A., 2016. (n.d.). Animal \u201cHypnosis\u201d and Waking Nightmares. Anomalistik.De Crick, F., G. Mitchison., 1983. The function of dream sleep. Nature. Springer *Hobson, J. A., & Mccarley, R. W. (197.). The brain as a dream state generator: an activation-synthesis hypothesis of the dream process. The American Journal of Psychiatry. Dresler, M., Koch, S. P., Wehrle, R., Spoormaker, V. I., Holsboer, F., Steiger, A., et al. (2011). Dreamed Movement Elicits Activation in the Sensorimotor Cortex. Current Biology : CB. Stickgold, R., Hobson, J. A., Fosse, R., & Fosse, M. (2001). Sleep, Learning, and Dreams: Off-line Memory Reprocessing. Science, 294(5544), 1052\u20131057. http://doi.org/10.1126/science.1063530 Stickgold, R. (2005). Sleep-dependent memory consolidation. Nature, 437(7063), 1272\u20131278. http://doi.org/10.1038/nature04286 Studies, J. H. J. O. C., 2014. (n.d.). Consciousness, dreams, and inference: the cartesian theatre revisited. Ingentaconnect.com Hallucinations # Bressloff, P. C., Cowan, J. D., Golubitsky, M., Thomas, P. J., & Wiener, M. C. (2002). What geometric visual hallucinations tell us about the visual cortex. Neural Computation, 14(3), 473\u2013491. http://doi.org/10.1162/089976602317250861 Cummings, J. L., & Miller, B. L. (1987). Visual hallucinations. Clinical occurrence and use in differential diagnosis. The Western Journal of Medicine, 146(1), 46\u201351. *Ermentrout, G. B., & Cowan, J. D. (1979). A mathematical theory of visual hallucination patterns. Biological Cybernetics, 34(3), 137\u2013150. http://doi.org/10.1007/BF00336965 Merabet, L. B., Maguire, D., Warde, A., Alterescu, K., Stickgold, R., & Pascual-Leone, A. (2004). Visual hallucinations during prolonged blindfolding in sighted subjects. Journal of Neuro-Ophthalmology, 24(2), 109\u2013113. Howard, R. J., Brammer, M. J., David, A., Woodruff, P., & Williams, S. (1998). The anatomy of conscious vision: an fMRI study of visual hallucinations. Nature neuroscience, 1(8), 738-742.Seri\u00e8s, P., Reichert, D. P., & Storkey, A. J. (2010). Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model, 2020\u20132028. Silverstein, S. M. (2016). Visual Perception Disturbances in Schizophrenia: A Unified Model. In The Neuropsychopathology of Schizophrenia: Molecules, Brain Systems, Motivation, and Cognition (3rd ed., Vol. 63, pp. 77\u2013132). Cham: Springer International Publishing. http://doi.org/10.1007/978-3-319-30596-7_4 Kumar, S., Sedley, W., Barnes, G. R., Teki, S., Friston, K. J., & Griffiths, T. D. (2014). A brain basis for musical hallucinations. Cortex, 52(C), 86\u201397. http://doi.org/10.1016/j.cortex.2013.12.002 Imagery and imagination # Chetverikov, A., & Kristj\u00e1nsson, \u00c1. (2016). On the joys of perceiving: Affect as feedback for perceptual predictions. Actpsy, 169(C), 1\u201310. http://doi.org/10.1016/j.actpsy.2016.05.005 Dijkstra, N., Zeidman, P., Ondobaka, S., Gerven, M. A. J., & Friston, K. (2017). Distinct Top-down and Bottom-up Brain Connectivity During Visual Perception and Imagery. Scientific Reports, 1\u20139. http://doi.org/10.1038/s41598-017-05888-8 Friston, K. J., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., & Ondobaka, S. (2017). Active Inference, Curiosity and Insight. Neural Computation, 29(10), 2633\u20132683. http://doi.org/10.1162/neco_a_00999 Kosslyn, S. M., & Thompson, W. L. (2003). When is early visual cortex activated during visual mental imagery? Psychological Bulletin, 129(5), 723\u2013746. http://doi.org/10.1037/0033-2909.129.5.723 Kosslyn, S. M., Alpert, N. M., Thompson, W. L., Maljkovic, V., Weise, S. B., Chabris, C. F., et al. (1993). Visual Mental Imagery Activates Topographically Organized Visual Cortex: PET Investigations. Journal of Cognitive Neuroscience, 5(3), 263\u2013287. http://doi.org/10.1162/jocn.1993.5.3.263 Kosslyn, S., & Ganis, G. (2000). Neural foundations of imagery. Nature Reviews \u2026. Pearson, J., Naselaris, T., Holmes, E. A., & Kosslyn, S. M. (2015). Mental Imagery: Functional Mechanisms and Clinical Applications. Trends in Cognitive Sciences, 19(10), 590\u2013602. http://doi.org/10.1016/j.tics.2015.08.003 Riecke, L., A. J. van Opstal, R. Goebel, and E. Formisano. \u201cHearing Illusory Sounds in Noise: Sensory-Perceptual Transformations in Primary Auditory Cortex.\u201d Journal of Neuroscience 27, no. 46 (November 14, 2007): 12684\u201389. https://doi.org/10.1523/JNEUROSCI.2713-07.2007. Schacter, D. L., Addis, D. R., Hassabis, D., Martin, V. C., Spreng, R. N., & Szpunar, K. K. (2012). The Future of Memory: Remembering, Imagining, and the Brain. Neuron, 76(4), 677\u2013694. http://doi.org/10.1016/j.neuron.2012.11.001 Zatorre, R. J., & Halpern, A. R. (2005). Mental Concerts: Musical Imagery and Auditory Cortex. Neuron, 47(1), 9\u201312. http://doi.org/10.1016/j.neuron.2005.06.013 Imagery and memory # Albers, A. M., Kok, P., Toni, I., Dijkerman, H. C., & de Lange, F. P. (2013). Shared Representations for Working Memory and Mental Imagery in Early Visual Cortex. Curbio, 23(15), 1427\u20131431. http://doi.org/10.1016/j.cub.2013.05.065 Christophel, T. B., Klink, P. C., Spitzer, B., Roelfsema, P. R., & Haynes, J.-D. (2017). The Distributed Nature of Working Memory. Trends in Cognitive Sciences, 1\u201315. http://doi.org/10.1016/j.tics.2016.12.007 Naselaris, T., Olman, C. A., Stansbury, D. E., Ugurbil, K., & Gallant, J. L. (2015). A voxel-wise encoding model for early visual areas decodes mental images of remembered scenes. NeuroImage, 105(C), 215\u2013228. http://doi.org/10.1016/j.neuroimage.2014.10.018 Self, M. W., van Kerkoerle, T., & Roelfsema, P. R. (2016). Layer-specificity in the effects of attention and working memory on activity in primary visual cortex. Nature Communications, 8, 1\u201312. http://doi.org/10.1038/ncomms13804 Stickgold, R. (2005). Sleep-dependent memory consolidation. Nature, 437(7063), 1272\u20131278. http://doi.org/10.1038/nature04286","title":"PSY8036SP2018"},{"location":"courses/PSY8036SP2018/#data-driven-generative-models-for-perception-dreaming-and-imagining-psy8036sp2018","text":"University of Minnesota, Spring Semester, 2018 Topics in Computational Vision Psy 8036 (Kersten) Psy 5993 Section 034 (Schrater) Instructors: Dan Kersten kersten@umn.edu Paul Schrater schrater@umn.edu Abstract It has been proposed that perception is fundamentally a process of \u201canalysis-by-synthesis\u201d in which the sensory input is analyzed bottom-up, with perceptual interpretations tested and refined by top-down predictions of the input, through synthesis. However, while the computational and neural study of the analysis component is well-developed, less is known about the principles and mechanisms that underly synthesis. This seminar will explore recent advances using \u201cdeep\u201d learning algorithms to discover hierarchical statistical regularities in large datasets of natural patterns, and the relevance of the learning results to models of human perception and recognition. These algorithms also provide the basis for the stochastic synthesis of novel, yet familiar patterns, which raises the question of whether the human experiences of dreams and hallucinations, and the ability to imagine, reflect the same statistical regularities that are discoverable using machine learning. The class format will include short introductory lectures by the instructors, and weekly student presentations of current literature. The short lectures will provide historical context as well as tutorials on machine learning (e.g. TensorFlow for neural network simulations).","title":"Data-driven generative models for perception, dreaming, and imagining PSY8036SP2018"},{"location":"courses/PSY8036SP2018/#background","text":"There is a long history of theories of perception in which the brain \u201cexplains\u201d sensory input in terms of external, behaviorally relevant causes. A current hypothesis is that this process is implemented in part by cortical feedback mechanisms that synthesize predictions of early data representations in order to test how well the brain's current interpretation of the world corresponds with the sensory data. In this view, perception involves a cycle in which the incoming data triggers a set of explanations, i.e. hypotheses, which are used to measure how far the expected sensory input differs from the actual input. From a computational perspective, such generative models of perceptual inference have a number of advantages over strictly bottom-up inference. A generative model can incorporate measures of \"goodness-of-fit\" to decide whether to accept or reject an interpretation--some explanations are better than others. Discrepancies between sensory data and predictions may also be used to direct attentional resources and signal whether more complex combinations of hypotheses are needed. Further, with sufficient structure, a generative model could provide the basis for the perceptual interpretation of sensory input outside the range of past experience. While computational theories for bottom-up neural mechanisms for perception have received considerable scientific attention, much less is known about top-down mechanisms. This seminar will explore the idea that the brain has hierarchically structured mechanisms that can synthesize patterns of input representations with the following constraints: 1) the mechanisms build on inductive structural biases that are innate; 2) the mechanisms reflect the statistical regularities induced by the physical causes of sensory experience, i.e. they are \"data-driven\"; 3) the need for cognitive processes to access semantic, perceptual content over levels of abstraction. Assumptions 1) and 2) constrain the class of generative models to be \"data-driven\", i.e. models that can be learned from sensory data. Recent computational methods for data-driven pattern synthesis (e.g. VAE, InfoGAN, Adversarial Bayes, StackGAN) will be covered in this seminar. We will also explore the proposal that the same circuitry that may underly feedback in perception is used during imagery, dreams, and hallucinations.","title":"Background"},{"location":"courses/PSY8036SP2018/#tentative-syllabus","text":"Week Topics Background material Discussion topics and papers 1: Jan 16 Background Models of perception Yuille, A., & Kersten, D. (2006). Vision as Bayesian inference: analysis by synthesis? Trends in Cognitive Sciences, 10(7), 301\u2013308. 2: Jan 23 Overview of machine learning Ackley, D. H., Hinton, G. E., & Sejnowski, T. J. (1985). A learning algorithm for Boltzmann machines. Cognitive Science, 9(1), 147\u2013169. 3: Jan 30 Shallow image models, textures Zhu, S. C., Wu, Y., & Mumford, D. (1998). Filters, random fields and maximum entropy (FRAME): Towards a unified theory for texture modeling. International Journal of Computer Vision, 27(2), 107\u2013126. McDermott, J. H., Schemitsch, M., & Simoncelli, E. P. (2013). Summary statistics in auditory perception. Nature Publishing Group, 16(4), 493\u2013498. 4: Feb 6 Hierarchical image models, deep learning Zhu, S.-C., & Mumford, D. (2006). Quest for a stochastic grammar of images. Foundations and Trends\u00ae in Computer Graphics and Vision, 2(4), 259\u2013362. Topic preview: Visual imagery 5: Feb 13 Hierarchical image models, deep learning Topic preview: Auditory imagery 6: Feb 20 Hierarchical image models, deep learning Topic preview: Hypnagogic imagery 7: Feb 27 Dynamic textures, patterns Xie, J., & Zhu, S. C. (n.d.). Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet. arXiv.org. Vondrick, C., Pirsiavash, H., & Torralba, A. (2016). Generating Videos with Scene Dynamics. Advances in Neural Information Processing Systems NIPS, 613\u2013621. Topic preview: Dreams 8: Mar 6 Visual imagery Christophel, T. B., Klink, P. C., Spitzer, B., Roelfsema, P. R., & Haynes, J.-D. (2017). The Distributed Nature of Working Memory. Trends in Cognitive Sciences, 1\u201315. Dijkstra, N., Zeidman, P., Ondobaka, S., Gerven, M. A. J., & Friston, K. (2017). Distinct Top-down and Bottom-up Brain Connectivity During Visual Perception and Imagery. Scientific Reports, 1\u20139. Topic preview: Lucid dreaming Mar 13 Spring Break 9: Mar 20 Auditory, musical imagery Zatorre, R. J., & Halpern, A. R. (2005). Mental Concerts: Musical Imagery and Auditory Cortex. Neuron, 47(1), 9\u201312. Riecke, L., A. J. van Opstal, R. Goebel, and E. Formisano. \u201cHearing Illusory Sounds in Noise: Sensory-Perceptual Transformations in Primary Auditory Cortex.\u201d Journal of Neuroscience 27, no. 46 (November 14, 2007): 12684\u201389. McDermott, Josh H., and Andrew J. Oxenham. \u201cSpectral Completion of Partially Masked Sounds.\u201d Proceedings of the National Academy of Sciences 105, no. 15 (2008): 5939\u20135944. Topic overview: Hallucinations & psychedelics 10: Mar 27 Hypnagogic imagery Schacter, D. L. (1976). The hypnagogic state: a critical review of the literature. Psychological Bulletin. Topic preview: Hallucinations & schizophrenia 11: Apr 3 Dreams Stickgold, R., Hobson, J. A., Fosse, R., & Fosse, M. (2001). Sleep, Learning, and Dreams: Off-line Memory Reprocessing. Science, 294(5544), 1052\u20131057. Crick, F., G. Mitchison., 1983. The function of dream sleep. Nature. Springer Topic preview: Imagination 12: Apr 10 Lucid dreaming Voss, U., Holzmann, R., Tuin, I., , J. A. Hobson., 2009. Lucid dreaming: a state of consciousness with features of both waking and non-lucid dreaming. Sleep. 13: Apr 17 Hallucinations Seri\u00e8s, P., Reichert, D. P., & Storkey, A. J. (2010). Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model, 2020\u20132028. Ermentrout, G. B., & Cowan, J. D. (1979). A mathematical theory of visual hallucination patterns. Biological Cybernetics, 34(3), 137\u2013150 Howard, R. J., Brammer, M. J., David, A., Woodruff, P., & Williams, S. (1998). The anatomy of conscious vision: an fMRI study of visual hallucinations. Nature neuroscience, 1(8), 738-742. 14: Apr 24 Hallucinations Kumar, S., Sedley, W., Barnes, G. R., Teki, S., Friston, K. J., & Griffiths, T. D. (2014). A brain basis for musical hallucinations. Cortex, 52(C), 86\u201397 15: May 1 Imagination, art and design Friston, K. J., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., & Ondobaka, S. (2017). Active Inference, Curiosity and Insight. Neural Computation, 29(10), 2633\u20132683. 16: May 8 Finals week FINAL PROJECT PRESENTATIONS","title":"Tentative Syllabus"},{"location":"courses/PSY8036SP2018/#sample-readings-under-construction","text":"","title":"Sample Readings (under construction)"},{"location":"courses/PSY8036SP2018/#background_1","text":"Ackley, D. H., Hinton, G. E., & Sejnowski, T. J. (1985). A learning algorithm for Boltzmann machines. Cognitive Science, 9(1), 147\u2013169. Bastos, A. M., Usrey, W. M., Adams, R. A., Mangun, G. R., Fries, P., & Friston, K. J. (2012). Canonical Microcircuits for Predictive Coding. Neuron, 76(4), 695\u2013711. Berkes, P., Orban, G., Lengyel, M., & Fiser, J. (2011). Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment. Science, 331(6013), 83\u201387. Dayan, P., Hinton, G. E., Neal, R. M., & Zemel, R. S. (1995). The Helmholtz Machine. Neural Computation, 7(5), 889\u2013904. Ouden, den, H. E. M. (2012). How prediction errors shape perception, attention, and motivation, 1\u201312. Orban, G., Pietro Berkes, Fiser, J., & Lengyel, M. (2016). Neural Variability and Sampling-Based Probabilistic Representations in the Visual Cortex. Neuron, 92(2), 530\u2013543. MacKay, D. M. (1956). Towards an information-flow model of human behaviour. British Journal of Psychology (London, England : 1953), 47(1), 30\u201343. Lake, B. M., Salakhutdinov, R., & Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266), 1332\u20131338. http://doi.org/10.1126/science.aab3050 LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436\u2013444. http://doi.org/10.1038/nature14539 McDermott, Josh H., and Andrew J. Oxenham. \u201cSpectral Completion of Partially Masked Sounds.\u201d Proceedings of the National Academy of Sciences 105, no. 15 (2008): 5939\u20135944.Mumford, D. (1992). On the computational architecture of the neocortex. Biological Cybernetics, 66(3), 241\u2013251. Mumford, D. (1994). Pattern theory: a unifying perspective, 187\u2013224. Rao, R. P. N., & Ballard, D. H. (1999). Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nature Neuroscience, 2, 79\u201387. Tu, Z., Chen, X., Yuille, A. L., & Zhu, S.-C. (2005). Image parsing: Unifying segmentation, detection, and recognition. International Journal of Computer Vision, 63(2), 113\u2013140. Yuille, A., & Kersten, D. (2006). Vision as Bayesian inference: analysis by synthesis? Trends in Cognitive Sciences, 10(7), 301\u2013308. Richards, W. (1971). The Fortification Illusions of Migraines, Scientific American, 1\u201310. Zhu, S.-C., & Mumford, D. (2006). Quest for a stochastic grammar of images. Foundations and Trends\u00ae in Computer Graphics and Vision, 2(4), 259\u2013362. http://doi.org/10.1561/0600000018","title":"Background"},{"location":"courses/PSY8036SP2018/#shallow-generative-models-texture-synthesis","text":"Freeman, J., & Simoncelli, E. P. (2011). Metamers of the ventral stream. Nature Publishing Group, 14(9), 1195\u20131201. http://doi.org/10.1038/nn.2889 McDermott, J. H., Schemitsch, M., & Simoncelli, E. P. (2013). Summary statistics in auditory perception. Nature Publishing Group, 16(4), 493\u2013498. McDermott, J. H., & Simoncelli, E. P. (2011). Sound Texture Perception via Statistics of the Auditory Periphery: Evidence from Sound Synthesis. Neuron, 71(5), 926\u2013940. Zhu, S. C., Wu, Y., & Mumford, D. (1998). Filters, random fields and maximum entropy (FRAME): Towards a unified theory for texture modeling. International Journal of Computer Vision, 27(2), 107\u2013126.","title":"Shallow generative models: Texture synthesis"},{"location":"courses/PSY8036SP2018/#hierarchical-deep-data-driven-generative-models","text":"Chen, X., Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., & Abbeel, P. (2016). InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets, 2172\u20132180. Goodfellow, I. (2016, December 31). NIPS 2016 Tutorial: Generative Adversarial Networks. Kulkarni, T. D., Whitney, W. F., Kohli, P., & Tenenbaum, J. (2015). Deep Convolutional Inverse Graphics Network, 2539\u20132547. Rock, J., Issaranon, T., Deshpande, A., & Forsyth, D. (2016, December 5). Authoring image decompositions with generative models. Varol, G., Romero, J., Martin, X., Mahmood, N., Black, M. J., Laptev, I., & Schmid, C. (2017, January 5). Learning from Synthetic Humans. Xie, J., Zhu, S.-C., & Wu, Y. N. (2016, June 3). Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet. Yosinski, J., Clune, J., Nguyen, A., Fuchs, T., & Lipson, H. (2015, June 22). Understanding Neural Networks Through Deep Visualization. Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., & Metaxas, D. (2016, December 10). StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks.","title":"Hierarchical (deep) data-driven generative models"},{"location":"courses/PSY8036SP2018/#hypnagogic-imagery","text":"Gurstelle, E. B., & de Oliveira, J. L. (2004). Daytime parahypnagogia: a state of consciousness that occurs when we almost fall asleep. Medical Hypotheses, 62(2), 166\u2013168. http://doi.org/10.1016/S0306-9877(03)00306-2 Holmes, E. A., James, E. L., Coode-Bate, T., & Deeprose, C. (2009). Can Playing the Computer Game \u201cTetris\u201d Reduce the Build-Up of Flashbacks for Trauma? A Proposal from Cognitive Science. PLoS ONE, 4(1), e4153. http://doi.org/10.1371/journal.pone.0004153.t004 Nielsen, T. A. (1995). Describing and modeling hypnagogic imagery using a systematic self-observation procedure. Dreaming, 5(2), 75\u201394. http://doi.org/10.1037/h0094426 Nielsen, T. A. (2016). A Self-Observational Study of Spontaneous Hypnagogic Imagery Using the Upright Napping Procedure. Imagination, Cognition and Personality, 11(4), 353\u2013366. http://doi.org/10.2190/3LVV-L5GY-UR5V-N0TG *Schacter, D. L. (1976). The hypnagogic state: a critical review of the literature. Psychological Bulletin. Stickgold, R. (2000). Replaying the Game: Hypnagogic Images in Normals and Amnesics. Science, 290(5490), 350\u2013353. http://doi.org/10.1126/science.290.5490.350","title":"Hypnagogic imagery"},{"location":"courses/PSY8036SP2018/#dreams","text":"Band, J. C. Z. F. A., 2016. (n.d.). Animal \u201cHypnosis\u201d and Waking Nightmares. Anomalistik.De Crick, F., G. Mitchison., 1983. The function of dream sleep. Nature. Springer *Hobson, J. A., & Mccarley, R. W. (197.). The brain as a dream state generator: an activation-synthesis hypothesis of the dream process. The American Journal of Psychiatry. Dresler, M., Koch, S. P., Wehrle, R., Spoormaker, V. I., Holsboer, F., Steiger, A., et al. (2011). Dreamed Movement Elicits Activation in the Sensorimotor Cortex. Current Biology : CB. Stickgold, R., Hobson, J. A., Fosse, R., & Fosse, M. (2001). Sleep, Learning, and Dreams: Off-line Memory Reprocessing. Science, 294(5544), 1052\u20131057. http://doi.org/10.1126/science.1063530 Stickgold, R. (2005). Sleep-dependent memory consolidation. Nature, 437(7063), 1272\u20131278. http://doi.org/10.1038/nature04286 Studies, J. H. J. O. C., 2014. (n.d.). Consciousness, dreams, and inference: the cartesian theatre revisited. Ingentaconnect.com","title":"Dreams"},{"location":"courses/PSY8036SP2018/#hallucinations","text":"Bressloff, P. C., Cowan, J. D., Golubitsky, M., Thomas, P. J., & Wiener, M. C. (2002). What geometric visual hallucinations tell us about the visual cortex. Neural Computation, 14(3), 473\u2013491. http://doi.org/10.1162/089976602317250861 Cummings, J. L., & Miller, B. L. (1987). Visual hallucinations. Clinical occurrence and use in differential diagnosis. The Western Journal of Medicine, 146(1), 46\u201351. *Ermentrout, G. B., & Cowan, J. D. (1979). A mathematical theory of visual hallucination patterns. Biological Cybernetics, 34(3), 137\u2013150. http://doi.org/10.1007/BF00336965 Merabet, L. B., Maguire, D., Warde, A., Alterescu, K., Stickgold, R., & Pascual-Leone, A. (2004). Visual hallucinations during prolonged blindfolding in sighted subjects. Journal of Neuro-Ophthalmology, 24(2), 109\u2013113. Howard, R. J., Brammer, M. J., David, A., Woodruff, P., & Williams, S. (1998). The anatomy of conscious vision: an fMRI study of visual hallucinations. Nature neuroscience, 1(8), 738-742.Seri\u00e8s, P., Reichert, D. P., & Storkey, A. J. (2010). Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model, 2020\u20132028. Silverstein, S. M. (2016). Visual Perception Disturbances in Schizophrenia: A Unified Model. In The Neuropsychopathology of Schizophrenia: Molecules, Brain Systems, Motivation, and Cognition (3rd ed., Vol. 63, pp. 77\u2013132). Cham: Springer International Publishing. http://doi.org/10.1007/978-3-319-30596-7_4 Kumar, S., Sedley, W., Barnes, G. R., Teki, S., Friston, K. J., & Griffiths, T. D. (2014). A brain basis for musical hallucinations. Cortex, 52(C), 86\u201397. http://doi.org/10.1016/j.cortex.2013.12.002","title":"Hallucinations"},{"location":"courses/PSY8036SP2018/#imagery-and-imagination","text":"Chetverikov, A., & Kristj\u00e1nsson, \u00c1. (2016). On the joys of perceiving: Affect as feedback for perceptual predictions. Actpsy, 169(C), 1\u201310. http://doi.org/10.1016/j.actpsy.2016.05.005 Dijkstra, N., Zeidman, P., Ondobaka, S., Gerven, M. A. J., & Friston, K. (2017). Distinct Top-down and Bottom-up Brain Connectivity During Visual Perception and Imagery. Scientific Reports, 1\u20139. http://doi.org/10.1038/s41598-017-05888-8 Friston, K. J., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., & Ondobaka, S. (2017). Active Inference, Curiosity and Insight. Neural Computation, 29(10), 2633\u20132683. http://doi.org/10.1162/neco_a_00999 Kosslyn, S. M., & Thompson, W. L. (2003). When is early visual cortex activated during visual mental imagery? Psychological Bulletin, 129(5), 723\u2013746. http://doi.org/10.1037/0033-2909.129.5.723 Kosslyn, S. M., Alpert, N. M., Thompson, W. L., Maljkovic, V., Weise, S. B., Chabris, C. F., et al. (1993). Visual Mental Imagery Activates Topographically Organized Visual Cortex: PET Investigations. Journal of Cognitive Neuroscience, 5(3), 263\u2013287. http://doi.org/10.1162/jocn.1993.5.3.263 Kosslyn, S., & Ganis, G. (2000). Neural foundations of imagery. Nature Reviews \u2026. Pearson, J., Naselaris, T., Holmes, E. A., & Kosslyn, S. M. (2015). Mental Imagery: Functional Mechanisms and Clinical Applications. Trends in Cognitive Sciences, 19(10), 590\u2013602. http://doi.org/10.1016/j.tics.2015.08.003 Riecke, L., A. J. van Opstal, R. Goebel, and E. Formisano. \u201cHearing Illusory Sounds in Noise: Sensory-Perceptual Transformations in Primary Auditory Cortex.\u201d Journal of Neuroscience 27, no. 46 (November 14, 2007): 12684\u201389. https://doi.org/10.1523/JNEUROSCI.2713-07.2007. Schacter, D. L., Addis, D. R., Hassabis, D., Martin, V. C., Spreng, R. N., & Szpunar, K. K. (2012). The Future of Memory: Remembering, Imagining, and the Brain. Neuron, 76(4), 677\u2013694. http://doi.org/10.1016/j.neuron.2012.11.001 Zatorre, R. J., & Halpern, A. R. (2005). Mental Concerts: Musical Imagery and Auditory Cortex. Neuron, 47(1), 9\u201312. http://doi.org/10.1016/j.neuron.2005.06.013","title":"Imagery and imagination"},{"location":"courses/PSY8036SP2018/#imagery-and-memory","text":"Albers, A. M., Kok, P., Toni, I., Dijkerman, H. C., & de Lange, F. P. (2013). Shared Representations for Working Memory and Mental Imagery in Early Visual Cortex. Curbio, 23(15), 1427\u20131431. http://doi.org/10.1016/j.cub.2013.05.065 Christophel, T. B., Klink, P. C., Spitzer, B., Roelfsema, P. R., & Haynes, J.-D. (2017). The Distributed Nature of Working Memory. Trends in Cognitive Sciences, 1\u201315. http://doi.org/10.1016/j.tics.2016.12.007 Naselaris, T., Olman, C. A., Stansbury, D. E., Ugurbil, K., & Gallant, J. L. (2015). A voxel-wise encoding model for early visual areas decodes mental images of remembered scenes. NeuroImage, 105(C), 215\u2013228. http://doi.org/10.1016/j.neuroimage.2014.10.018 Self, M. W., van Kerkoerle, T., & Roelfsema, P. R. (2016). Layer-specificity in the effects of attention and working memory on activity in primary visual cortex. Nature Communications, 8, 1\u201312. http://doi.org/10.1038/ncomms13804 Stickgold, R. (2005). Sleep-dependent memory consolidation. Nature, 437(7063), 1272\u20131278. http://doi.org/10.1038/nature04286","title":"Imagery and memory"},{"location":"courses/PSY8036SP2019/","text":"Deep networks: Behavior, Brain and Theory PSY8036SP2019 # University of Minnesota, Spring Semester, 2019 Topics in Computational Vision PSY 8036 PSY 5993 Section 034 Instructors: Dan Kersten: kersten@umn.edu Paul Schrater: schrater@umn.edu Abstract Recent rapid advances in deep learning networks have provided the means to produce \u201cimage computable models\u201d of human vision\u2013models that take natural images as input and produce accurate predictions of perceptual decisions. However, the current and future value of deep network research for understanding the brain\u2019s visual system faces both methodological and conceptual challenges. What are the best methods to compare deep networks to perceptual behavior and to the brain? And how can we achieve a conceptual understanding of the networks, to determine which elements are important and which are not? We will read and discuss empirical papers that compare network models of object recognition to behavior and the brain. The seminar will also review work that is helping to understand what functions networks can compute, and the limitations on learning to generalize. Finally, we will discuss advances that will be needed to understand the human ability to interpret virtually any image\u2013an ability that spans a wide range of visual tasks. The class format will include short introductory lectures by the instructors to provide historical context and weekly student presentations of current literature. Students will have the opportunity to collaborate on final programming projects. The course will also introduce and use Julia, a rapidly developing language for scientific programming, which is fast, flexible, and relatively easy to learn and use. Background # Warning !!!DRAFT!!! There are good online resources for learning about artificial neural networks, and in particular deep convolutional neural networks. For video content, there is the Neural Networks for Machine Learning from Geoff Hinton\u2019s 2016 coursera lectures and Fei-Fei Li\u2019s Stanford 231n course, Convolutional Neural Networks for Visual Recognition . For books, see Ian Goodfellow\u2019s Deep Learning free online or for purchase , and Visual Cortex and Deep Networks: Learning Invariant Representations for purchase by Tomaso Poggio and Fabio Anselmi. For an excellent basic background and review, watch the first 9 lectures of the Stanford 231n course . Introduction # Week 1 Deep architectures # The first class will cover background and overview of the problems of human image understanding and visual recognition and how these problems have been approached. We\u2019ll go over the goals of the seminar in the context of three questions: 1) Is computational vision close to producing biologically consistent, predictive, models of human visual recognition performance? 2) assuming that candidate models exist; how well do we understand them for example, to decide when two networks are equivalent? 3) What is missing from current theories, in terms of conceptual understanding, behavioral functionality and their neural bases? To address the first question above, we\u2019ll review empirical papers, from psychophysics and neuroscience aimed at understanding the basic-level or \u201ccore\u201d function of rapid object identification with the goal of determining best ways of comparing network models to behavior and the brain. To address the second question, we will review theoretical work that seeks to understand what functions networks can compute, and how efficiently they can learn the parameters (e.g. \u201cweights\u201d) of those functions. Finally, we will assess where research may need to go to understand human ability to interpret virtually any image\u2013a challenge that will require advances in dynamic neural architectures that allow task flexibility. Background: For a short video Introduction to deep networks, see Lecture 1 from the MIT short course (6.S191): Introduction to Deep Learning. To run Julia programs and Jupyter notebooks locally on your computer, first install Julia , and then use the anaconda distribution to install Jupyter. For video instructions see: installing Julia and Jupyter . But the quickest and easiest way to start learning and using the Julia language is to sign in to JuliaBox where you\u2019ll immediately be able to create notebooks and access tutorials. Week 3 Deep networks and the brain & Intro to Julia programming # Background: Video introduction to Julia programming. Introduction to Julia for Data Science and Scientific Computing Readings: Kietzmann, T. C., McClure, P., & Kriegeskorte, N. (2018, June 5). Deep Neural Networks In Computational Neuroscience. bioRxiv. doi: 10.1101/133504 Week 4 Deep networks and the brain reviews & Simulations with the Julia language # Readings: Turner, M. H., Sanchez Giraldo, L. G., Schwartz, O., & Rieke, F. (2019, January). Stimulus- and goal-oriented frameworks for understanding natural vision. Nature Neuroscience, 22(1), 15\u201324. doi: 10.1038/s41593-018-0284-0 Jacobs, R. A. & Bates, C. J. (2018, November 27). Comparing the Visual Representations and Performance of Humans and Deep Neural Networks. Current Directions in Psychological Science, 0963721418801342. doi: 10.1177/0963721418801342 Kay, K. N. (2018, October 15). Principles for models of neural information processing. NeuroImage. New Advances in Encoding and Decoding of Brain Signals, 180, 101\u2013109. doi: 10.1016/j.neuroimage.2017.08.016 Empirical tests # The most basic perceptual test is \u201clook and see\u201d which is what is often what is done by modelers. One can treat \u201cadversarial examples\u201d as behavioral tests to guide model development. However, one can be systematic and ask what parametric manipulations can be made of test images that we expect humans to generalize \u201cfor free\u201d. How best to design \u201cadversarial tasks\" for a model network? For example, based on typical experience, we expect little cost to testing on certain families of novel image variations. These manipulations can be based on 3D variables (e.g. object transformations such as 3D rotation, cast shadows, occlusion), or image variables (e.g. lower contrast, blur, noise). Manipulations can be based on highly artificial variations that we already know humans have generalization abilities, if limited, such as reverse contrast, non-linear histograms, morphs, and atypical occlusions. Given these manipulations, what are good quantitative measures that can be applied to both human and model observers? When does more model training solve the problem, and can we understand when to rule out a class of network architectures? Most current perceptual tests have been with feedforward DCNNs, and that is where we start. Week 5 Empirical tests of DCNNs: human object recognition # Methodological challenges and the design of adversarial tasks. Readings: Ullman, S., Assif, L., Fetaya, E., & Harari, D. (2016, March 8). Atoms of recognition in human and computer vision. Proceedings of the National Academy of Sciences, 113(10), 2744\u20132749. doi: 10.1073/pnas.1513198113 . pmid: 26884200 Zhang, R., Isola, P., Efros, A. A., Shechtman, E., & Wang, O. (2018). The Unreasonable Effectiveness of Deep Features as a Perceptual Metric, 10. Retrieved from http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0299.pdf Elsayed, G. F., Shankar, S., Cheung, B., Papernot, N., Kurakin, A., Goodfellow, I., & Sohl-Dickstein, J. (2018, February 22). Adversarial Examples that Fool both Computer Vision and Time-Limited Humans. arXiv: 1802.08195 [cs, q-bio, stat] . Retrieved November 13, 2018, from http://arxiv.org/abs/1802.08195 Geirhos, R., Temme, C. R. M., Rauber, J., Sch\ufffdtt, H. H., Bethge, M., & Wichmann, F. A. (2018). Generalisation in humans and deep neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 31 (pp. 7549\u20137561). Curran Associates, Inc. Retrieved January 28, 2019, from http://papers.nips.cc/paper/7982-generalisation-in-humans-and-deep-neural-networks.pdf Week 6 Empirical tests of DCNNs and methods of study # Including discussion of generative models, manifold discovery, posterior estimation \u2013 useful tools? Readings: Hill, M. Q., Parde, C. J., Castillo, C. D., Colon, Y. I., Ranjan, R., Chen, J.-C., \u2026, & O\u2019Toole, A. J. (2018, December 28). Deep Convolutional Neural Networks in the Face of Caricature: Identity and Image Revealed. arXiv: 1812.10902 [cs] . Retrieved January 23, 2019, from http://arxiv.org/abs/1812.10902 Zhang, M., Feng, J., Ma, K. T., Lim, J. H., Zhao, Q., & Kreiman, G. (2018, December). Finding any Waldo with zero-shot invariant and efficient visual search. Nature Communications, 9(1). doi: 10.1038/s41467-018-06217-x Ricci, M., Kim, J., & Serre, T. (2018, February 9). Same-different problems strain convolutional neural networks. arXiv: 1802.03390 [cs, q-bio] . Retrieved December 11, 2018, from http://arxiv.org/abs/1802.03390 Luo, W., Li, Y., Urtasun, R., & Zemel, R. (n.d.). Understanding the Effective Receptive Field in Deep Convolutional Neural Networks, 9 Week 7 DCNNs and the brain # Zhou, B., Bau, D., Oliva, A., & Torralba, A. (2018). Interpreting Deep Visual Representations via Network Dissection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1\u20131. doi: 10.1109/TPAMI.2018.2858759 Readings: Rajalingham, R., Issa, E. B., Bashivan, P., Kar, K., Schmidt, K., & DiCarlo, J. J. (2018, February 12). Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks. doi: 10.1101/240614 Schrimpf, M., Kubilius, J., Hong, H., Majaj, N. J., Rajalingham, R., Issa, E. B., \u2026, & DiCarlo, J. J. (2018, September 5). Brain-Score: Which Artificial Neural Network for Object Recognition is most Brain-Like? doi: 10.1101/407007 Breedlove, J. L., St-Yves, G., Olman, C. A., & Naselaris, T. (2018, November 9). Human brain activity during mental imagery exhibits signatures of inference in a hierarchical generative model. doi: 10.1101/462226 Understanding deep networks # Week 8 Theory: shallow vs. deep # Universal approximators. Hierarchy as a solution to over-fitting. Poggio, T., Mhaskar, H., Rosasco, L., Miranda, B., & Liao, Q. (2017, October 1). Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review. International Journal of Automation and Computing, 14(5), 503\u2013519. doi: 10.1007/s11633-017-1054-2 Garriga-Alonso, A., Aitchison, L., & Rasmussen, C. E. (2019). DEEP CONVOLUTIONAL NETWORKS AS SHALLOW GAUSSIAN PROCESSES, 16 Lin, H. & Jegelka, S. (2018, June 28). ResNet with one-neuron hidden layers is a Universal Approximator. arXiv: 1806.10909 [cs, stat] . Retrieved January 18, 2019, from http://arxiv.org/abs/1806.10909 Week 9 Theory: DCNNs and implicit generative models # Feedforward DCNNs, implicit generative models, texture, and maximum entropy. Readings: Xie, J., Zhu, S.-C., & Wu, Y. N. (2017). Synthesizing dynamic patterns by spatial-temporal generative convnet. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 7093\u20137101). Retrieved from http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Synthesizing_Dynamic_Patterns_CVPR_2017_paper.pdf Zhang, Q., Wu, Y. N., & Zhu, S.-C. (2017, October 2). Interpretable Convolutional Neural Networks. arXiv: 1710.00935 [cs] . Retrieved February 22, 2018, from http://arxiv.org/abs/1710.00935 WU, Y. N., XIE, J., LU, Y., & ZHU, S.-C. (2018). Sparse and Deep Generalizations of the FRAME Model. Retrieved from https://www.intlpress.com/site/pub/files/_fulltext/journals/amsa/2018/0003/0001/AMSA-2018-0003-0001-a007.pdf Week 10 Theory: invariance # Readings: Tacchetti, A., Isik, L., & Poggio, T. A. (2018, September 15). Invariant Recognition Shapes Neural Representations of Visual Input. Annual Review of Vision Science, 4(1), 403\u2013422. doi: 10.1146/annurev-vision-091517-034103 Leibo, J. Z., Liao, Q., Anselmi, F., & Poggio, T. (2015, October 23). The Invariance Hypothesis Implies Domain-Specific Regions in Visual Cortex. PLOS Computational Biology, 11(10), e1004390. doi: 10.1371/journal.pcbi.1004390 Azulay, A. & Weiss, Y. (2018, May 30). Why do deep convolutional networks generalize so poorly to small image transformations? arXiv: 1805.12177 [cs] . Retrieved October 4, 2018, from http://arxiv.org/abs/1805.12177 Week 11 Theory: normalization # The value and types of normalization: batch, spatial, temporal, and channel normalization. Liao, Q., Kawaguchi, K., & Poggio, T. (2016, October 19). Streaming Normalization: Towards Simpler and More Biologically-plausible Normalizations for Online and Recurrent Learning. arXiv: 1610.06160 [cs] . Retrieved December 7, 2018, from http://arxiv.org/abs/1610.06160 What\u2019s missing? # Week 12 Compositionality and semantic accessibility # The problems of occlusion and articulation (e.g. body pose). Computing spatial relationships,... Readings: Burgess, C. P., Matthey, L., Watters, N., Kabra, R., Higgins, I., Botvinick, M., & Lerchner, A. (2019, January 22). MONet: Unsupervised Scene Decomposition and Representation. arXiv: 1901.11390 [cs, stat] . Retrieved February 5, 2019, from http://arxiv.org/abs/1901.11390 Tang, H., Schrimpf, M., Lotter, W., Moerman, C., Paredes, A., Ortega Caro, J., \u2026, & Kreiman, G. (2018, August 28). Recurrent computations for visual pattern completion. Proceedings of the National Academy of Sciences, 115(35), 8835\u20138840. doi: 10.1073/pnas.1719397115 Zhang, Z., Xie, C., Wang, J.","title":"Deep networks: Behavior, Brain and Theory <small>PSY8036SP2019</small>"},{"location":"courses/PSY8036SP2019/#deep-networks-behavior-brain-and-theory-psy8036sp2019","text":"University of Minnesota, Spring Semester, 2019 Topics in Computational Vision PSY 8036 PSY 5993 Section 034 Instructors: Dan Kersten: kersten@umn.edu Paul Schrater: schrater@umn.edu Abstract Recent rapid advances in deep learning networks have provided the means to produce \u201cimage computable models\u201d of human vision\u2013models that take natural images as input and produce accurate predictions of perceptual decisions. However, the current and future value of deep network research for understanding the brain\u2019s visual system faces both methodological and conceptual challenges. What are the best methods to compare deep networks to perceptual behavior and to the brain? And how can we achieve a conceptual understanding of the networks, to determine which elements are important and which are not? We will read and discuss empirical papers that compare network models of object recognition to behavior and the brain. The seminar will also review work that is helping to understand what functions networks can compute, and the limitations on learning to generalize. Finally, we will discuss advances that will be needed to understand the human ability to interpret virtually any image\u2013an ability that spans a wide range of visual tasks. The class format will include short introductory lectures by the instructors to provide historical context and weekly student presentations of current literature. Students will have the opportunity to collaborate on final programming projects. The course will also introduce and use Julia, a rapidly developing language for scientific programming, which is fast, flexible, and relatively easy to learn and use.","title":"Deep networks: Behavior, Brain and Theory PSY8036SP2019"},{"location":"courses/PSY8036SP2019/#background","text":"Warning !!!DRAFT!!! There are good online resources for learning about artificial neural networks, and in particular deep convolutional neural networks. For video content, there is the Neural Networks for Machine Learning from Geoff Hinton\u2019s 2016 coursera lectures and Fei-Fei Li\u2019s Stanford 231n course, Convolutional Neural Networks for Visual Recognition . For books, see Ian Goodfellow\u2019s Deep Learning free online or for purchase , and Visual Cortex and Deep Networks: Learning Invariant Representations for purchase by Tomaso Poggio and Fabio Anselmi. For an excellent basic background and review, watch the first 9 lectures of the Stanford 231n course .","title":"Background"},{"location":"courses/PSY8036SP2019/#introduction","text":"","title":"Introduction"},{"location":"courses/PSY8036SP2019/#week-1-deep-architectures","text":"The first class will cover background and overview of the problems of human image understanding and visual recognition and how these problems have been approached. We\u2019ll go over the goals of the seminar in the context of three questions: 1) Is computational vision close to producing biologically consistent, predictive, models of human visual recognition performance? 2) assuming that candidate models exist; how well do we understand them for example, to decide when two networks are equivalent? 3) What is missing from current theories, in terms of conceptual understanding, behavioral functionality and their neural bases? To address the first question above, we\u2019ll review empirical papers, from psychophysics and neuroscience aimed at understanding the basic-level or \u201ccore\u201d function of rapid object identification with the goal of determining best ways of comparing network models to behavior and the brain. To address the second question, we will review theoretical work that seeks to understand what functions networks can compute, and how efficiently they can learn the parameters (e.g. \u201cweights\u201d) of those functions. Finally, we will assess where research may need to go to understand human ability to interpret virtually any image\u2013a challenge that will require advances in dynamic neural architectures that allow task flexibility. Background: For a short video Introduction to deep networks, see Lecture 1 from the MIT short course (6.S191): Introduction to Deep Learning. To run Julia programs and Jupyter notebooks locally on your computer, first install Julia , and then use the anaconda distribution to install Jupyter. For video instructions see: installing Julia and Jupyter . But the quickest and easiest way to start learning and using the Julia language is to sign in to JuliaBox where you\u2019ll immediately be able to create notebooks and access tutorials.","title":"Week 1 Deep architectures"},{"location":"courses/PSY8036SP2019/#week-3-deep-networks-and-the-brain-intro-to-julia-programming","text":"Background: Video introduction to Julia programming. Introduction to Julia for Data Science and Scientific Computing Readings: Kietzmann, T. C., McClure, P., & Kriegeskorte, N. (2018, June 5). Deep Neural Networks In Computational Neuroscience. bioRxiv. doi: 10.1101/133504","title":"Week 3 Deep networks and the brain &amp; Intro to Julia programming"},{"location":"courses/PSY8036SP2019/#week-4-deep-networks-and-the-brain-reviews-simulations-with-the-julia-language","text":"Readings: Turner, M. H., Sanchez Giraldo, L. G., Schwartz, O., & Rieke, F. (2019, January). Stimulus- and goal-oriented frameworks for understanding natural vision. Nature Neuroscience, 22(1), 15\u201324. doi: 10.1038/s41593-018-0284-0 Jacobs, R. A. & Bates, C. J. (2018, November 27). Comparing the Visual Representations and Performance of Humans and Deep Neural Networks. Current Directions in Psychological Science, 0963721418801342. doi: 10.1177/0963721418801342 Kay, K. N. (2018, October 15). Principles for models of neural information processing. NeuroImage. New Advances in Encoding and Decoding of Brain Signals, 180, 101\u2013109. doi: 10.1016/j.neuroimage.2017.08.016","title":"Week 4 Deep networks and the brain reviews &amp; Simulations with the Julia language"},{"location":"courses/PSY8036SP2019/#empirical-tests","text":"The most basic perceptual test is \u201clook and see\u201d which is what is often what is done by modelers. One can treat \u201cadversarial examples\u201d as behavioral tests to guide model development. However, one can be systematic and ask what parametric manipulations can be made of test images that we expect humans to generalize \u201cfor free\u201d. How best to design \u201cadversarial tasks\" for a model network? For example, based on typical experience, we expect little cost to testing on certain families of novel image variations. These manipulations can be based on 3D variables (e.g. object transformations such as 3D rotation, cast shadows, occlusion), or image variables (e.g. lower contrast, blur, noise). Manipulations can be based on highly artificial variations that we already know humans have generalization abilities, if limited, such as reverse contrast, non-linear histograms, morphs, and atypical occlusions. Given these manipulations, what are good quantitative measures that can be applied to both human and model observers? When does more model training solve the problem, and can we understand when to rule out a class of network architectures? Most current perceptual tests have been with feedforward DCNNs, and that is where we start.","title":"Empirical tests"},{"location":"courses/PSY8036SP2019/#week-5-empirical-tests-of-dcnns-human-object-recognition","text":"Methodological challenges and the design of adversarial tasks. Readings: Ullman, S., Assif, L., Fetaya, E., & Harari, D. (2016, March 8). Atoms of recognition in human and computer vision. Proceedings of the National Academy of Sciences, 113(10), 2744\u20132749. doi: 10.1073/pnas.1513198113 . pmid: 26884200 Zhang, R., Isola, P., Efros, A. A., Shechtman, E., & Wang, O. (2018). The Unreasonable Effectiveness of Deep Features as a Perceptual Metric, 10. Retrieved from http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0299.pdf Elsayed, G. F., Shankar, S., Cheung, B., Papernot, N., Kurakin, A., Goodfellow, I., & Sohl-Dickstein, J. (2018, February 22). Adversarial Examples that Fool both Computer Vision and Time-Limited Humans. arXiv: 1802.08195 [cs, q-bio, stat] . Retrieved November 13, 2018, from http://arxiv.org/abs/1802.08195 Geirhos, R., Temme, C. R. M., Rauber, J., Sch\ufffdtt, H. H., Bethge, M., & Wichmann, F. A. (2018). Generalisation in humans and deep neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 31 (pp. 7549\u20137561). Curran Associates, Inc. Retrieved January 28, 2019, from http://papers.nips.cc/paper/7982-generalisation-in-humans-and-deep-neural-networks.pdf","title":"Week 5 Empirical tests of DCNNs: human object recognition"},{"location":"courses/PSY8036SP2019/#week-6-empirical-tests-of-dcnns-and-methods-of-study","text":"Including discussion of generative models, manifold discovery, posterior estimation \u2013 useful tools? Readings: Hill, M. Q., Parde, C. J., Castillo, C. D., Colon, Y. I., Ranjan, R., Chen, J.-C., \u2026, & O\u2019Toole, A. J. (2018, December 28). Deep Convolutional Neural Networks in the Face of Caricature: Identity and Image Revealed. arXiv: 1812.10902 [cs] . Retrieved January 23, 2019, from http://arxiv.org/abs/1812.10902 Zhang, M., Feng, J., Ma, K. T., Lim, J. H., Zhao, Q., & Kreiman, G. (2018, December). Finding any Waldo with zero-shot invariant and efficient visual search. Nature Communications, 9(1). doi: 10.1038/s41467-018-06217-x Ricci, M., Kim, J., & Serre, T. (2018, February 9). Same-different problems strain convolutional neural networks. arXiv: 1802.03390 [cs, q-bio] . Retrieved December 11, 2018, from http://arxiv.org/abs/1802.03390 Luo, W., Li, Y., Urtasun, R., & Zemel, R. (n.d.). Understanding the Effective Receptive Field in Deep Convolutional Neural Networks, 9","title":"Week 6 Empirical tests of DCNNs and methods of study"},{"location":"courses/PSY8036SP2019/#week-7-dcnns-and-the-brain","text":"Zhou, B., Bau, D., Oliva, A., & Torralba, A. (2018). Interpreting Deep Visual Representations via Network Dissection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1\u20131. doi: 10.1109/TPAMI.2018.2858759 Readings: Rajalingham, R., Issa, E. B., Bashivan, P., Kar, K., Schmidt, K., & DiCarlo, J. J. (2018, February 12). Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks. doi: 10.1101/240614 Schrimpf, M., Kubilius, J., Hong, H., Majaj, N. J., Rajalingham, R., Issa, E. B., \u2026, & DiCarlo, J. J. (2018, September 5). Brain-Score: Which Artificial Neural Network for Object Recognition is most Brain-Like? doi: 10.1101/407007 Breedlove, J. L., St-Yves, G., Olman, C. A., & Naselaris, T. (2018, November 9). Human brain activity during mental imagery exhibits signatures of inference in a hierarchical generative model. doi: 10.1101/462226","title":"Week 7 DCNNs and the brain"},{"location":"courses/PSY8036SP2019/#understanding-deep-networks","text":"","title":"Understanding deep networks"},{"location":"courses/PSY8036SP2019/#week-8-theory-shallow-vs-deep","text":"Universal approximators. Hierarchy as a solution to over-fitting. Poggio, T., Mhaskar, H., Rosasco, L., Miranda, B., & Liao, Q. (2017, October 1). Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review. International Journal of Automation and Computing, 14(5), 503\u2013519. doi: 10.1007/s11633-017-1054-2 Garriga-Alonso, A., Aitchison, L., & Rasmussen, C. E. (2019). DEEP CONVOLUTIONAL NETWORKS AS SHALLOW GAUSSIAN PROCESSES, 16 Lin, H. & Jegelka, S. (2018, June 28). ResNet with one-neuron hidden layers is a Universal Approximator. arXiv: 1806.10909 [cs, stat] . Retrieved January 18, 2019, from http://arxiv.org/abs/1806.10909","title":"Week 8 Theory: shallow vs. deep"},{"location":"courses/PSY8036SP2019/#week-9-theory-dcnns-and-implicit-generative-models","text":"Feedforward DCNNs, implicit generative models, texture, and maximum entropy. Readings: Xie, J., Zhu, S.-C., & Wu, Y. N. (2017). Synthesizing dynamic patterns by spatial-temporal generative convnet. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 7093\u20137101). Retrieved from http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Synthesizing_Dynamic_Patterns_CVPR_2017_paper.pdf Zhang, Q., Wu, Y. N., & Zhu, S.-C. (2017, October 2). Interpretable Convolutional Neural Networks. arXiv: 1710.00935 [cs] . Retrieved February 22, 2018, from http://arxiv.org/abs/1710.00935 WU, Y. N., XIE, J., LU, Y., & ZHU, S.-C. (2018). Sparse and Deep Generalizations of the FRAME Model. Retrieved from https://www.intlpress.com/site/pub/files/_fulltext/journals/amsa/2018/0003/0001/AMSA-2018-0003-0001-a007.pdf","title":"Week 9 Theory: DCNNs and implicit generative models"},{"location":"courses/PSY8036SP2019/#week-10-theory-invariance","text":"Readings: Tacchetti, A., Isik, L., & Poggio, T. A. (2018, September 15). Invariant Recognition Shapes Neural Representations of Visual Input. Annual Review of Vision Science, 4(1), 403\u2013422. doi: 10.1146/annurev-vision-091517-034103 Leibo, J. Z., Liao, Q., Anselmi, F., & Poggio, T. (2015, October 23). The Invariance Hypothesis Implies Domain-Specific Regions in Visual Cortex. PLOS Computational Biology, 11(10), e1004390. doi: 10.1371/journal.pcbi.1004390 Azulay, A. & Weiss, Y. (2018, May 30). Why do deep convolutional networks generalize so poorly to small image transformations? arXiv: 1805.12177 [cs] . Retrieved October 4, 2018, from http://arxiv.org/abs/1805.12177","title":"Week 10 Theory: invariance"},{"location":"courses/PSY8036SP2019/#week-11-theory-normalization","text":"The value and types of normalization: batch, spatial, temporal, and channel normalization. Liao, Q., Kawaguchi, K., & Poggio, T. (2016, October 19). Streaming Normalization: Towards Simpler and More Biologically-plausible Normalizations for Online and Recurrent Learning. arXiv: 1610.06160 [cs] . Retrieved December 7, 2018, from http://arxiv.org/abs/1610.06160","title":"Week 11 Theory: normalization"},{"location":"courses/PSY8036SP2019/#whats-missing","text":"","title":"What\u2019s missing?"},{"location":"courses/PSY8036SP2019/#week-12-compositionality-and-semantic-accessibility","text":"The problems of occlusion and articulation (e.g. body pose). Computing spatial relationships,... Readings: Burgess, C. P., Matthey, L., Watters, N., Kabra, R., Higgins, I., Botvinick, M., & Lerchner, A. (2019, January 22). MONet: Unsupervised Scene Decomposition and Representation. arXiv: 1901.11390 [cs, stat] . Retrieved February 5, 2019, from http://arxiv.org/abs/1901.11390 Tang, H., Schrimpf, M., Lotter, W., Moerman, C., Paredes, A., Ortega Caro, J., \u2026, & Kreiman, G. (2018, August 28). Recurrent computations for visual pattern completion. Proceedings of the National Academy of Sciences, 115(35), 8835\u20138840. doi: 10.1073/pnas.1719397115 Zhang, Z., Xie, C., Wang, J.","title":"Week 12 Compositionality and semantic accessibility"},{"location":"datasets/camouflage/camouflage/","text":"The digital embryo camouflage challenge dataset # Download data (camouflage-embryos-data.zip) This dataset provides 20 images of 9 objects. Each image has a novel camouflage albedo texture map, and a novel background of other digital embryos, also with a novel arrangements and camouflage patterns. The target object is always in front, i.e. unoccluded and \"in plain view\". The test set consists of 20 additional camouflaged images of the 9 objects. It is difficult to perceive the target object in any given image without experience. Video As an illustration of the effectiveness of the camouflage, note that on initial viewing a target object in the video below only becomes visible once it moves. Try pausing, then restarting the video. After a while, you may learn to recognize and segment the target object even in a static view. Making Digital Embryos # Digital embryos Digital embryos are are created using a stochastic process which is analogous to embryological development. The novelty of the objects arises from the random initialization of growth \"hormones\" to the vertices of the polygon \"egge\". Because of the embryological nature of the algorithm, digital embryos bear some resemble to living forms, although they do not usually look like any particular plant or animal species. Digital embryos may have applications in art, entertainment, research, or anywhere novel 3D shapes are useful. Our interest is in vision research to understand how we learn novel objects. the embryos are used to study camouflage, visual learning, and object recognition. 1 For more information on the algorithm, see Brady, M. J. (1999). Psychophysical investigations of incomplete forms and forms with background . Available from Dissertations & Theses @ CIC Institutions; ProQuest Dissertations & Theses A&I. (304522737) For more on the development and applications of digital embryos, see Jay Hegd\u00e9's web page Mark J Brady and Daniel Kersten. Bootstrapped learning of novel objects. Journal of Vision , 3(6):2\u20132, 2003. doi:10.1167/3.6.2 . \u21a9","title":"Camouflage"},{"location":"datasets/camouflage/camouflage/#the-digital-embryo-camouflage-challenge-dataset","text":"Download data (camouflage-embryos-data.zip) This dataset provides 20 images of 9 objects. Each image has a novel camouflage albedo texture map, and a novel background of other digital embryos, also with a novel arrangements and camouflage patterns. The target object is always in front, i.e. unoccluded and \"in plain view\". The test set consists of 20 additional camouflaged images of the 9 objects. It is difficult to perceive the target object in any given image without experience. Video As an illustration of the effectiveness of the camouflage, note that on initial viewing a target object in the video below only becomes visible once it moves. Try pausing, then restarting the video. After a while, you may learn to recognize and segment the target object even in a static view.","title":"The digital embryo camouflage challenge dataset"},{"location":"datasets/camouflage/camouflage/#making-digital-embryos","text":"Digital embryos Digital embryos are are created using a stochastic process which is analogous to embryological development. The novelty of the objects arises from the random initialization of growth \"hormones\" to the vertices of the polygon \"egge\". Because of the embryological nature of the algorithm, digital embryos bear some resemble to living forms, although they do not usually look like any particular plant or animal species. Digital embryos may have applications in art, entertainment, research, or anywhere novel 3D shapes are useful. Our interest is in vision research to understand how we learn novel objects. the embryos are used to study camouflage, visual learning, and object recognition. 1 For more information on the algorithm, see Brady, M. J. (1999). Psychophysical investigations of incomplete forms and forms with background . Available from Dissertations & Theses @ CIC Institutions; ProQuest Dissertations & Theses A&I. (304522737) For more on the development and applications of digital embryos, see Jay Hegd\u00e9's web page Mark J Brady and Daniel Kersten. Bootstrapped learning of novel objects. Journal of Vision , 3(6):2\u20132, 2003. doi:10.1167/3.6.2 . \u21a9","title":"Making Digital Embryos"},{"location":"demos/lightness-shape/","text":"Lightness and shape Demonstration # Lightness and shape Your browser does not support HTML5 video. The gradually shaded left and right halves of the flat object on the left are physically identical. The central regions of the left and right cylinders on the right also have identical intensity profiles. Further, all four of gradients are identical to across the central regions. The horizontal light intensity profiles are illustrated by the red lines. 1 David C Knill and Daniel Kersten. Apparent surface curvature affects lightness perception. Nature , 351(6323):228\u2013230, 1991. doi:10.1038/351228a0 . \u21a9","title":"Lightness and Shape"},{"location":"demos/lightness-shape/#lightness-and-shape-demonstration","text":"Lightness and shape Your browser does not support HTML5 video. The gradually shaded left and right halves of the flat object on the left are physically identical. The central regions of the left and right cylinders on the right also have identical intensity profiles. Further, all four of gradients are identical to across the central regions. The horizontal light intensity profiles are illustrated by the red lines. 1 David C Knill and Daniel Kersten. Apparent surface curvature affects lightness perception. Nature , 351(6323):228\u2013230, 1991. doi:10.1038/351228a0 . \u21a9","title":"Lightness and shape Demonstration"},{"location":"demos/motion-surface/","text":"Visual motion and the perception of surface material Demonstrations # Motion can affect material appearance The first half of the movie simulates a chrome teapot rotating in mid-air. The appearance is what one might expect, that of a shiny chrome teapot. Half way through the movie, the reflection gets painted on to the teapot making a \"sticky reflection.\" The painted-on pattern mimics what one would see in the reflection of a perfectly reflecting chrome teapot. Now the body of the teapot appears more or less like a painted matte object during the second half of the movie. Try stopping the movie. The teapot reverts to a more shiny appearance. Croissant-shaped object Now with a croissant-shaped object. Polygonal object Many observers report that this polygonal object seems to look transparent rather than shiny-opaque. 'Wrong' optic flow This shows the \"wrong\" optic flow, in which the environment is moving about a different axis than the teapot. Teapot Your browser does not support the video tag. The highlights at points of high curvature in this last one conflict to some extent with the matte interpretation, and thus one can see these points as from a shiny object. Also, note the lack of contact information (inter-reflections are not modeled in this rendering) for the handle and spout. Movies were created by Bruce Hartung in collaboration with Dan Kersten and advice from Ted Adelson. The measured illumination map was obtained from http://www.debevec.org/Probes/ . This study was reported at the 2002 meeting of the Vision Sciences Society in Sarasota, Florida: Hartung, B., & Kersten, D. (2002). Distinguishing Shiny from Matte. Presented at Vision Sciences Society meeting, Sarasota, Florida. For more on human perception of shiny objects, see: Doerschner, K., Fleming, R. W., Yilmaz, O., Schrater, P. R., Hartung, B., & Kersten, D. (2011). Visual Motion and the Perception of Surface Material. Current Biology, 21(23), 2010\u20132016. link","title":"Motion and Surface Material"},{"location":"demos/motion-surface/#visual-motion-and-the-perception-of-surface-material-demonstrations","text":"Motion can affect material appearance The first half of the movie simulates a chrome teapot rotating in mid-air. The appearance is what one might expect, that of a shiny chrome teapot. Half way through the movie, the reflection gets painted on to the teapot making a \"sticky reflection.\" The painted-on pattern mimics what one would see in the reflection of a perfectly reflecting chrome teapot. Now the body of the teapot appears more or less like a painted matte object during the second half of the movie. Try stopping the movie. The teapot reverts to a more shiny appearance. Croissant-shaped object Now with a croissant-shaped object. Polygonal object Many observers report that this polygonal object seems to look transparent rather than shiny-opaque. 'Wrong' optic flow This shows the \"wrong\" optic flow, in which the environment is moving about a different axis than the teapot. Teapot Your browser does not support the video tag. The highlights at points of high curvature in this last one conflict to some extent with the matte interpretation, and thus one can see these points as from a shiny object. Also, note the lack of contact information (inter-reflections are not modeled in this rendering) for the handle and spout. Movies were created by Bruce Hartung in collaboration with Dan Kersten and advice from Ted Adelson. The measured illumination map was obtained from http://www.debevec.org/Probes/ . This study was reported at the 2002 meeting of the Vision Sciences Society in Sarasota, Florida: Hartung, B., & Kersten, D. (2002). Distinguishing Shiny from Matte. Presented at Vision Sciences Society meeting, Sarasota, Florida. For more on human perception of shiny objects, see: Doerschner, K., Fleming, R. W., Yilmaz, O., Schrater, P. R., Hartung, B., & Kersten, D. (2011). Visual Motion and the Perception of Surface Material. Current Biology, 21(23), 2010\u20132016. link","title":"Visual motion and the perception of surface material Demonstrations"},{"location":"demos/retinotopy/","text":"Illustration of log-polar mapping # The primary visual cortex (V1) has a retinotopic map of the visual field, i.e. activation of receptors at nearby positions on the retina in turn activate neurons at nearby positions in V1. The mapping is approximatately log-polar. If you fixate on a red dot and view the upper (or lower) moving pattern on the left, your primary visual cortex will produce a wave of neural activity moving from posterior towards anterior portions of your occipital lobe. This is illustrated on the upper right (or lower right). The red dot indexes the foveal locations. Retinal stimulation and corresponding spatial pattern of V1 activity","title":"Retinotopy"},{"location":"demos/retinotopy/#illustration-of-log-polar-mapping","text":"The primary visual cortex (V1) has a retinotopic map of the visual field, i.e. activation of receptors at nearby positions on the retina in turn activate neurons at nearby positions in V1. The mapping is approximatately log-polar. If you fixate on a red dot and view the upper (or lower) moving pattern on the left, your primary visual cortex will produce a wave of neural activity moving from posterior towards anterior portions of your occipital lobe. This is illustrated on the upper right (or lower right). The red dot indexes the foveal locations. Retinal stimulation and corresponding spatial pattern of V1 activity","title":"Illustration of log-polar mapping"},{"location":"demos/shadows/","text":"Cast shadows, motion, and depth # The strength of a cast shadow as a cue for depth can be illustrated by moving a shadow without moving the object casting the shadow. Illusory motion from shadows The green square doesn't move in the image, but it appears to move in depth, and in the image. 1 In the next demonstration, the image size of the ball is unchanged, but the position of the ball changes. The ball follows a diagonal trajectory inside a box. The ball's shadow first moves diagonally in a trajectory parallel to the ball, then it moves horizontally. Moving cast shadows induce apparent motion in depth The ball's trajectory is the same for both segments of the animation. The apparent motion in depth of the ball is strikingly different in the two cases. When the shadow is diagonal, the ball appears to slide along the floor to the back of the box. When the shadow trajectory is horizontal, the ball appears to rise above the floor of the box. 2 Shadows and the bouncing ball This demo provides a simple demonstration of vision's preference to interpret motion of a ball and its shadow as consistent with a stationary light source. 12 minutes worth of assorted shadow demonstrations Developed at the Max Planck Institute for Biological Cybernetics with Pascal Mamassian, Isabelle B\u00fclthoff, David Knill and Heinrich B\u00fclthoff in 1993. The video includes demonstrations of: how cast shadows can affect perceived size through depth; how motion of the light source are interpreted as changes in depth; apparent non-linear motion from cast shadows; how additional cues for light source motion don't help in interpreting ball motion as due to light source rather than depth changes; how cast shadows can reduce shape ambiguity; influence perceived rigidity; how the shape of an object doesn't need to match the shape or color of the \"shadow\" patch; and how the reversing the contrast of the \"shadow\"-i.e. a light rather than dark shadow--reduces the effect; and how illumination from below also can reduce the strength of the depth illusion. Shadows and sterescopic depth In this video, as in the above green square demo, the size of the green square doesn't change. But a stereoscopic cue is introduced that is in conflict with the shadow cue. The demonstration should be viewed with crossed-fusion--i.e. the left image to the right eye, and the right image to the left eye. The shadows are still effective at conveying a change in depth, despite the stereo disparity cue consistent with a depth change in the opposite direction\u2014at least initially. After a while, the disparity seems to win out and the green square move appears to move towards the checkerboard background and \"squeeze\" the dark shadow away, then recede from the background and \"suck\" the shadow under itself. Daniel Kersten, David C Knill, Pascal Mamassian, and Isabelle B\u00fclthoff. Illusory motion from shadows. Nature , 379(6560):31, 1996. doi:10.1038/379031a0 . \u21a9 Daniel Kersten, Pascal Mamassian, and David C Knill. Moving cast shadows induce apparent motion in depth. Perception , 26(2):171\u2013192, 1997. doi:10.1068/p260171 . \u21a9","title":"Shadows"},{"location":"demos/shadows/#cast-shadows-motion-and-depth","text":"The strength of a cast shadow as a cue for depth can be illustrated by moving a shadow without moving the object casting the shadow. Illusory motion from shadows The green square doesn't move in the image, but it appears to move in depth, and in the image. 1 In the next demonstration, the image size of the ball is unchanged, but the position of the ball changes. The ball follows a diagonal trajectory inside a box. The ball's shadow first moves diagonally in a trajectory parallel to the ball, then it moves horizontally. Moving cast shadows induce apparent motion in depth The ball's trajectory is the same for both segments of the animation. The apparent motion in depth of the ball is strikingly different in the two cases. When the shadow is diagonal, the ball appears to slide along the floor to the back of the box. When the shadow trajectory is horizontal, the ball appears to rise above the floor of the box. 2 Shadows and the bouncing ball This demo provides a simple demonstration of vision's preference to interpret motion of a ball and its shadow as consistent with a stationary light source. 12 minutes worth of assorted shadow demonstrations Developed at the Max Planck Institute for Biological Cybernetics with Pascal Mamassian, Isabelle B\u00fclthoff, David Knill and Heinrich B\u00fclthoff in 1993. The video includes demonstrations of: how cast shadows can affect perceived size through depth; how motion of the light source are interpreted as changes in depth; apparent non-linear motion from cast shadows; how additional cues for light source motion don't help in interpreting ball motion as due to light source rather than depth changes; how cast shadows can reduce shape ambiguity; influence perceived rigidity; how the shape of an object doesn't need to match the shape or color of the \"shadow\" patch; and how the reversing the contrast of the \"shadow\"-i.e. a light rather than dark shadow--reduces the effect; and how illumination from below also can reduce the strength of the depth illusion. Shadows and sterescopic depth In this video, as in the above green square demo, the size of the green square doesn't change. But a stereoscopic cue is introduced that is in conflict with the shadow cue. The demonstration should be viewed with crossed-fusion--i.e. the left image to the right eye, and the right image to the left eye. The shadows are still effective at conveying a change in depth, despite the stereo disparity cue consistent with a depth change in the opposite direction\u2014at least initially. After a while, the disparity seems to win out and the green square move appears to move towards the checkerboard background and \"squeeze\" the dark shadow away, then recede from the background and \"suck\" the shadow under itself. Daniel Kersten, David C Knill, Pascal Mamassian, and Isabelle B\u00fclthoff. Illusory motion from shadows. Nature , 379(6560):31, 1996. doi:10.1038/379031a0 . \u21a9 Daniel Kersten, Pascal Mamassian, and David C Knill. Moving cast shadows induce apparent motion in depth. Perception , 26(2):171\u2013192, 1997. doi:10.1068/p260171 . \u21a9","title":"Cast shadows, motion, and depth"},{"location":"demos/transparency/","text":"Transparency and structure from motion Bistability in transparency and rigidity # 1.1 Two transparent rotating squares When you initially view the video above, 1.1 Two transparent rotating squares , you will most likely see one transparent (darker) square surface in front of a (lighter) opaque square surface, together rotating about a single axis. This percept corresponds an interpretation of the front and back faces of a rotating cube viewed from above, and looks like 1.2 Two faces rotating about a single central axis . However, if you look long enough at 1.1 Two transparent rotating squares , eventually your perceptual interpretation will suddenly flip, and you will see the two surfaces apparently slipping and sliding over one another, where the lighter upper surface now appears transparent and in front, while the lower darker surface now appears opaque and in back. This percept looks like 1.3 Two faces rotating about two distinct axes . It is as if your \"visual brain\" \"knows\" about the physics of rotation, coming up with the split axes interpretation even before you are cognitively aware that this is a second possible physical explanation for the rotation. 1 1.2 Two faces rotating about a single central axis 1.3 Two faces rotating about two distinct axes Daniel Kersten, Heinrich H B\u00fclthoff, Bennett L Schwartz, and Kenneth J Kurtz. Interaction between transparency and structure from motion. Neural Computation , 4(4):573\u2013589, 1992. doi:10.1162/neco.1992.4.4.573 . \u21a9","title":"Transparency"},{"location":"demos/transparency/#transparency-and-structure-from-motion-bistability-in-transparency-and-rigidity","text":"1.1 Two transparent rotating squares When you initially view the video above, 1.1 Two transparent rotating squares , you will most likely see one transparent (darker) square surface in front of a (lighter) opaque square surface, together rotating about a single axis. This percept corresponds an interpretation of the front and back faces of a rotating cube viewed from above, and looks like 1.2 Two faces rotating about a single central axis . However, if you look long enough at 1.1 Two transparent rotating squares , eventually your perceptual interpretation will suddenly flip, and you will see the two surfaces apparently slipping and sliding over one another, where the lighter upper surface now appears transparent and in front, while the lower darker surface now appears opaque and in back. This percept looks like 1.3 Two faces rotating about two distinct axes . It is as if your \"visual brain\" \"knows\" about the physics of rotation, coming up with the split axes interpretation even before you are cognitively aware that this is a second possible physical explanation for the rotation. 1 1.2 Two faces rotating about a single central axis 1.3 Two faces rotating about two distinct axes Daniel Kersten, Heinrich H B\u00fclthoff, Bennett L Schwartz, and Kenneth J Kurtz. Interaction between transparency and structure from motion. Neural Computation , 4(4):573\u2013589, 1992. doi:10.1162/neco.1992.4.4.573 . \u21a9","title":"Transparency and structure from motion Bistability in transparency and rigidity"}]}